<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1997, 2014, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/metadataOnStackMark.hpp"
  27 #include "classfile/systemDictionary.hpp"
  28 #include "code/debugInfoRec.hpp"
  29 #include "gc_interface/collectedHeap.inline.hpp"
  30 #include "interpreter/bytecodeStream.hpp"
  31 #include "interpreter/bytecodeTracer.hpp"
  32 #include "interpreter/bytecodes.hpp"
  33 #include "interpreter/interpreter.hpp"
  34 #include "interpreter/oopMapCache.hpp"
  35 #include "memory/gcLocker.hpp"
  36 #include "memory/generation.hpp"
  37 #include "memory/heapInspection.hpp"
  38 #include "memory/metadataFactory.hpp"
  39 #include "memory/oopFactory.hpp"
  40 #include "oops/constMethod.hpp"
  41 #include "oops/methodData.hpp"
  42 #include "oops/method.hpp"
  43 #include "oops/oop.inline.hpp"
  44 #include "oops/symbol.hpp"
  45 #include "prims/jvmtiExport.hpp"
  46 #include "prims/methodHandles.hpp"
  47 #include "prims/nativeLookup.hpp"
  48 #include "runtime/arguments.hpp"
  49 #include "runtime/compilationPolicy.hpp"
  50 #include "runtime/frame.inline.hpp"
  51 #include "runtime/handles.inline.hpp"
  52 #include "runtime/relocator.hpp"
  53 #include "runtime/sharedRuntime.hpp"
  54 #include "runtime/signature.hpp"
  55 #include "utilities/quickSort.hpp"
  56 #include "utilities/xmlstream.hpp"
  57 
  58 
  59 // Implementation of Method
  60 
  61 Method* Method::allocate(ClassLoaderData* loader_data,
  62                          int byte_code_size,
  63                          AccessFlags access_flags,
  64                          InlineTableSizes* sizes,
  65                          ConstMethod::MethodType method_type,
  66                          TRAPS) {
  67   assert(!access_flags.is_native() || byte_code_size == 0,
  68          "native methods should not contain byte codes");
  69   ConstMethod* cm = ConstMethod::allocate(loader_data,
  70                                           byte_code_size,
  71                                           sizes,
  72                                           method_type,
  73                                           CHECK_NULL);
  74 
  75   int size = Method::size(access_flags.is_native());
  76 
  77   return new (loader_data, size, false, MetaspaceObj::MethodType, THREAD) Method(cm, access_flags, size);
  78 }
  79 
  80 Method::Method(ConstMethod* xconst, AccessFlags access_flags, int size) {
  81   No_Safepoint_Verifier no_safepoint;
  82   set_constMethod(xconst);
  83   set_access_flags(access_flags);
  84   set_method_size(size);
  85 #ifdef CC_INTERP
  86   set_result_index(T_VOID);
  87 #endif
  88   set_intrinsic_id(vmIntrinsics::_none);
  89   set_jfr_towrite(false);
  90   set_force_inline(false);
  91   set_hidden(false);
  92   set_dont_inline(false);
  93   set_method_data(NULL);
  94   set_method_counters(NULL);
  95   set_vtable_index(Method::garbage_vtable_index);
  96 
  97   // Fix and bury in Method*
  98   set_interpreter_entry(NULL); // sets i2i entry and from_int
  99   set_adapter_entry(NULL);
 100   clear_code(); // from_c/from_i get set to c2i/i2i
 101 
 102   if (access_flags.is_native()) {
 103     clear_native_function();
 104     set_signature_handler(NULL);
 105   }
 106 
 107   NOT_PRODUCT(set_compiled_invocation_count(0);)
 108 }
 109 
 110 // Release Method*.  The nmethod will be gone when we get here because
 111 // we've walked the code cache.
 112 void Method::deallocate_contents(ClassLoaderData* loader_data) {
 113   MetadataFactory::free_metadata(loader_data, constMethod());
 114   set_constMethod(NULL);
 115   MetadataFactory::free_metadata(loader_data, method_data());
 116   set_method_data(NULL);
 117   MetadataFactory::free_metadata(loader_data, method_counters());
 118   set_method_counters(NULL);
 119   // The nmethod will be gone when we get here.
 120   if (code() != NULL) _code = NULL;
 121 }
 122 
 123 address Method::get_i2c_entry() {
 124   assert(_adapter != NULL, "must have");
 125   return _adapter-&gt;get_i2c_entry();
 126 }
 127 
 128 address Method::get_c2i_entry() {
 129   assert(_adapter != NULL, "must have");
 130   return _adapter-&gt;get_c2i_entry();
 131 }
 132 
 133 address Method::get_c2i_unverified_entry() {
 134   assert(_adapter != NULL, "must have");
 135   return _adapter-&gt;get_c2i_unverified_entry();
 136 }
 137 
 138 char* Method::name_and_sig_as_C_string() const {
 139   return name_and_sig_as_C_string(constants()-&gt;pool_holder(), name(), signature());
 140 }
 141 
 142 char* Method::name_and_sig_as_C_string(char* buf, int size) const {
 143   return name_and_sig_as_C_string(constants()-&gt;pool_holder(), name(), signature(), buf, size);
 144 }
 145 
 146 char* Method::name_and_sig_as_C_string(Klass* klass, Symbol* method_name, Symbol* signature) {
 147   const char* klass_name = klass-&gt;external_name();
 148   int klass_name_len  = (int)strlen(klass_name);
 149   int method_name_len = method_name-&gt;utf8_length();
 150   int len             = klass_name_len + 1 + method_name_len + signature-&gt;utf8_length();
 151   char* dest          = NEW_RESOURCE_ARRAY(char, len + 1);
 152   strcpy(dest, klass_name);
 153   dest[klass_name_len] = '.';
 154   strcpy(&amp;dest[klass_name_len + 1], method_name-&gt;as_C_string());
 155   strcpy(&amp;dest[klass_name_len + 1 + method_name_len], signature-&gt;as_C_string());
 156   dest[len] = 0;
 157   return dest;
 158 }
 159 
 160 char* Method::name_and_sig_as_C_string(Klass* klass, Symbol* method_name, Symbol* signature, char* buf, int size) {
 161   Symbol* klass_name = klass-&gt;name();
 162   klass_name-&gt;as_klass_external_name(buf, size);
 163   int len = (int)strlen(buf);
 164 
 165   if (len &lt; size - 1) {
 166     buf[len++] = '.';
 167 
 168     method_name-&gt;as_C_string(&amp;(buf[len]), size - len);
 169     len = (int)strlen(buf);
 170 
 171     signature-&gt;as_C_string(&amp;(buf[len]), size - len);
 172   }
 173 
 174   return buf;
 175 }
 176 
 177 int Method::fast_exception_handler_bci_for(methodHandle mh, KlassHandle ex_klass, int throw_bci, TRAPS) {
 178   // exception table holds quadruple entries of the form (beg_bci, end_bci, handler_bci, klass_index)
 179   // access exception table
 180   ExceptionTable table(mh());
 181   int length = table.length();
 182   // iterate through all entries sequentially
 183   constantPoolHandle pool(THREAD, mh-&gt;constants());
 184   for (int i = 0; i &lt; length; i ++) {
 185     //reacquire the table in case a GC happened
 186     ExceptionTable table(mh());
 187     int beg_bci = table.start_pc(i);
 188     int end_bci = table.end_pc(i);
 189     assert(beg_bci &lt;= end_bci, "inconsistent exception table");
 190     if (beg_bci &lt;= throw_bci &amp;&amp; throw_bci &lt; end_bci) {
 191       // exception handler bci range covers throw_bci =&gt; investigate further
 192       int handler_bci = table.handler_pc(i);
 193       int klass_index = table.catch_type_index(i);
 194       if (klass_index == 0) {
 195         return handler_bci;
 196       } else if (ex_klass.is_null()) {
 197         return handler_bci;
 198       } else {
 199         // we know the exception class =&gt; get the constraint class
 200         // this may require loading of the constraint class; if verification
 201         // fails or some other exception occurs, return handler_bci
 202         Klass* k = pool-&gt;klass_at(klass_index, CHECK_(handler_bci));
 203         KlassHandle klass = KlassHandle(THREAD, k);
 204         assert(klass.not_null(), "klass not loaded");
 205         if (ex_klass-&gt;is_subtype_of(klass())) {
 206           return handler_bci;
 207         }
 208       }
 209     }
 210   }
 211 
 212   return -1;
 213 }
 214 
 215 void Method::mask_for(int bci, InterpreterOopMap* mask) {
 216 
 217   Thread* myThread    = Thread::current();
 218   methodHandle h_this(myThread, this);
 219 #ifdef ASSERT
 220   bool has_capability = myThread-&gt;is_VM_thread() ||
 221                         myThread-&gt;is_ConcurrentGC_thread() ||
 222                         myThread-&gt;is_GC_task_thread();
 223 
 224   if (!has_capability) {
 225     if (!VerifyStack &amp;&amp; !VerifyLastFrame) {
 226       // verify stack calls this outside VM thread
 227       warning("oopmap should only be accessed by the "
 228               "VM, GC task or CMS threads (or during debugging)");
 229       InterpreterOopMap local_mask;
 230       method_holder()-&gt;mask_for(h_this, bci, &amp;local_mask);
 231       local_mask.print();
 232     }
 233   }
 234 #endif
 235   method_holder()-&gt;mask_for(h_this, bci, mask);
 236   return;
 237 }
 238 
 239 
 240 int Method::bci_from(address bcp) const {
 241 #ifdef ASSERT
 242   { ResourceMark rm;
 243   assert(is_native() &amp;&amp; bcp == code_base() || contains(bcp) || is_error_reported(),
 244          err_msg("bcp doesn't belong to this method: bcp: " INTPTR_FORMAT ", method: %s", bcp, name_and_sig_as_C_string()));
 245   }
 246 #endif
 247   return bcp - code_base();
 248 }
 249 
 250 
 251 // Return (int)bcx if it appears to be a valid BCI.
 252 // Return bci_from((address)bcx) if it appears to be a valid BCP.
 253 // Return -1 otherwise.
 254 // Used by profiling code, when invalid data is a possibility.
 255 // The caller is responsible for validating the Method* itself.
 256 int Method::validate_bci_from_bcx(intptr_t bcx) const {
 257   // keep bci as -1 if not a valid bci
 258   int bci = -1;
 259   if (bcx == 0 || (address)bcx == code_base()) {
 260     // code_size() may return 0 and we allow 0 here
 261     // the method may be native
 262     bci = 0;
 263   } else if (frame::is_bci(bcx)) {
 264     if (bcx &lt; code_size()) {
 265       bci = (int)bcx;
 266     }
 267   } else if (contains((address)bcx)) {
 268     bci = (address)bcx - code_base();
 269   }
 270   // Assert that if we have dodged any asserts, bci is negative.
 271   assert(bci == -1 || bci == bci_from(bcp_from(bci)), "sane bci if &gt;=0");
 272   return bci;
 273 }
 274 
 275 address Method::bcp_from(int bci) const {
 276   assert((is_native() &amp;&amp; bci == 0)  || (!is_native() &amp;&amp; 0 &lt;= bci &amp;&amp; bci &lt; code_size()), err_msg("illegal bci: %d", bci));
 277   address bcp = code_base() + bci;
 278   assert(is_native() &amp;&amp; bcp == code_base() || contains(bcp), "bcp doesn't belong to this method");
 279   return bcp;
 280 }
 281 
 282 
 283 int Method::size(bool is_native) {
 284   // If native, then include pointers for native_function and signature_handler
 285   int extra_bytes = (is_native) ? 2*sizeof(address*) : 0;
 286   int extra_words = align_size_up(extra_bytes, BytesPerWord) / BytesPerWord;
 287   return align_object_size(header_size() + extra_words);
 288 }
 289 
 290 
 291 Symbol* Method::klass_name() const {
 292   Klass* k = method_holder();
 293   assert(k-&gt;is_klass(), "must be klass");
 294   InstanceKlass* ik = (InstanceKlass*) k;
 295   return ik-&gt;name();
 296 }
 297 
 298 
 299 // Attempt to return method oop to original state.  Clear any pointers
 300 // (to objects outside the shared spaces).  We won't be able to predict
 301 // where they should point in a new JVM.  Further initialize some
 302 // entries now in order allow them to be write protected later.
 303 
 304 void Method::remove_unshareable_info() {
 305   unlink_method();
 306 }
 307 
 308 
 309 bool Method::was_executed_more_than(int n) {
 310   // Invocation counter is reset when the Method* is compiled.
 311   // If the method has compiled code we therefore assume it has
 312   // be excuted more than n times.
 313   if (is_accessor() || is_empty_method() || (code() != NULL)) {
 314     // interpreter doesn't bump invocation counter of trivial methods
 315     // compiler does not bump invocation counter of compiled methods
 316     return true;
 317   }
 318   else if ((method_counters() != NULL &amp;&amp;
 319             method_counters()-&gt;invocation_counter()-&gt;carry()) ||
 320            (method_data() != NULL &amp;&amp;
 321             method_data()-&gt;invocation_counter()-&gt;carry())) {
 322     // The carry bit is set when the counter overflows and causes
 323     // a compilation to occur.  We don't know how many times
 324     // the counter has been reset, so we simply assume it has
 325     // been executed more than n times.
 326     return true;
 327   } else {
 328     return invocation_count() &gt; n;
 329   }
 330 }
 331 
 332 void Method::print_invocation_count() {
 333   if (is_static()) tty-&gt;print("static ");
 334   if (is_final()) tty-&gt;print("final ");
 335   if (is_synchronized()) tty-&gt;print("synchronized ");
 336   if (is_native()) tty-&gt;print("native ");
 337   tty-&gt;print("%s::", method_holder()-&gt;external_name());
 338   name()-&gt;print_symbol_on(tty);
 339   signature()-&gt;print_symbol_on(tty);
 340 
 341   if (WizardMode) {
 342     // dump the size of the byte codes
 343     tty-&gt;print(" {%d}", code_size());
 344   }
 345   tty-&gt;cr();
 346 
 347   tty-&gt;print_cr ("  interpreter_invocation_count: %8d ", interpreter_invocation_count());
 348   tty-&gt;print_cr ("  invocation_counter:           %8d ", invocation_count());
 349   tty-&gt;print_cr ("  backedge_counter:             %8d ", backedge_count());
 350 #ifndef PRODUCT
 351   if (CountCompiledCalls) {
 352     tty-&gt;print_cr ("  compiled_invocation_count: %8d ", compiled_invocation_count());
 353   }
 354 #endif
 355 }
 356 
 357 // Build a MethodData* object to hold information about this method
 358 // collected in the interpreter.
 359 void Method::build_interpreter_method_data(methodHandle method, TRAPS) {
 360   // Do not profile method if current thread holds the pending list lock,
 361   // which avoids deadlock for acquiring the MethodData_lock.
 362   if (InstanceRefKlass::owns_pending_list_lock((JavaThread*)THREAD)) {
 363     return;
 364   }
 365 
 366   // Grab a lock here to prevent multiple
 367   // MethodData*s from being created.
 368   MutexLocker ml(MethodData_lock, THREAD);
 369   if (method-&gt;method_data() == NULL) {
 370     ClassLoaderData* loader_data = method-&gt;method_holder()-&gt;class_loader_data();
 371     MethodData* method_data = MethodData::allocate(loader_data, method, CHECK);
 372     method-&gt;set_method_data(method_data);
 373     if (PrintMethodData &amp;&amp; (Verbose || WizardMode)) {
 374       ResourceMark rm(THREAD);
 375       tty-&gt;print("build_interpreter_method_data for ");
 376       method-&gt;print_name(tty);
 377       tty-&gt;cr();
 378       // At the end of the run, the MDO, full of data, will be dumped.
 379     }
 380   }
 381 }
 382 
 383 MethodCounters* Method::build_method_counters(Method* m, TRAPS) {
 384   methodHandle mh(m);
 385   ClassLoaderData* loader_data = mh-&gt;method_holder()-&gt;class_loader_data();
 386   MethodCounters* counters = MethodCounters::allocate(loader_data, CHECK_NULL);
 387   if (mh-&gt;method_counters() == NULL) {
 388     mh-&gt;set_method_counters(counters);
 389   } else {
 390     MetadataFactory::free_metadata(loader_data, counters);
 391   }
 392   return mh-&gt;method_counters();
 393 }
 394 
 395 void Method::cleanup_inline_caches() {
 396   // The current system doesn't use inline caches in the interpreter
 397   // =&gt; nothing to do (keep this method around for future use)
 398 }
 399 
 400 
 401 int Method::extra_stack_words() {
 402   // not an inline function, to avoid a header dependency on Interpreter
 403   return extra_stack_entries() * Interpreter::stackElementSize;
 404 }
 405 
 406 
 407 void Method::compute_size_of_parameters(Thread *thread) {
 408   ArgumentSizeComputer asc(signature());
 409   set_size_of_parameters(asc.size() + (is_static() ? 0 : 1));
 410 }
 411 
 412 #ifdef CC_INTERP
 413 void Method::set_result_index(BasicType type)          {
 414   _result_index = Interpreter::BasicType_as_index(type);
 415 }
 416 #endif
 417 
 418 BasicType Method::result_type() const {
 419   ResultTypeFinder rtf(signature());
 420   return rtf.type();
 421 }
 422 
 423 
 424 bool Method::is_empty_method() const {
 425   return  code_size() == 1
 426       &amp;&amp; *code_base() == Bytecodes::_return;
 427 }
 428 
 429 
 430 bool Method::is_vanilla_constructor() const {
 431   // Returns true if this method is a vanilla constructor, i.e. an "&lt;init&gt;" "()V" method
 432   // which only calls the superclass vanilla constructor and possibly does stores of
 433   // zero constants to local fields:
 434   //
 435   //   aload_0
 436   //   invokespecial
 437   //   indexbyte1
 438   //   indexbyte2
 439   //
 440   // followed by an (optional) sequence of:
 441   //
 442   //   aload_0
 443   //   aconst_null / iconst_0 / fconst_0 / dconst_0
 444   //   putfield
 445   //   indexbyte1
 446   //   indexbyte2
 447   //
 448   // followed by:
 449   //
 450   //   return
 451 
 452   assert(name() == vmSymbols::object_initializer_name(),    "Should only be called for default constructors");
 453   assert(signature() == vmSymbols::void_method_signature(), "Should only be called for default constructors");
 454   int size = code_size();
 455   // Check if size match
 456   if (size == 0 || size % 5 != 0) return false;
 457   address cb = code_base();
 458   int last = size - 1;
 459   if (cb[0] != Bytecodes::_aload_0 || cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {
 460     // Does not call superclass default constructor
 461     return false;
 462   }
 463   // Check optional sequence
 464   for (int i = 4; i &lt; last; i += 5) {
 465     if (cb[i] != Bytecodes::_aload_0) return false;
 466     if (!Bytecodes::is_zero_const(Bytecodes::cast(cb[i+1]))) return false;
 467     if (cb[i+2] != Bytecodes::_putfield) return false;
 468   }
 469   return true;
 470 }
 471 
 472 
 473 bool Method::compute_has_loops_flag() {
 474   BytecodeStream bcs(this);
 475   Bytecodes::Code bc;
 476 
 477   while ((bc = bcs.next()) &gt;= 0) {
 478     switch( bc ) {
 479       case Bytecodes::_ifeq:
 480       case Bytecodes::_ifnull:
 481       case Bytecodes::_iflt:
 482       case Bytecodes::_ifle:
 483       case Bytecodes::_ifne:
 484       case Bytecodes::_ifnonnull:
 485       case Bytecodes::_ifgt:
 486       case Bytecodes::_ifge:
 487       case Bytecodes::_if_icmpeq:
 488       case Bytecodes::_if_icmpne:
 489       case Bytecodes::_if_icmplt:
 490       case Bytecodes::_if_icmpgt:
 491       case Bytecodes::_if_icmple:
 492       case Bytecodes::_if_icmpge:
 493       case Bytecodes::_if_acmpeq:
 494       case Bytecodes::_if_acmpne:
 495       case Bytecodes::_goto:
 496       case Bytecodes::_jsr:
 497         if( bcs.dest() &lt; bcs.next_bci() ) _access_flags.set_has_loops();
 498         break;
 499 
 500       case Bytecodes::_goto_w:
 501       case Bytecodes::_jsr_w:
 502         if( bcs.dest_w() &lt; bcs.next_bci() ) _access_flags.set_has_loops();
 503         break;
 504     }
 505   }
 506   _access_flags.set_loops_flag_init();
 507   return _access_flags.has_loops();
 508 }
 509 
 510 bool Method::is_final_method(AccessFlags class_access_flags) const {
 511   // or "does_not_require_vtable_entry"
 512   // default method or overpass can occur, is not final (reuses vtable entry)
 513   // private methods get vtable entries for backward class compatibility.
 514   if (is_overpass() || is_default_method())  return false;
 515   return is_final() || class_access_flags.is_final();
 516 }
 517 
 518 bool Method::is_final_method() const {
 519   return is_final_method(method_holder()-&gt;access_flags());
 520 }
 521 
 522 bool Method::is_default_method() const {
 523   if (method_holder() != NULL &amp;&amp;
 524       method_holder()-&gt;is_interface() &amp;&amp;
 525       !is_abstract()) {
 526     return true;
 527   } else {
 528     return false;
 529   }
 530 }
 531 
 532 bool Method::can_be_statically_bound(AccessFlags class_access_flags) const {
 533   if (is_final_method(class_access_flags))  return true;
 534 #ifdef ASSERT
 535   ResourceMark rm;
 536   bool is_nonv = (vtable_index() == nonvirtual_vtable_index);
 537   if (class_access_flags.is_interface()) {
 538       assert(is_nonv == is_static(), err_msg("is_nonv=%s", name_and_sig_as_C_string()));
 539   }
 540 #endif
 541   assert(valid_vtable_index() || valid_itable_index(), "method must be linked before we ask this question");
 542   return vtable_index() == nonvirtual_vtable_index;
 543 }
 544 
 545 bool Method::can_be_statically_bound() const {
 546   return can_be_statically_bound(method_holder()-&gt;access_flags());
 547 }
 548 
 549 bool Method::is_accessor() const {
 550   if (code_size() != 5) return false;
 551   if (size_of_parameters() != 1) return false;
 552   if (java_code_at(0) != Bytecodes::_aload_0 ) return false;
 553   if (java_code_at(1) != Bytecodes::_getfield) return false;
 554   if (java_code_at(4) != Bytecodes::_areturn &amp;&amp;
 555       java_code_at(4) != Bytecodes::_ireturn ) return false;
 556   return true;
 557 }
 558 
 559 
 560 bool Method::is_initializer() const {
 561   return name() == vmSymbols::object_initializer_name() || is_static_initializer();
 562 }
 563 
 564 bool Method::has_valid_initializer_flags() const {
 565   return (is_static() ||
 566           method_holder()-&gt;major_version() &lt; 51);
 567 }
 568 
 569 bool Method::is_static_initializer() const {
 570   // For classfiles version 51 or greater, ensure that the clinit method is
 571   // static.  Non-static methods with the name "&lt;clinit&gt;" are not static
 572   // initializers. (older classfiles exempted for backward compatibility)
 573   return name() == vmSymbols::class_initializer_name() &amp;&amp;
 574          has_valid_initializer_flags();
 575 }
 576 
 577 
 578 objArrayHandle Method::resolved_checked_exceptions_impl(Method* method, TRAPS) {
 579   int length = method-&gt;checked_exceptions_length();
 580   if (length == 0) {  // common case
 581     return objArrayHandle(THREAD, Universe::the_empty_class_klass_array());
 582   } else {
 583     methodHandle h_this(THREAD, method);
 584     objArrayOop m_oop = oopFactory::new_objArray(SystemDictionary::Class_klass(), length, CHECK_(objArrayHandle()));
 585     objArrayHandle mirrors (THREAD, m_oop);
 586     for (int i = 0; i &lt; length; i++) {
 587       CheckedExceptionElement* table = h_this-&gt;checked_exceptions_start(); // recompute on each iteration, not gc safe
 588       Klass* k = h_this-&gt;constants()-&gt;klass_at(table[i].class_cp_index, CHECK_(objArrayHandle()));
 589       assert(k-&gt;is_subclass_of(SystemDictionary::Throwable_klass()), "invalid exception class");
 590       mirrors-&gt;obj_at_put(i, k-&gt;java_mirror());
 591     }
 592     return mirrors;
 593   }
 594 };
 595 
 596 
 597 int Method::line_number_from_bci(int bci) const {
 598   if (bci == SynchronizationEntryBCI) bci = 0;
 599   assert(bci == 0 || 0 &lt;= bci &amp;&amp; bci &lt; code_size(), "illegal bci");
 600   int best_bci  =  0;
 601   int best_line = -1;
 602 
 603   if (has_linenumber_table()) {
 604     // The line numbers are a short array of 2-tuples [start_pc, line_number].
 605     // Not necessarily sorted and not necessarily one-to-one.
 606     CompressedLineNumberReadStream stream(compressed_linenumber_table());
 607     while (stream.read_pair()) {
 608       if (stream.bci() == bci) {
 609         // perfect match
 610         return stream.line();
 611       } else {
 612         // update best_bci/line
 613         if (stream.bci() &lt; bci &amp;&amp; stream.bci() &gt;= best_bci) {
 614           best_bci  = stream.bci();
 615           best_line = stream.line();
 616         }
 617       }
 618     }
 619   }
 620   return best_line;
 621 }
 622 
 623 
 624 bool Method::is_klass_loaded_by_klass_index(int klass_index) const {
 625   if( constants()-&gt;tag_at(klass_index).is_unresolved_klass() ) {
 626     Thread *thread = Thread::current();
 627     Symbol* klass_name = constants()-&gt;klass_name_at(klass_index);
 628     Handle loader(thread, method_holder()-&gt;class_loader());
 629     Handle prot  (thread, method_holder()-&gt;protection_domain());
 630     return SystemDictionary::find(klass_name, loader, prot, thread) != NULL;
 631   } else {
 632     return true;
 633   }
 634 }
 635 
 636 
 637 bool Method::is_klass_loaded(int refinfo_index, bool must_be_resolved) const {
 638   int klass_index = constants()-&gt;klass_ref_index_at(refinfo_index);
 639   if (must_be_resolved) {
 640     // Make sure klass is resolved in constantpool.
 641     if (constants()-&gt;tag_at(klass_index).is_unresolved_klass()) return false;
 642   }
 643   return is_klass_loaded_by_klass_index(klass_index);
 644 }
 645 
 646 
 647 void Method::set_native_function(address function, bool post_event_flag) {
 648   assert(function != NULL, "use clear_native_function to unregister natives");
 649   assert(!is_method_handle_intrinsic() || function == SharedRuntime::native_method_throw_unsatisfied_link_error_entry(), "");
 650   address* native_function = native_function_addr();
 651 
 652   // We can see racers trying to place the same native function into place. Once
 653   // is plenty.
 654   address current = *native_function;
 655   if (current == function) return;
 656   if (post_event_flag &amp;&amp; JvmtiExport::should_post_native_method_bind() &amp;&amp;
 657       function != NULL) {
 658     // native_method_throw_unsatisfied_link_error_entry() should only
 659     // be passed when post_event_flag is false.
 660     assert(function !=
 661       SharedRuntime::native_method_throw_unsatisfied_link_error_entry(),
 662       "post_event_flag mis-match");
 663 
 664     // post the bind event, and possible change the bind function
 665     JvmtiExport::post_native_method_bind(this, &amp;function);
 666   }
 667   *native_function = function;
 668   // This function can be called more than once. We must make sure that we always
 669   // use the latest registered method -&gt; check if a stub already has been generated.
 670   // If so, we have to make it not_entrant.
 671   nmethod* nm = code(); // Put it into local variable to guard against concurrent updates
 672   if (nm != NULL) {
 673     nm-&gt;make_not_entrant();
 674   }
 675 }
 676 
 677 
 678 bool Method::has_native_function() const {
 679   if (is_method_handle_intrinsic())
 680     return false;  // special-cased in SharedRuntime::generate_native_wrapper
 681   address func = native_function();
 682   return (func != NULL &amp;&amp; func != SharedRuntime::native_method_throw_unsatisfied_link_error_entry());
 683 }
 684 
 685 
 686 void Method::clear_native_function() {
 687   // Note: is_method_handle_intrinsic() is allowed here.
 688   set_native_function(
 689     SharedRuntime::native_method_throw_unsatisfied_link_error_entry(),
 690     !native_bind_event_is_interesting);
 691   clear_code();
 692 }
 693 
 694 address Method::critical_native_function() {
 695   methodHandle mh(this);
 696   return NativeLookup::lookup_critical_entry(mh);
 697 }
 698 
 699 
 700 void Method::set_signature_handler(address handler) {
 701   address* signature_handler =  signature_handler_addr();
 702   *signature_handler = handler;
 703 }
 704 
 705 
 706 void Method::print_made_not_compilable(int comp_level, bool is_osr, bool report, const char* reason) {
 707   if (PrintCompilation &amp;&amp; report) {
 708     ttyLocker ttyl;
 709     tty-&gt;print("made not %scompilable on ", is_osr ? "OSR " : "");
 710     if (comp_level == CompLevel_all) {
 711       tty-&gt;print("all levels ");
 712     } else {
 713       tty-&gt;print("levels ");
 714       for (int i = (int)CompLevel_none; i &lt;= comp_level; i++) {
 715         tty-&gt;print("%d ", i);
 716       }
 717     }
 718     this-&gt;print_short_name(tty);
 719     int size = this-&gt;code_size();
 720     if (size &gt; 0) {
 721       tty-&gt;print(" (%d bytes)", size);
 722     }
 723     if (reason != NULL) {
 724       tty-&gt;print("   %s", reason);
 725     }
 726     tty-&gt;cr();
 727   }
 728   if ((TraceDeoptimization || LogCompilation) &amp;&amp; (xtty != NULL)) {
 729     ttyLocker ttyl;
 730     xtty-&gt;begin_elem("make_not_%scompilable thread='" UINTX_FORMAT "'",
 731                      is_osr ? "osr_" : "", os::current_thread_id());
 732     if (reason != NULL) {
 733       xtty-&gt;print(" reason=\'%s\'", reason);
 734     }
 735     xtty-&gt;method(this);
 736     xtty-&gt;stamp();
 737     xtty-&gt;end_elem();
 738   }
 739 }
 740 
 741 bool Method::is_always_compilable() const {
 742   // Generated adapters must be compiled
 743   if (is_method_handle_intrinsic() &amp;&amp; is_synthetic()) {
 744     assert(!is_not_c1_compilable(), "sanity check");
 745     assert(!is_not_c2_compilable(), "sanity check");
 746     return true;
 747   }
 748 
 749   return false;
 750 }
 751 
 752 bool Method::is_not_compilable(int comp_level) const {
 753   if (number_of_breakpoints() &gt; 0)
 754     return true;
 755   if (is_always_compilable())
 756     return false;
 757   if (comp_level == CompLevel_any)
 758     return is_not_c1_compilable() || is_not_c2_compilable();
 759   if (is_c1_compile(comp_level))
 760     return is_not_c1_compilable();
 761   if (is_c2_compile(comp_level))
 762     return is_not_c2_compilable();
 763   return false;
 764 }
 765 
 766 // call this when compiler finds that this method is not compilable
 767 void Method::set_not_compilable(int comp_level, bool report, const char* reason) {
 768   if (is_always_compilable()) {
 769     // Don't mark a method which should be always compilable
 770     return;
 771   }
 772   print_made_not_compilable(comp_level, /*is_osr*/ false, report, reason);
 773   if (comp_level == CompLevel_all) {
 774     set_not_c1_compilable();
 775     set_not_c2_compilable();
 776   } else {
 777     if (is_c1_compile(comp_level))
 778       set_not_c1_compilable();
 779     if (is_c2_compile(comp_level))
 780       set_not_c2_compilable();
 781   }
 782   CompilationPolicy::policy()-&gt;disable_compilation(this);
 783   assert(!CompilationPolicy::can_be_compiled(this, comp_level), "sanity check");
 784 }
 785 
 786 bool Method::is_not_osr_compilable(int comp_level) const {
 787   if (is_not_compilable(comp_level))
 788     return true;
 789   if (comp_level == CompLevel_any)
 790     return is_not_c1_osr_compilable() || is_not_c2_osr_compilable();
 791   if (is_c1_compile(comp_level))
 792     return is_not_c1_osr_compilable();
 793   if (is_c2_compile(comp_level))
 794     return is_not_c2_osr_compilable();
 795   return false;
 796 }
 797 
 798 void Method::set_not_osr_compilable(int comp_level, bool report, const char* reason) {
 799   print_made_not_compilable(comp_level, /*is_osr*/ true, report, reason);
 800   if (comp_level == CompLevel_all) {
 801     set_not_c1_osr_compilable();
 802     set_not_c2_osr_compilable();
 803   } else {
 804     if (is_c1_compile(comp_level))
 805       set_not_c1_osr_compilable();
 806     if (is_c2_compile(comp_level))
 807       set_not_c2_osr_compilable();
 808   }
 809   CompilationPolicy::policy()-&gt;disable_compilation(this);
 810   assert(!CompilationPolicy::can_be_osr_compiled(this, comp_level), "sanity check");
 811 }
 812 
 813 // Revert to using the interpreter and clear out the nmethod
 814 void Method::clear_code() {
 815 
 816   // this may be NULL if c2i adapters have not been made yet
 817   // Only should happen at allocate time.
 818   if (_adapter == NULL) {
 819     _from_compiled_entry    = NULL;
 820   } else {
 821     _from_compiled_entry    = _adapter-&gt;get_c2i_entry();
 822   }
 823   OrderAccess::storestore();
 824   _from_interpreted_entry = _i2i_entry;
 825   OrderAccess::storestore();
 826   _code = NULL;
 827 }
 828 
 829 // Called by class data sharing to remove any entry points (which are not shared)
 830 void Method::unlink_method() {
 831   _code = NULL;
 832   _i2i_entry = NULL;
 833   _from_interpreted_entry = NULL;
 834   if (is_native()) {
 835     *native_function_addr() = NULL;
 836     set_signature_handler(NULL);
 837   }
 838   NOT_PRODUCT(set_compiled_invocation_count(0);)
 839   _adapter = NULL;
 840   _from_compiled_entry = NULL;
 841 
 842   // In case of DumpSharedSpaces, _method_data should always be NULL.
 843   //
 844   // During runtime (!DumpSharedSpaces), when we are cleaning a
 845   // shared class that failed to load, this-&gt;link_method() may
 846   // have already been called (before an exception happened), so
 847   // this-&gt;_method_data may not be NULL.
 848   assert(!DumpSharedSpaces || _method_data == NULL, "unexpected method data?");
 849 
 850   set_method_data(NULL);
 851   set_method_counters(NULL);
 852 }
 853 
 854 // Called when the method_holder is getting linked. Setup entrypoints so the method
 855 // is ready to be called from interpreter, compiler, and vtables.
 856 void Method::link_method(methodHandle h_method, TRAPS) {
 857   // If the code cache is full, we may reenter this function for the
 858   // leftover methods that weren't linked.
 859   if (_i2i_entry != NULL) return;
 860 
 861   assert(_adapter == NULL, "init'd to NULL" );
 862   assert( _code == NULL, "nothing compiled yet" );
 863 
 864   // Setup interpreter entrypoint
 865   assert(this == h_method(), "wrong h_method()" );
 866   address entry = Interpreter::entry_for_method(h_method);
 867   assert(entry != NULL, "interpreter entry must be non-null");
 868   // Sets both _i2i_entry and _from_interpreted_entry
 869   set_interpreter_entry(entry);
 870 
 871   // Don't overwrite already registered native entries.
 872   if (is_native() &amp;&amp; !has_native_function()) {
 873     set_native_function(
 874       SharedRuntime::native_method_throw_unsatisfied_link_error_entry(),
 875       !native_bind_event_is_interesting);
 876   }
 877 
 878   // Setup compiler entrypoint.  This is made eagerly, so we do not need
 879   // special handling of vtables.  An alternative is to make adapters more
 880   // lazily by calling make_adapter() from from_compiled_entry() for the
 881   // normal calls.  For vtable calls life gets more complicated.  When a
 882   // call-site goes mega-morphic we need adapters in all methods which can be
 883   // called from the vtable.  We need adapters on such methods that get loaded
 884   // later.  Ditto for mega-morphic itable calls.  If this proves to be a
 885   // problem we'll make these lazily later.
 886   (void) make_adapters(h_method, CHECK);
 887 
 888   // ONLY USE the h_method now as make_adapter may have blocked
 889 
 890 }
 891 
 892 address Method::make_adapters(methodHandle mh, TRAPS) {
 893   // Adapters for compiled code are made eagerly here.  They are fairly
 894   // small (generally &lt; 100 bytes) and quick to make (and cached and shared)
 895   // so making them eagerly shouldn't be too expensive.
 896   AdapterHandlerEntry* adapter = AdapterHandlerLibrary::get_adapter(mh);
 897   if (adapter == NULL ) {
 898     THROW_MSG_NULL(vmSymbols::java_lang_VirtualMachineError(), "out of space in CodeCache for adapters");
 899   }
 900 
 901   mh-&gt;set_adapter_entry(adapter);
 902   mh-&gt;_from_compiled_entry = adapter-&gt;get_c2i_entry();
 903   return adapter-&gt;get_c2i_entry();
 904 }
 905 
 906 void Method::restore_unshareable_info(TRAPS) {
 907   // Since restore_unshareable_info can be called more than once for a method, don't
 908   // redo any work.   If this field is restored, there is nothing to do.
 909   if (_from_compiled_entry == NULL) {
 910     // restore method's vtable by calling a virtual function
 911     restore_vtable();
 912 
 913     methodHandle mh(THREAD, this);
 914     link_method(mh, CHECK);
 915   }
 916 }
 917 
 918 
 919 // The verified_code_entry() must be called when a invoke is resolved
 920 // on this method.
 921 
 922 // It returns the compiled code entry point, after asserting not null.
 923 // This function is called after potential safepoints so that nmethod
 924 // or adapter that it points to is still live and valid.
 925 // This function must not hit a safepoint!
 926 address Method::verified_code_entry() {
 927   debug_only(No_Safepoint_Verifier nsv;)
 928   assert(_from_compiled_entry != NULL, "must be set");
 929   return _from_compiled_entry;
 930 }
 931 
 932 // Check that if an nmethod ref exists, it has a backlink to this or no backlink at all
 933 // (could be racing a deopt).
 934 // Not inline to avoid circular ref.
 935 bool Method::check_code() const {
 936   // cached in a register or local.  There's a race on the value of the field.
 937   nmethod *code = (nmethod *)OrderAccess::load_ptr_acquire(&amp;_code);
 938   return code == NULL || (code-&gt;method() == NULL) || (code-&gt;method() == (Method*)this &amp;&amp; !code-&gt;is_osr_method());
 939 }
 940 
 941 // Install compiled code.  Instantly it can execute.
 942 void Method::set_code(methodHandle mh, nmethod *code) {
 943   assert( code, "use clear_code to remove code" );
 944   assert( mh-&gt;check_code(), "" );
 945 
 946   guarantee(mh-&gt;adapter() != NULL, "Adapter blob must already exist!");
 947 
 948   // These writes must happen in this order, because the interpreter will
 949   // directly jump to from_interpreted_entry which jumps to an i2c adapter
 950   // which jumps to _from_compiled_entry.
 951   mh-&gt;_code = code;             // Assign before allowing compiled code to exec
 952 
 953   int comp_level = code-&gt;comp_level();
 954   // In theory there could be a race here. In practice it is unlikely
 955   // and not worth worrying about.
 956   if (comp_level &gt; mh-&gt;highest_comp_level()) {
 957     mh-&gt;set_highest_comp_level(comp_level);
 958   }
 959 
 960   OrderAccess::storestore();
 961 #ifdef SHARK
 962   mh-&gt;_from_interpreted_entry = code-&gt;insts_begin();
 963 #else //!SHARK
 964   mh-&gt;_from_compiled_entry = code-&gt;verified_entry_point();
 965   OrderAccess::storestore();
 966   // Instantly compiled code can execute.
 967   if (!mh-&gt;is_method_handle_intrinsic())
 968     mh-&gt;_from_interpreted_entry = mh-&gt;get_i2c_entry();
 969 #endif //!SHARK
 970 }
 971 
 972 
 973 bool Method::is_overridden_in(Klass* k) const {
 974   InstanceKlass* ik = InstanceKlass::cast(k);
 975 
 976   if (ik-&gt;is_interface()) return false;
 977 
 978   // If method is an interface, we skip it - except if it
 979   // is a miranda method
 980   if (method_holder()-&gt;is_interface()) {
 981     // Check that method is not a miranda method
 982     if (ik-&gt;lookup_method(name(), signature()) == NULL) {
 983       // No implementation exist - so miranda method
 984       return false;
 985     }
 986     return true;
 987   }
 988 
 989   assert(ik-&gt;is_subclass_of(method_holder()), "should be subklass");
 990   assert(ik-&gt;vtable() != NULL, "vtable should exist");
 991   if (!has_vtable_index()) {
 992     return false;
 993   } else {
 994     Method* vt_m = ik-&gt;method_at_vtable(vtable_index());
 995     return vt_m != this;
 996   }
 997 }
 998 
 999 
1000 // give advice about whether this Method* should be cached or not
1001 bool Method::should_not_be_cached() const {
1002   if (is_old()) {
1003     // This method has been redefined. It is either EMCP or obsolete
1004     // and we don't want to cache it because that would pin the method
1005     // down and prevent it from being collectible if and when it
1006     // finishes executing.
1007     return true;
1008   }
1009 
1010   // caching this method should be just fine
1011   return false;
1012 }
1013 
1014 
1015 /**
1016  *  Returns true if this is one of the specially treated methods for
1017  *  security related stack walks (like Reflection.getCallerClass).
1018  */
1019 bool Method::is_ignored_by_security_stack_walk() const {
<a name="1" id="anc1"></a><span class="removed">1020   const bool use_new_reflection = JDK_Version::is_gte_jdk14x_version() &amp;&amp; UseNewReflection;</span>
<span class="removed">1021 </span>
1022   if (intrinsic_id() == vmIntrinsics::_invoke) {
1023     // This is Method.invoke() -- ignore it
1024     return true;
1025   }
<a name="2" id="anc2"></a><span class="changed">1026   if (use_new_reflection &amp;&amp;</span>
1027       method_holder()-&gt;is_subclass_of(SystemDictionary::reflect_MethodAccessorImpl_klass())) {
1028     // This is an auxilary frame -- ignore it
1029     return true;
1030   }
1031   if (is_method_handle_intrinsic() || is_compiled_lambda_form()) {
1032     // This is an internal adapter frame for method handles -- ignore it
1033     return true;
1034   }
1035   return false;
1036 }
1037 
1038 
1039 // Constant pool structure for invoke methods:
1040 enum {
1041   _imcp_invoke_name = 1,        // utf8: 'invokeExact', etc.
1042   _imcp_invoke_signature,       // utf8: (variable Symbol*)
1043   _imcp_limit
1044 };
1045 
1046 // Test if this method is an MH adapter frame generated by Java code.
1047 // Cf. java/lang/invoke/InvokerBytecodeGenerator
1048 bool Method::is_compiled_lambda_form() const {
1049   return intrinsic_id() == vmIntrinsics::_compiledLambdaForm;
1050 }
1051 
1052 // Test if this method is an internal MH primitive method.
1053 bool Method::is_method_handle_intrinsic() const {
1054   vmIntrinsics::ID iid = intrinsic_id();
1055   return (MethodHandles::is_signature_polymorphic(iid) &amp;&amp;
1056           MethodHandles::is_signature_polymorphic_intrinsic(iid));
1057 }
1058 
1059 bool Method::has_member_arg() const {
1060   vmIntrinsics::ID iid = intrinsic_id();
1061   return (MethodHandles::is_signature_polymorphic(iid) &amp;&amp;
1062           MethodHandles::has_member_arg(iid));
1063 }
1064 
1065 // Make an instance of a signature-polymorphic internal MH primitive.
1066 methodHandle Method::make_method_handle_intrinsic(vmIntrinsics::ID iid,
1067                                                          Symbol* signature,
1068                                                          TRAPS) {
1069   ResourceMark rm;
1070   methodHandle empty;
1071 
1072   KlassHandle holder = SystemDictionary::MethodHandle_klass();
1073   Symbol* name = MethodHandles::signature_polymorphic_intrinsic_name(iid);
1074   assert(iid == MethodHandles::signature_polymorphic_name_id(name), "");
1075   if (TraceMethodHandles) {
1076     tty-&gt;print_cr("make_method_handle_intrinsic MH.%s%s", name-&gt;as_C_string(), signature-&gt;as_C_string());
1077   }
1078 
1079   // invariant:   cp-&gt;symbol_at_put is preceded by a refcount increment (more usually a lookup)
1080   name-&gt;increment_refcount();
1081   signature-&gt;increment_refcount();
1082 
1083   int cp_length = _imcp_limit;
1084   ClassLoaderData* loader_data = holder-&gt;class_loader_data();
1085   constantPoolHandle cp;
1086   {
1087     ConstantPool* cp_oop = ConstantPool::allocate(loader_data, cp_length, CHECK_(empty));
1088     cp = constantPoolHandle(THREAD, cp_oop);
1089   }
1090   cp-&gt;set_pool_holder(InstanceKlass::cast(holder()));
1091   cp-&gt;symbol_at_put(_imcp_invoke_name,       name);
1092   cp-&gt;symbol_at_put(_imcp_invoke_signature,  signature);
1093   cp-&gt;set_has_preresolution();
1094 
1095   // decide on access bits:  public or not?
1096   int flags_bits = (JVM_ACC_NATIVE | JVM_ACC_SYNTHETIC | JVM_ACC_FINAL);
1097   bool must_be_static = MethodHandles::is_signature_polymorphic_static(iid);
1098   if (must_be_static)  flags_bits |= JVM_ACC_STATIC;
1099   assert((flags_bits &amp; JVM_ACC_PUBLIC) == 0, "do not expose these methods");
1100 
1101   methodHandle m;
1102   {
1103     InlineTableSizes sizes;
1104     Method* m_oop = Method::allocate(loader_data, 0,
1105                                      accessFlags_from(flags_bits), &amp;sizes,
1106                                      ConstMethod::NORMAL, CHECK_(empty));
1107     m = methodHandle(THREAD, m_oop);
1108   }
1109   m-&gt;set_constants(cp());
1110   m-&gt;set_name_index(_imcp_invoke_name);
1111   m-&gt;set_signature_index(_imcp_invoke_signature);
1112   assert(MethodHandles::is_signature_polymorphic_name(m-&gt;name()), "");
1113   assert(m-&gt;signature() == signature, "");
1114 #ifdef CC_INTERP
1115   ResultTypeFinder rtf(signature);
1116   m-&gt;set_result_index(rtf.type());
1117 #endif
1118   m-&gt;compute_size_of_parameters(THREAD);
1119   m-&gt;init_intrinsic_id();
1120   assert(m-&gt;is_method_handle_intrinsic(), "");
1121 #ifdef ASSERT
1122   if (!MethodHandles::is_signature_polymorphic(m-&gt;intrinsic_id()))  m-&gt;print();
1123   assert(MethodHandles::is_signature_polymorphic(m-&gt;intrinsic_id()), "must be an invoker");
1124   assert(m-&gt;intrinsic_id() == iid, "correctly predicted iid");
1125 #endif //ASSERT
1126 
1127   // Finally, set up its entry points.
1128   assert(m-&gt;can_be_statically_bound(), "");
1129   m-&gt;set_vtable_index(Method::nonvirtual_vtable_index);
1130   m-&gt;link_method(m, CHECK_(empty));
1131 
1132   if (TraceMethodHandles &amp;&amp; (Verbose || WizardMode))
1133     m-&gt;print_on(tty);
1134 
1135   return m;
1136 }
1137 
1138 Klass* Method::check_non_bcp_klass(Klass* klass) {
1139   if (klass != NULL &amp;&amp; klass-&gt;class_loader() != NULL) {
1140     if (klass-&gt;oop_is_objArray())
1141       klass = ObjArrayKlass::cast(klass)-&gt;bottom_klass();
1142     return klass;
1143   }
1144   return NULL;
1145 }
1146 
1147 
1148 methodHandle Method::clone_with_new_data(methodHandle m, u_char* new_code, int new_code_length,
1149                                                 u_char* new_compressed_linenumber_table, int new_compressed_linenumber_size, TRAPS) {
1150   // Code below does not work for native methods - they should never get rewritten anyway
1151   assert(!m-&gt;is_native(), "cannot rewrite native methods");
1152   // Allocate new Method*
1153   AccessFlags flags = m-&gt;access_flags();
1154 
1155   ConstMethod* cm = m-&gt;constMethod();
1156   int checked_exceptions_len = cm-&gt;checked_exceptions_length();
1157   int localvariable_len = cm-&gt;localvariable_table_length();
1158   int exception_table_len = cm-&gt;exception_table_length();
1159   int method_parameters_len = cm-&gt;method_parameters_length();
1160   int method_annotations_len = cm-&gt;method_annotations_length();
1161   int parameter_annotations_len = cm-&gt;parameter_annotations_length();
1162   int type_annotations_len = cm-&gt;type_annotations_length();
1163   int default_annotations_len = cm-&gt;default_annotations_length();
1164 
1165   InlineTableSizes sizes(
1166       localvariable_len,
1167       new_compressed_linenumber_size,
1168       exception_table_len,
1169       checked_exceptions_len,
1170       method_parameters_len,
1171       cm-&gt;generic_signature_index(),
1172       method_annotations_len,
1173       parameter_annotations_len,
1174       type_annotations_len,
1175       default_annotations_len,
1176       0);
1177 
1178   ClassLoaderData* loader_data = m-&gt;method_holder()-&gt;class_loader_data();
1179   Method* newm_oop = Method::allocate(loader_data,
1180                                       new_code_length,
1181                                       flags,
1182                                       &amp;sizes,
1183                                       m-&gt;method_type(),
1184                                       CHECK_(methodHandle()));
1185   methodHandle newm (THREAD, newm_oop);
1186   int new_method_size = newm-&gt;method_size();
1187 
1188   // Create a shallow copy of Method part, but be careful to preserve the new ConstMethod*
1189   ConstMethod* newcm = newm-&gt;constMethod();
1190   int new_const_method_size = newm-&gt;constMethod()-&gt;size();
1191 
1192   memcpy(newm(), m(), sizeof(Method));
1193 
1194   // Create shallow copy of ConstMethod.
1195   memcpy(newcm, m-&gt;constMethod(), sizeof(ConstMethod));
1196 
1197   // Reset correct method/const method, method size, and parameter info
1198   newm-&gt;set_constMethod(newcm);
1199   newm-&gt;constMethod()-&gt;set_code_size(new_code_length);
1200   newm-&gt;constMethod()-&gt;set_constMethod_size(new_const_method_size);
1201   newm-&gt;set_method_size(new_method_size);
1202   assert(newm-&gt;code_size() == new_code_length, "check");
1203   assert(newm-&gt;method_parameters_length() == method_parameters_len, "check");
1204   assert(newm-&gt;checked_exceptions_length() == checked_exceptions_len, "check");
1205   assert(newm-&gt;exception_table_length() == exception_table_len, "check");
1206   assert(newm-&gt;localvariable_table_length() == localvariable_len, "check");
1207   // Copy new byte codes
1208   memcpy(newm-&gt;code_base(), new_code, new_code_length);
1209   // Copy line number table
1210   if (new_compressed_linenumber_size &gt; 0) {
1211     memcpy(newm-&gt;compressed_linenumber_table(),
1212            new_compressed_linenumber_table,
1213            new_compressed_linenumber_size);
1214   }
1215   // Copy method_parameters
1216   if (method_parameters_len &gt; 0) {
1217     memcpy(newm-&gt;method_parameters_start(),
1218            m-&gt;method_parameters_start(),
1219            method_parameters_len * sizeof(MethodParametersElement));
1220   }
1221   // Copy checked_exceptions
1222   if (checked_exceptions_len &gt; 0) {
1223     memcpy(newm-&gt;checked_exceptions_start(),
1224            m-&gt;checked_exceptions_start(),
1225            checked_exceptions_len * sizeof(CheckedExceptionElement));
1226   }
1227   // Copy exception table
1228   if (exception_table_len &gt; 0) {
1229     memcpy(newm-&gt;exception_table_start(),
1230            m-&gt;exception_table_start(),
1231            exception_table_len * sizeof(ExceptionTableElement));
1232   }
1233   // Copy local variable number table
1234   if (localvariable_len &gt; 0) {
1235     memcpy(newm-&gt;localvariable_table_start(),
1236            m-&gt;localvariable_table_start(),
1237            localvariable_len * sizeof(LocalVariableTableElement));
1238   }
1239   // Copy stackmap table
1240   if (m-&gt;has_stackmap_table()) {
1241     int code_attribute_length = m-&gt;stackmap_data()-&gt;length();
1242     Array&lt;u1&gt;* stackmap_data =
1243       MetadataFactory::new_array&lt;u1&gt;(loader_data, code_attribute_length, 0, CHECK_NULL);
1244     memcpy((void*)stackmap_data-&gt;adr_at(0),
1245            (void*)m-&gt;stackmap_data()-&gt;adr_at(0), code_attribute_length);
1246     newm-&gt;set_stackmap_data(stackmap_data);
1247   }
1248 
1249   // copy annotations over to new method
1250   newcm-&gt;copy_annotations_from(cm);
1251   return newm;
1252 }
1253 
1254 vmSymbols::SID Method::klass_id_for_intrinsics(Klass* holder) {
1255   // if loader is not the default loader (i.e., != NULL), we can't know the intrinsics
1256   // because we are not loading from core libraries
1257   // exception: the AES intrinsics come from lib/ext/sunjce_provider.jar
1258   // which does not use the class default class loader so we check for its loader here
1259   InstanceKlass* ik = InstanceKlass::cast(holder);
1260   if ((ik-&gt;class_loader() != NULL) &amp;&amp; !SystemDictionary::is_ext_class_loader(ik-&gt;class_loader())) {
1261     return vmSymbols::NO_SID;   // regardless of name, no intrinsics here
1262   }
1263 
1264   // see if the klass name is well-known:
1265   Symbol* klass_name = ik-&gt;name();
1266   return vmSymbols::find_sid(klass_name);
1267 }
1268 
1269 void Method::init_intrinsic_id() {
1270   assert(_intrinsic_id == vmIntrinsics::_none, "do this just once");
1271   const uintptr_t max_id_uint = right_n_bits((int)(sizeof(_intrinsic_id) * BitsPerByte));
1272   assert((uintptr_t)vmIntrinsics::ID_LIMIT &lt;= max_id_uint, "else fix size");
1273   assert(intrinsic_id_size_in_bytes() == sizeof(_intrinsic_id), "");
1274 
1275   // the klass name is well-known:
1276   vmSymbols::SID klass_id = klass_id_for_intrinsics(method_holder());
1277   assert(klass_id != vmSymbols::NO_SID, "caller responsibility");
1278 
1279   // ditto for method and signature:
1280   vmSymbols::SID  name_id = vmSymbols::find_sid(name());
1281   if (klass_id != vmSymbols::VM_SYMBOL_ENUM_NAME(java_lang_invoke_MethodHandle)
1282       &amp;&amp; name_id == vmSymbols::NO_SID)
1283     return;
1284   vmSymbols::SID   sig_id = vmSymbols::find_sid(signature());
1285   if (klass_id != vmSymbols::VM_SYMBOL_ENUM_NAME(java_lang_invoke_MethodHandle)
1286       &amp;&amp; sig_id == vmSymbols::NO_SID)  return;
1287   jshort flags = access_flags().as_short();
1288 
1289   vmIntrinsics::ID id = vmIntrinsics::find_id(klass_id, name_id, sig_id, flags);
1290   if (id != vmIntrinsics::_none) {
1291     set_intrinsic_id(id);
1292     return;
1293   }
1294 
1295   // A few slightly irregular cases:
1296   switch (klass_id) {
1297   case vmSymbols::VM_SYMBOL_ENUM_NAME(java_lang_StrictMath):
1298     // Second chance: check in regular Math.
1299     switch (name_id) {
1300     case vmSymbols::VM_SYMBOL_ENUM_NAME(min_name):
1301     case vmSymbols::VM_SYMBOL_ENUM_NAME(max_name):
1302     case vmSymbols::VM_SYMBOL_ENUM_NAME(sqrt_name):
1303       // pretend it is the corresponding method in the non-strict class:
1304       klass_id = vmSymbols::VM_SYMBOL_ENUM_NAME(java_lang_Math);
1305       id = vmIntrinsics::find_id(klass_id, name_id, sig_id, flags);
1306       break;
1307     }
1308     break;
1309 
1310   // Signature-polymorphic methods: MethodHandle.invoke*, InvokeDynamic.*.
1311   case vmSymbols::VM_SYMBOL_ENUM_NAME(java_lang_invoke_MethodHandle):
1312     if (!is_native())  break;
1313     id = MethodHandles::signature_polymorphic_name_id(method_holder(), name());
1314     if (is_static() != MethodHandles::is_signature_polymorphic_static(id))
1315       id = vmIntrinsics::_none;
1316     break;
1317   }
1318 
1319   if (id != vmIntrinsics::_none) {
1320     // Set up its iid.  It is an alias method.
1321     set_intrinsic_id(id);
1322     return;
1323   }
1324 }
1325 
1326 // These two methods are static since a GC may move the Method
1327 bool Method::load_signature_classes(methodHandle m, TRAPS) {
1328   if (THREAD-&gt;is_Compiler_thread()) {
1329     // There is nothing useful this routine can do from within the Compile thread.
1330     // Hopefully, the signature contains only well-known classes.
1331     // We could scan for this and return true/false, but the caller won't care.
1332     return false;
1333   }
1334   bool sig_is_loaded = true;
1335   Handle class_loader(THREAD, m-&gt;method_holder()-&gt;class_loader());
1336   Handle protection_domain(THREAD, m-&gt;method_holder()-&gt;protection_domain());
1337   ResourceMark rm(THREAD);
1338   Symbol*  signature = m-&gt;signature();
1339   for(SignatureStream ss(signature); !ss.is_done(); ss.next()) {
1340     if (ss.is_object()) {
1341       Symbol* sym = ss.as_symbol(CHECK_(false));
1342       Symbol*  name  = sym;
1343       Klass* klass = SystemDictionary::resolve_or_null(name, class_loader,
1344                                              protection_domain, THREAD);
1345       // We are loading classes eagerly. If a ClassNotFoundException or
1346       // a LinkageError was generated, be sure to ignore it.
1347       if (HAS_PENDING_EXCEPTION) {
1348         if (PENDING_EXCEPTION-&gt;is_a(SystemDictionary::ClassNotFoundException_klass()) ||
1349             PENDING_EXCEPTION-&gt;is_a(SystemDictionary::LinkageError_klass())) {
1350           CLEAR_PENDING_EXCEPTION;
1351         } else {
1352           return false;
1353         }
1354       }
1355       if( klass == NULL) { sig_is_loaded = false; }
1356     }
1357   }
1358   return sig_is_loaded;
1359 }
1360 
1361 bool Method::has_unloaded_classes_in_signature(methodHandle m, TRAPS) {
1362   Handle class_loader(THREAD, m-&gt;method_holder()-&gt;class_loader());
1363   Handle protection_domain(THREAD, m-&gt;method_holder()-&gt;protection_domain());
1364   ResourceMark rm(THREAD);
1365   Symbol*  signature = m-&gt;signature();
1366   for(SignatureStream ss(signature); !ss.is_done(); ss.next()) {
1367     if (ss.type() == T_OBJECT) {
1368       Symbol* name = ss.as_symbol_or_null();
1369       if (name == NULL) return true;
1370       Klass* klass = SystemDictionary::find(name, class_loader, protection_domain, THREAD);
1371       if (klass == NULL) return true;
1372     }
1373   }
1374   return false;
1375 }
1376 
1377 // Exposed so field engineers can debug VM
1378 void Method::print_short_name(outputStream* st) {
1379   ResourceMark rm;
1380 #ifdef PRODUCT
1381   st-&gt;print(" %s::", method_holder()-&gt;external_name());
1382 #else
1383   st-&gt;print(" %s::", method_holder()-&gt;internal_name());
1384 #endif
1385   name()-&gt;print_symbol_on(st);
1386   if (WizardMode) signature()-&gt;print_symbol_on(st);
1387   else if (MethodHandles::is_signature_polymorphic(intrinsic_id()))
1388     MethodHandles::print_as_basic_type_signature_on(st, signature(), true);
1389 }
1390 
1391 // Comparer for sorting an object array containing
1392 // Method*s.
1393 static int method_comparator(Method* a, Method* b) {
1394   return a-&gt;name()-&gt;fast_compare(b-&gt;name());
1395 }
1396 
1397 // This is only done during class loading, so it is OK to assume method_idnum matches the methods() array
1398 // default_methods also uses this without the ordering for fast find_method
1399 void Method::sort_methods(Array&lt;Method*&gt;* methods, bool idempotent, bool set_idnums) {
1400   int length = methods-&gt;length();
1401   if (length &gt; 1) {
1402     {
1403       No_Safepoint_Verifier nsv;
1404       QuickSort::sort&lt;Method*&gt;(methods-&gt;data(), length, method_comparator, idempotent);
1405     }
1406     // Reset method ordering
1407     if (set_idnums) {
1408       for (int i = 0; i &lt; length; i++) {
1409         Method* m = methods-&gt;at(i);
1410         m-&gt;set_method_idnum(i);
1411       }
1412     }
1413   }
1414 }
1415 
1416 //-----------------------------------------------------------------------------------
1417 // Non-product code unless JVM/TI needs it
1418 
1419 #if !defined(PRODUCT) || INCLUDE_JVMTI
1420 class SignatureTypePrinter : public SignatureTypeNames {
1421  private:
1422   outputStream* _st;
1423   bool _use_separator;
1424 
1425   void type_name(const char* name) {
1426     if (_use_separator) _st-&gt;print(", ");
1427     _st-&gt;print(name);
1428     _use_separator = true;
1429   }
1430 
1431  public:
1432   SignatureTypePrinter(Symbol* signature, outputStream* st) : SignatureTypeNames(signature) {
1433     _st = st;
1434     _use_separator = false;
1435   }
1436 
1437   void print_parameters()              { _use_separator = false; iterate_parameters(); }
1438   void print_returntype()              { _use_separator = false; iterate_returntype(); }
1439 };
1440 
1441 
1442 void Method::print_name(outputStream* st) {
1443   Thread *thread = Thread::current();
1444   ResourceMark rm(thread);
1445   SignatureTypePrinter sig(signature(), st);
1446   st-&gt;print("%s ", is_static() ? "static" : "virtual");
1447   sig.print_returntype();
1448   st-&gt;print(" %s.", method_holder()-&gt;internal_name());
1449   name()-&gt;print_symbol_on(st);
1450   st-&gt;print("(");
1451   sig.print_parameters();
1452   st-&gt;print(")");
1453 }
1454 #endif // !PRODUCT || INCLUDE_JVMTI
1455 
1456 
1457 void Method::print_codes_on(outputStream* st) const {
1458   print_codes_on(0, code_size(), st);
1459 }
1460 
1461 void Method::print_codes_on(int from, int to, outputStream* st) const {
1462   Thread *thread = Thread::current();
1463   ResourceMark rm(thread);
1464   methodHandle mh (thread, (Method*)this);
1465   BytecodeStream s(mh);
1466   s.set_interval(from, to);
1467   BytecodeTracer::set_closure(BytecodeTracer::std_closure());
1468   while (s.next() &gt;= 0) BytecodeTracer::trace(mh, s.bcp(), st);
1469 }
1470 
1471 
1472 // Simple compression of line number tables. We use a regular compressed stream, except that we compress deltas
1473 // between (bci,line) pairs since they are smaller. If (bci delta, line delta) fits in (5-bit unsigned, 3-bit unsigned)
1474 // we save it as one byte, otherwise we write a 0xFF escape character and use regular compression. 0x0 is used
1475 // as end-of-stream terminator.
1476 
1477 void CompressedLineNumberWriteStream::write_pair_regular(int bci_delta, int line_delta) {
1478   // bci and line number does not compress into single byte.
1479   // Write out escape character and use regular compression for bci and line number.
1480   write_byte((jubyte)0xFF);
1481   write_signed_int(bci_delta);
1482   write_signed_int(line_delta);
1483 }
1484 
1485 // See comment in method.hpp which explains why this exists.
1486 #if defined(_M_AMD64) &amp;&amp; _MSC_VER &gt;= 1400
1487 #pragma optimize("", off)
1488 void CompressedLineNumberWriteStream::write_pair(int bci, int line) {
1489   write_pair_inline(bci, line);
1490 }
1491 #pragma optimize("", on)
1492 #endif
1493 
1494 CompressedLineNumberReadStream::CompressedLineNumberReadStream(u_char* buffer) : CompressedReadStream(buffer) {
1495   _bci = 0;
1496   _line = 0;
1497 };
1498 
1499 
1500 bool CompressedLineNumberReadStream::read_pair() {
1501   jubyte next = read_byte();
1502   // Check for terminator
1503   if (next == 0) return false;
1504   if (next == 0xFF) {
1505     // Escape character, regular compression used
1506     _bci  += read_signed_int();
1507     _line += read_signed_int();
1508   } else {
1509     // Single byte compression used
1510     _bci  += next &gt;&gt; 3;
1511     _line += next &amp; 0x7;
1512   }
1513   return true;
1514 }
1515 
1516 
1517 Bytecodes::Code Method::orig_bytecode_at(int bci) const {
1518   BreakpointInfo* bp = method_holder()-&gt;breakpoints();
1519   for (; bp != NULL; bp = bp-&gt;next()) {
1520     if (bp-&gt;match(this, bci)) {
1521       return bp-&gt;orig_bytecode();
1522     }
1523   }
1524   {
1525     ResourceMark rm;
1526     fatal(err_msg("no original bytecode found in %s at bci %d", name_and_sig_as_C_string(), bci));
1527   }
1528   return Bytecodes::_shouldnotreachhere;
1529 }
1530 
1531 void Method::set_orig_bytecode_at(int bci, Bytecodes::Code code) {
1532   assert(code != Bytecodes::_breakpoint, "cannot patch breakpoints this way");
1533   BreakpointInfo* bp = method_holder()-&gt;breakpoints();
1534   for (; bp != NULL; bp = bp-&gt;next()) {
1535     if (bp-&gt;match(this, bci)) {
1536       bp-&gt;set_orig_bytecode(code);
1537       // and continue, in case there is more than one
1538     }
1539   }
1540 }
1541 
1542 void Method::set_breakpoint(int bci) {
1543   InstanceKlass* ik = method_holder();
1544   BreakpointInfo *bp = new BreakpointInfo(this, bci);
1545   bp-&gt;set_next(ik-&gt;breakpoints());
1546   ik-&gt;set_breakpoints(bp);
1547   // do this last:
1548   bp-&gt;set(this);
1549 }
1550 
1551 static void clear_matches(Method* m, int bci) {
1552   InstanceKlass* ik = m-&gt;method_holder();
1553   BreakpointInfo* prev_bp = NULL;
1554   BreakpointInfo* next_bp;
1555   for (BreakpointInfo* bp = ik-&gt;breakpoints(); bp != NULL; bp = next_bp) {
1556     next_bp = bp-&gt;next();
1557     // bci value of -1 is used to delete all breakpoints in method m (ex: clear_all_breakpoint).
1558     if (bci &gt;= 0 ? bp-&gt;match(m, bci) : bp-&gt;match(m)) {
1559       // do this first:
1560       bp-&gt;clear(m);
1561       // unhook it
1562       if (prev_bp != NULL)
1563         prev_bp-&gt;set_next(next_bp);
1564       else
1565         ik-&gt;set_breakpoints(next_bp);
1566       delete bp;
1567       // When class is redefined JVMTI sets breakpoint in all versions of EMCP methods
1568       // at same location. So we have multiple matching (method_index and bci)
1569       // BreakpointInfo nodes in BreakpointInfo list. We should just delete one
1570       // breakpoint for clear_breakpoint request and keep all other method versions
1571       // BreakpointInfo for future clear_breakpoint request.
1572       // bcivalue of -1 is used to clear all breakpoints (see clear_all_breakpoints)
1573       // which is being called when class is unloaded. We delete all the Breakpoint
1574       // information for all versions of method. We may not correctly restore the original
1575       // bytecode in all method versions, but that is ok. Because the class is being unloaded
1576       // so these methods won't be used anymore.
1577       if (bci &gt;= 0) {
1578         break;
1579       }
1580     } else {
1581       // This one is a keeper.
1582       prev_bp = bp;
1583     }
1584   }
1585 }
1586 
1587 void Method::clear_breakpoint(int bci) {
1588   assert(bci &gt;= 0, "");
1589   clear_matches(this, bci);
1590 }
1591 
1592 void Method::clear_all_breakpoints() {
1593   clear_matches(this, -1);
1594 }
1595 
1596 
1597 int Method::invocation_count() {
1598   MethodCounters *mcs = method_counters();
1599   if (TieredCompilation) {
1600     MethodData* const mdo = method_data();
1601     if (((mcs != NULL) ? mcs-&gt;invocation_counter()-&gt;carry() : false) ||
1602         ((mdo != NULL) ? mdo-&gt;invocation_counter()-&gt;carry() : false)) {
1603       return InvocationCounter::count_limit;
1604     } else {
1605       return ((mcs != NULL) ? mcs-&gt;invocation_counter()-&gt;count() : 0) +
1606              ((mdo != NULL) ? mdo-&gt;invocation_counter()-&gt;count() : 0);
1607     }
1608   } else {
1609     return (mcs == NULL) ? 0 : mcs-&gt;invocation_counter()-&gt;count();
1610   }
1611 }
1612 
1613 int Method::backedge_count() {
1614   MethodCounters *mcs = method_counters();
1615   if (TieredCompilation) {
1616     MethodData* const mdo = method_data();
1617     if (((mcs != NULL) ? mcs-&gt;backedge_counter()-&gt;carry() : false) ||
1618         ((mdo != NULL) ? mdo-&gt;backedge_counter()-&gt;carry() : false)) {
1619       return InvocationCounter::count_limit;
1620     } else {
1621       return ((mcs != NULL) ? mcs-&gt;backedge_counter()-&gt;count() : 0) +
1622              ((mdo != NULL) ? mdo-&gt;backedge_counter()-&gt;count() : 0);
1623     }
1624   } else {
1625     return (mcs == NULL) ? 0 : mcs-&gt;backedge_counter()-&gt;count();
1626   }
1627 }
1628 
1629 int Method::highest_comp_level() const {
1630   const MethodData* mdo = method_data();
1631   if (mdo != NULL) {
1632     return mdo-&gt;highest_comp_level();
1633   } else {
1634     return CompLevel_none;
1635   }
1636 }
1637 
1638 int Method::highest_osr_comp_level() const {
1639   const MethodData* mdo = method_data();
1640   if (mdo != NULL) {
1641     return mdo-&gt;highest_osr_comp_level();
1642   } else {
1643     return CompLevel_none;
1644   }
1645 }
1646 
1647 void Method::set_highest_comp_level(int level) {
1648   MethodData* mdo = method_data();
1649   if (mdo != NULL) {
1650     mdo-&gt;set_highest_comp_level(level);
1651   }
1652 }
1653 
1654 void Method::set_highest_osr_comp_level(int level) {
1655   MethodData* mdo = method_data();
1656   if (mdo != NULL) {
1657     mdo-&gt;set_highest_osr_comp_level(level);
1658   }
1659 }
1660 
1661 BreakpointInfo::BreakpointInfo(Method* m, int bci) {
1662   _bci = bci;
1663   _name_index = m-&gt;name_index();
1664   _signature_index = m-&gt;signature_index();
1665   _orig_bytecode = (Bytecodes::Code) *m-&gt;bcp_from(_bci);
1666   if (_orig_bytecode == Bytecodes::_breakpoint)
1667     _orig_bytecode = m-&gt;orig_bytecode_at(_bci);
1668   _next = NULL;
1669 }
1670 
1671 void BreakpointInfo::set(Method* method) {
1672 #ifdef ASSERT
1673   {
1674     Bytecodes::Code code = (Bytecodes::Code) *method-&gt;bcp_from(_bci);
1675     if (code == Bytecodes::_breakpoint)
1676       code = method-&gt;orig_bytecode_at(_bci);
1677     assert(orig_bytecode() == code, "original bytecode must be the same");
1678   }
1679 #endif
1680   Thread *thread = Thread::current();
1681   *method-&gt;bcp_from(_bci) = Bytecodes::_breakpoint;
1682   method-&gt;incr_number_of_breakpoints(thread);
1683   SystemDictionary::notice_modification();
1684   {
1685     // Deoptimize all dependents on this method
1686     HandleMark hm(thread);
1687     methodHandle mh(thread, method);
1688     Universe::flush_dependents_on_method(mh);
1689   }
1690 }
1691 
1692 void BreakpointInfo::clear(Method* method) {
1693   *method-&gt;bcp_from(_bci) = orig_bytecode();
1694   assert(method-&gt;number_of_breakpoints() &gt; 0, "must not go negative");
1695   method-&gt;decr_number_of_breakpoints(Thread::current());
1696 }
1697 
1698 // jmethodID handling
1699 
1700 // This is a block allocating object, sort of like JNIHandleBlock, only a
1701 // lot simpler.  There aren't many of these, they aren't long, they are rarely
1702 // deleted and so we can do some suboptimal things.
1703 // It's allocated on the CHeap because once we allocate a jmethodID, we can
1704 // never get rid of it.
1705 // It would be nice to be able to parameterize the number of methods for
1706 // the null_class_loader but then we'd have to turn this and ClassLoaderData
1707 // into templates.
1708 
1709 // I feel like this brain dead class should exist somewhere in the STL
1710 
1711 class JNIMethodBlock : public CHeapObj&lt;mtClass&gt; {
1712   enum { number_of_methods = 8 };
1713 
1714   Method*         _methods[number_of_methods];
1715   int             _top;
1716   JNIMethodBlock* _next;
1717  public:
1718   static Method* const _free_method;
1719 
1720   JNIMethodBlock() : _next(NULL), _top(0) {
1721     for (int i = 0; i&lt; number_of_methods; i++) _methods[i] = _free_method;
1722   }
1723 
1724   Method** add_method(Method* m) {
1725     if (_top &lt; number_of_methods) {
1726       // top points to the next free entry.
1727       int i = _top;
1728       _methods[i] = m;
1729       _top++;
1730       return &amp;_methods[i];
1731     } else if (_top == number_of_methods) {
1732       // if the next free entry ran off the block see if there's a free entry
1733       for (int i = 0; i&lt; number_of_methods; i++) {
1734         if (_methods[i] == _free_method) {
1735           _methods[i] = m;
1736           return &amp;_methods[i];
1737         }
1738       }
1739       // Only check each block once for frees.  They're very unlikely.
1740       // Increment top past the end of the block.
1741       _top++;
1742     }
1743     // need to allocate a next block.
1744     if (_next == NULL) {
1745       _next = new JNIMethodBlock();
1746     }
1747     return _next-&gt;add_method(m);
1748   }
1749 
1750   bool contains(Method** m) {
1751     for (JNIMethodBlock* b = this; b != NULL; b = b-&gt;_next) {
1752       for (int i = 0; i&lt; number_of_methods; i++) {
1753         if (&amp;(b-&gt;_methods[i]) == m) {
1754           return true;
1755         }
1756       }
1757     }
1758     return false;  // not found
1759   }
1760 
1761   // Doesn't really destroy it, just marks it as free so it can be reused.
1762   void destroy_method(Method** m) {
1763 #ifdef ASSERT
1764     assert(contains(m), "should be a methodID");
1765 #endif // ASSERT
1766     *m = _free_method;
1767   }
1768 
1769   // During class unloading the methods are cleared, which is different
1770   // than freed.
1771   void clear_all_methods() {
1772     for (JNIMethodBlock* b = this; b != NULL; b = b-&gt;_next) {
1773       for (int i = 0; i&lt; number_of_methods; i++) {
1774         _methods[i] = NULL;
1775       }
1776     }
1777   }
1778 #ifndef PRODUCT
1779   int count_methods() {
1780     // count all allocated methods
1781     int count = 0;
1782     for (JNIMethodBlock* b = this; b != NULL; b = b-&gt;_next) {
1783       for (int i = 0; i&lt; number_of_methods; i++) {
1784         if (_methods[i] != _free_method) count++;
1785       }
1786     }
1787     return count;
1788   }
1789 #endif // PRODUCT
1790 };
1791 
1792 // Something that can't be mistaken for an address or a markOop
1793 Method* const JNIMethodBlock::_free_method = (Method*)55;
1794 
1795 // Add a method id to the jmethod_ids
1796 jmethodID Method::make_jmethod_id(ClassLoaderData* loader_data, Method* m) {
1797   ClassLoaderData* cld = loader_data;
1798 
1799   if (!SafepointSynchronize::is_at_safepoint()) {
1800     // Have to add jmethod_ids() to class loader data thread-safely.
1801     // Also have to add the method to the list safely, which the cld lock
1802     // protects as well.
1803     MutexLockerEx ml(cld-&gt;metaspace_lock(),  Mutex::_no_safepoint_check_flag);
1804     if (cld-&gt;jmethod_ids() == NULL) {
1805       cld-&gt;set_jmethod_ids(new JNIMethodBlock());
1806     }
1807     // jmethodID is a pointer to Method*
1808     return (jmethodID)cld-&gt;jmethod_ids()-&gt;add_method(m);
1809   } else {
1810     // At safepoint, we are single threaded and can set this.
1811     if (cld-&gt;jmethod_ids() == NULL) {
1812       cld-&gt;set_jmethod_ids(new JNIMethodBlock());
1813     }
1814     // jmethodID is a pointer to Method*
1815     return (jmethodID)cld-&gt;jmethod_ids()-&gt;add_method(m);
1816   }
1817 }
1818 
1819 // Mark a jmethodID as free.  This is called when there is a data race in
1820 // InstanceKlass while creating the jmethodID cache.
1821 void Method::destroy_jmethod_id(ClassLoaderData* loader_data, jmethodID m) {
1822   ClassLoaderData* cld = loader_data;
1823   Method** ptr = (Method**)m;
1824   assert(cld-&gt;jmethod_ids() != NULL, "should have method handles");
1825   cld-&gt;jmethod_ids()-&gt;destroy_method(ptr);
1826 }
1827 
1828 void Method::change_method_associated_with_jmethod_id(jmethodID jmid, Method* new_method) {
1829   // Can't assert the method_holder is the same because the new method has the
1830   // scratch method holder.
1831   assert(resolve_jmethod_id(jmid)-&gt;method_holder()-&gt;class_loader()
1832            == new_method-&gt;method_holder()-&gt;class_loader(),
1833          "changing to a different class loader");
1834   // Just change the method in place, jmethodID pointer doesn't change.
1835   *((Method**)jmid) = new_method;
1836 }
1837 
1838 bool Method::is_method_id(jmethodID mid) {
1839   Method* m = resolve_jmethod_id(mid);
1840   assert(m != NULL, "should be called with non-null method");
1841   InstanceKlass* ik = m-&gt;method_holder();
1842   ClassLoaderData* cld = ik-&gt;class_loader_data();
1843   if (cld-&gt;jmethod_ids() == NULL) return false;
1844   return (cld-&gt;jmethod_ids()-&gt;contains((Method**)mid));
1845 }
1846 
1847 Method* Method::checked_resolve_jmethod_id(jmethodID mid) {
1848   if (mid == NULL) return NULL;
1849   Method* o = resolve_jmethod_id(mid);
1850   if (o == NULL || o == JNIMethodBlock::_free_method || !((Metadata*)o)-&gt;is_method()) {
1851     return NULL;
1852   }
1853   return o;
1854 };
1855 
1856 void Method::set_on_stack(const bool value) {
1857   // Set both the method itself and its constant pool.  The constant pool
1858   // on stack means some method referring to it is also on the stack.
1859   _access_flags.set_on_stack(value);
1860   constants()-&gt;set_on_stack(value);
1861   if (value) MetadataOnStackMark::record(this);
1862 }
1863 
1864 // Called when the class loader is unloaded to make all methods weak.
1865 void Method::clear_jmethod_ids(ClassLoaderData* loader_data) {
1866   loader_data-&gt;jmethod_ids()-&gt;clear_all_methods();
1867 }
1868 
1869 
1870 // Check that this pointer is valid by checking that the vtbl pointer matches
1871 bool Method::is_valid_method() const {
1872   if (this == NULL) {
1873     return false;
1874   } else if (!is_metaspace_object()) {
1875     return false;
1876   } else {
1877     Method m;
1878     // This assumes that the vtbl pointer is the first word of a C++ object.
1879     // This assumption is also in universe.cpp patch_klass_vtble
1880     void* vtbl2 = dereference_vptr((void*)&amp;m);
1881     void* this_vtbl = dereference_vptr((void*)this);
1882     return vtbl2 == this_vtbl;
1883   }
1884 }
1885 
1886 #ifndef PRODUCT
1887 void Method::print_jmethod_ids(ClassLoaderData* loader_data, outputStream* out) {
1888   out-&gt;print_cr("jni_method_id count = %d", loader_data-&gt;jmethod_ids()-&gt;count_methods());
1889 }
1890 #endif // PRODUCT
1891 
1892 
1893 // Printing
1894 
1895 #ifndef PRODUCT
1896 
1897 void Method::print_on(outputStream* st) const {
1898   ResourceMark rm;
1899   assert(is_method(), "must be method");
1900   st-&gt;print_cr(internal_name());
1901   // get the effect of PrintOopAddress, always, for methods:
1902   st-&gt;print_cr(" - this oop:          "INTPTR_FORMAT, (intptr_t)this);
1903   st-&gt;print   (" - method holder:     "); method_holder()-&gt;print_value_on(st); st-&gt;cr();
1904   st-&gt;print   (" - constants:         "INTPTR_FORMAT" ", (address)constants());
1905   constants()-&gt;print_value_on(st); st-&gt;cr();
1906   st-&gt;print   (" - access:            0x%x  ", access_flags().as_int()); access_flags().print_on(st); st-&gt;cr();
1907   st-&gt;print   (" - name:              ");    name()-&gt;print_value_on(st); st-&gt;cr();
1908   st-&gt;print   (" - signature:         ");    signature()-&gt;print_value_on(st); st-&gt;cr();
1909   st-&gt;print_cr(" - max stack:         %d",   max_stack());
1910   st-&gt;print_cr(" - max locals:        %d",   max_locals());
1911   st-&gt;print_cr(" - size of params:    %d",   size_of_parameters());
1912   st-&gt;print_cr(" - method size:       %d",   method_size());
1913   if (intrinsic_id() != vmIntrinsics::_none)
1914     st-&gt;print_cr(" - intrinsic id:      %d %s", intrinsic_id(), vmIntrinsics::name_at(intrinsic_id()));
1915   if (highest_comp_level() != CompLevel_none)
1916     st-&gt;print_cr(" - highest level:     %d", highest_comp_level());
1917   st-&gt;print_cr(" - vtable index:      %d",   _vtable_index);
1918   st-&gt;print_cr(" - i2i entry:         " INTPTR_FORMAT, interpreter_entry());
1919   st-&gt;print(   " - adapters:          ");
1920   AdapterHandlerEntry* a = ((Method*)this)-&gt;adapter();
1921   if (a == NULL)
1922     st-&gt;print_cr(INTPTR_FORMAT, a);
1923   else
1924     a-&gt;print_adapter_on(st);
1925   st-&gt;print_cr(" - compiled entry     " INTPTR_FORMAT, from_compiled_entry());
1926   st-&gt;print_cr(" - code size:         %d",   code_size());
1927   if (code_size() != 0) {
1928     st-&gt;print_cr(" - code start:        " INTPTR_FORMAT, code_base());
1929     st-&gt;print_cr(" - code end (excl):   " INTPTR_FORMAT, code_base() + code_size());
1930   }
1931   if (method_data() != NULL) {
1932     st-&gt;print_cr(" - method data:       " INTPTR_FORMAT, (address)method_data());
1933   }
1934   st-&gt;print_cr(" - checked ex length: %d",   checked_exceptions_length());
1935   if (checked_exceptions_length() &gt; 0) {
1936     CheckedExceptionElement* table = checked_exceptions_start();
1937     st-&gt;print_cr(" - checked ex start:  " INTPTR_FORMAT, table);
1938     if (Verbose) {
1939       for (int i = 0; i &lt; checked_exceptions_length(); i++) {
1940         st-&gt;print_cr("   - throws %s", constants()-&gt;printable_name_at(table[i].class_cp_index));
1941       }
1942     }
1943   }
1944   if (has_linenumber_table()) {
1945     u_char* table = compressed_linenumber_table();
1946     st-&gt;print_cr(" - linenumber start:  " INTPTR_FORMAT, table);
1947     if (Verbose) {
1948       CompressedLineNumberReadStream stream(table);
1949       while (stream.read_pair()) {
1950         st-&gt;print_cr("   - line %d: %d", stream.line(), stream.bci());
1951       }
1952     }
1953   }
1954   st-&gt;print_cr(" - localvar length:   %d",   localvariable_table_length());
1955   if (localvariable_table_length() &gt; 0) {
1956     LocalVariableTableElement* table = localvariable_table_start();
1957     st-&gt;print_cr(" - localvar start:    " INTPTR_FORMAT, table);
1958     if (Verbose) {
1959       for (int i = 0; i &lt; localvariable_table_length(); i++) {
1960         int bci = table[i].start_bci;
1961         int len = table[i].length;
1962         const char* name = constants()-&gt;printable_name_at(table[i].name_cp_index);
1963         const char* desc = constants()-&gt;printable_name_at(table[i].descriptor_cp_index);
1964         int slot = table[i].slot;
1965         st-&gt;print_cr("   - %s %s bci=%d len=%d slot=%d", desc, name, bci, len, slot);
1966       }
1967     }
1968   }
1969   if (code() != NULL) {
1970     st-&gt;print   (" - compiled code: ");
1971     code()-&gt;print_value_on(st);
1972   }
1973   if (is_native()) {
1974     st-&gt;print_cr(" - native function:   " INTPTR_FORMAT, native_function());
1975     st-&gt;print_cr(" - signature handler: " INTPTR_FORMAT, signature_handler());
1976   }
1977 }
1978 
1979 #endif //PRODUCT
1980 
1981 void Method::print_value_on(outputStream* st) const {
1982   assert(is_method(), "must be method");
1983   st-&gt;print(internal_name());
1984   print_address_on(st);
1985   st-&gt;print(" ");
1986   name()-&gt;print_value_on(st);
1987   st-&gt;print(" ");
1988   signature()-&gt;print_value_on(st);
1989   st-&gt;print(" in ");
1990   method_holder()-&gt;print_value_on(st);
1991   if (WizardMode) st-&gt;print("#%d", _vtable_index);
1992   if (WizardMode) st-&gt;print("[%d,%d]", size_of_parameters(), max_locals());
1993   if (WizardMode &amp;&amp; code() != NULL) st-&gt;print(" ((nmethod*)%p)", code());
1994 }
1995 
1996 #if INCLUDE_SERVICES
1997 // Size Statistics
1998 void Method::collect_statistics(KlassSizeStats *sz) const {
1999   int mysize = sz-&gt;count(this);
2000   sz-&gt;_method_bytes += mysize;
2001   sz-&gt;_method_all_bytes += mysize;
2002   sz-&gt;_rw_bytes += mysize;
2003 
2004   if (constMethod()) {
2005     constMethod()-&gt;collect_statistics(sz);
2006   }
2007   if (method_data()) {
2008     method_data()-&gt;collect_statistics(sz);
2009   }
2010 }
2011 #endif // INCLUDE_SERVICES
2012 
2013 // Verification
2014 
2015 void Method::verify_on(outputStream* st) {
2016   guarantee(is_method(), "object must be method");
2017   guarantee(constants()-&gt;is_constantPool(), "should be constant pool");
2018   guarantee(constMethod()-&gt;is_constMethod(), "should be ConstMethod*");
2019   MethodData* md = method_data();
2020   guarantee(md == NULL ||
2021       md-&gt;is_methodData(), "should be method data");
2022 }
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="3" type="hidden" /></form></body></html>

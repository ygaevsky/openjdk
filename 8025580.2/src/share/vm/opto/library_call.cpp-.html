<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/share/vm/opto/library_call.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1999, 2013, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "classfile/vmSymbols.hpp"
  28 #include "compiler/compileBroker.hpp"
  29 #include "compiler/compileLog.hpp"
  30 #include "oops/objArrayKlass.hpp"
  31 #include "opto/addnode.hpp"
  32 #include "opto/callGenerator.hpp"
  33 #include "opto/castnode.hpp"
  34 #include "opto/cfgnode.hpp"
  35 #include "opto/convertnode.hpp"
  36 #include "opto/countbitsnode.hpp"
  37 #include "opto/intrinsicnode.hpp"
  38 #include "opto/idealKit.hpp"
  39 #include "opto/mathexactnode.hpp"
  40 #include "opto/movenode.hpp"
  41 #include "opto/mulnode.hpp"
  42 #include "opto/narrowptrnode.hpp"
  43 #include "opto/parse.hpp"
  44 #include "opto/runtime.hpp"
  45 #include "opto/subnode.hpp"
  46 #include "prims/nativeLookup.hpp"
  47 #include "runtime/sharedRuntime.hpp"
  48 #include "trace/traceMacros.hpp"
  49 
  50 class LibraryIntrinsic : public InlineCallGenerator {
  51   // Extend the set of intrinsics known to the runtime:
  52  public:
  53  private:
  54   bool             _is_virtual;
  55   bool             _is_predicted;
  56   bool             _does_virtual_dispatch;
  57   vmIntrinsics::ID _intrinsic_id;
  58 
  59  public:
  60   LibraryIntrinsic(ciMethod* m, bool is_virtual, bool is_predicted, bool does_virtual_dispatch, vmIntrinsics::ID id)
  61     : InlineCallGenerator(m),
  62       _is_virtual(is_virtual),
  63       _is_predicted(is_predicted),
  64       _does_virtual_dispatch(does_virtual_dispatch),
  65       _intrinsic_id(id)
  66   {
  67   }
  68   virtual bool is_intrinsic() const { return true; }
  69   virtual bool is_virtual()   const { return _is_virtual; }
  70   virtual bool is_predicted()   const { return _is_predicted; }
  71   virtual bool does_virtual_dispatch()   const { return _does_virtual_dispatch; }
  72   virtual JVMState* generate(JVMState* jvms, Parse* parent_parser);
  73   virtual Node* generate_predicate(JVMState* jvms);
  74   vmIntrinsics::ID intrinsic_id() const { return _intrinsic_id; }
  75 };
  76 
  77 
  78 // Local helper class for LibraryIntrinsic:
  79 class LibraryCallKit : public GraphKit {
  80  private:
  81   LibraryIntrinsic* _intrinsic;     // the library intrinsic being called
  82   Node*             _result;        // the result node, if any
  83   int               _reexecute_sp;  // the stack pointer when bytecode needs to be reexecuted
  84 
  85   const TypeOopPtr* sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr = false);
  86 
  87  public:
  88   LibraryCallKit(JVMState* jvms, LibraryIntrinsic* intrinsic)
  89     : GraphKit(jvms),
  90       _intrinsic(intrinsic),
  91       _result(NULL)
  92   {
  93     // Check if this is a root compile.  In that case we don't have a caller.
  94     if (!jvms-&gt;has_method()) {
  95       _reexecute_sp = sp();
  96     } else {
  97       // Find out how many arguments the interpreter needs when deoptimizing
  98       // and save the stack pointer value so it can used by uncommon_trap.
  99       // We find the argument count by looking at the declared signature.
 100       bool ignored_will_link;
 101       ciSignature* declared_signature = NULL;
 102       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 103       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 104       _reexecute_sp = sp() + nargs;  // "push" arguments back on stack
 105     }
 106   }
 107 
 108   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 109 
 110   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 111   int               bci()       const    { return jvms()-&gt;bci(); }
 112   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 113   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 114   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 115 
 116   bool try_to_inline();
 117   Node* try_to_predicate();
 118 
 119   void push_result() {
 120     // Push the result onto the stack.
 121     if (!stopped() &amp;&amp; result() != NULL) {
 122       BasicType bt = result()-&gt;bottom_type()-&gt;basic_type();
 123       push_node(bt, result());
 124     }
 125   }
 126 
 127  private:
 128   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 129     fatal(err_msg_res("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid)));
 130   }
 131 
 132   void  set_result(Node* n) { assert(_result == NULL, "only set once"); _result = n; }
 133   void  set_result(RegionNode* region, PhiNode* value);
 134   Node*     result() { return _result; }
 135 
 136   virtual int reexecute_sp() { return _reexecute_sp; }
 137 
 138   // Helper functions to inline natives
 139   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 140   Node* generate_slow_guard(Node* test, RegionNode* region);
 141   Node* generate_fair_guard(Node* test, RegionNode* region);
 142   Node* generate_negative_guard(Node* index, RegionNode* region,
 143                                 // resulting CastII of index:
 144                                 Node* *pos_index = NULL);
 145   Node* generate_nonpositive_guard(Node* index, bool never_negative,
 146                                    // resulting CastII of index:
 147                                    Node* *pos_index = NULL);
 148   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 149                              Node* array_length,
 150                              RegionNode* region);
 151   Node* generate_current_thread(Node* &amp;tls_output);
 152   address basictype2arraycopy(BasicType t, Node *src_offset, Node *dest_offset,
 153                               bool disjoint_bases, const char* &amp;name, bool dest_uninitialized);
 154   Node* load_mirror_from_klass(Node* klass);
 155   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 156                                       RegionNode* region, int null_path,
 157                                       int offset);
 158   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 159                                RegionNode* region, int null_path) {
 160     int offset = java_lang_Class::klass_offset_in_bytes();
 161     return load_klass_from_mirror_common(mirror, never_see_null,
 162                                          region, null_path,
 163                                          offset);
 164   }
 165   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 166                                      RegionNode* region, int null_path) {
 167     int offset = java_lang_Class::array_klass_offset_in_bytes();
 168     return load_klass_from_mirror_common(mirror, never_see_null,
 169                                          region, null_path,
 170                                          offset);
 171   }
 172   Node* generate_access_flags_guard(Node* kls,
 173                                     int modifier_mask, int modifier_bits,
 174                                     RegionNode* region);
 175   Node* generate_interface_guard(Node* kls, RegionNode* region);
 176   Node* generate_array_guard(Node* kls, RegionNode* region) {
 177     return generate_array_guard_common(kls, region, false, false);
 178   }
 179   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 180     return generate_array_guard_common(kls, region, false, true);
 181   }
 182   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 183     return generate_array_guard_common(kls, region, true, false);
 184   }
 185   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 186     return generate_array_guard_common(kls, region, true, true);
 187   }
 188   Node* generate_array_guard_common(Node* kls, RegionNode* region,
 189                                     bool obj_array, bool not_array);
 190   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 191   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 192                                      bool is_virtual = false, bool is_static = false);
 193   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 194     return generate_method_call(method_id, false, true);
 195   }
 196   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 197     return generate_method_call(method_id, true, false);
 198   }
 199   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static);
 200 
 201   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2);
 202   Node* make_string_method_node(int opcode, Node* str1, Node* str2);
 203   bool inline_string_compareTo();
 204   bool inline_string_indexOf();
 205   Node* string_indexOf(Node* string_object, ciTypeArray* target_array, jint offset, jint cache_i, jint md2_i);
 206   bool inline_string_equals();
 207   Node* round_double_node(Node* n);
 208   bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);
 209   bool inline_math_native(vmIntrinsics::ID id);
 210   bool inline_trig(vmIntrinsics::ID id);
 211   bool inline_math(vmIntrinsics::ID id);
 212   template &lt;typename OverflowOp&gt;
 213   bool inline_math_overflow(Node* arg1, Node* arg2);
 214   void inline_math_mathExact(Node* math, Node* test);
 215   bool inline_math_addExactI(bool is_increment);
 216   bool inline_math_addExactL(bool is_increment);
 217   bool inline_math_multiplyExactI();
 218   bool inline_math_multiplyExactL();
 219   bool inline_math_negateExactI();
 220   bool inline_math_negateExactL();
 221   bool inline_math_subtractExactI(bool is_decrement);
 222   bool inline_math_subtractExactL(bool is_decrement);
 223   bool inline_exp();
 224   bool inline_pow();
 225   void finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName);
 226   bool inline_min_max(vmIntrinsics::ID id);
 227   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 228   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 229   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset);
 230   Node* make_unsafe_address(Node* base, Node* offset);
 231   // Helper for inline_unsafe_access.
 232   // Generates the guards that check whether the result of
 233   // Unsafe.getObject should be recorded in an SATB log buffer.
 234   void insert_pre_barrier(Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar);
 235   bool inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile);
 236   bool inline_unsafe_prefetch(bool is_native_ptr, bool is_store, bool is_static);
 237   static bool klass_needs_init_guard(Node* kls);
 238   bool inline_unsafe_allocate();
 239   bool inline_unsafe_copyMemory();
 240   bool inline_native_currentThread();
 241 #ifdef TRACE_HAVE_INTRINSICS
 242   bool inline_native_classID();
 243   bool inline_native_threadID();
 244 #endif
 245   bool inline_native_time_funcs(address method, const char* funcName);
 246   bool inline_native_isInterrupted();
 247   bool inline_native_Class_query(vmIntrinsics::ID id);
 248   bool inline_native_subtype_check();
 249 
 250   bool inline_native_newArray();
 251   bool inline_native_getLength();
 252   bool inline_array_copyOf(bool is_copyOfRange);
 253   bool inline_array_equals();
 254   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark);
 255   bool inline_native_clone(bool is_virtual);
 256   bool inline_native_Reflection_getCallerClass();
 257   // Helper function for inlining native object hash method
 258   bool inline_native_hashcode(bool is_virtual, bool is_static);
 259   bool inline_native_getClass();
 260 
 261   // Helper functions for inlining arraycopy
 262   bool inline_arraycopy();
 263   void generate_arraycopy(const TypePtr* adr_type,
 264                           BasicType basic_elem_type,
 265                           Node* src,  Node* src_offset,
 266                           Node* dest, Node* dest_offset,
 267                           Node* copy_length,
 268                           bool disjoint_bases = false,
 269                           bool length_never_negative = false,
 270                           RegionNode* slow_region = NULL);
 271   AllocateArrayNode* tightly_coupled_allocation(Node* ptr,
 272                                                 RegionNode* slow_region);
 273   void generate_clear_array(const TypePtr* adr_type,
 274                             Node* dest,
 275                             BasicType basic_elem_type,
 276                             Node* slice_off,
 277                             Node* slice_len,
 278                             Node* slice_end);
 279   bool generate_block_arraycopy(const TypePtr* adr_type,
 280                                 BasicType basic_elem_type,
 281                                 AllocateNode* alloc,
 282                                 Node* src,  Node* src_offset,
 283                                 Node* dest, Node* dest_offset,
 284                                 Node* dest_size, bool dest_uninitialized);
 285   void generate_slow_arraycopy(const TypePtr* adr_type,
 286                                Node* src,  Node* src_offset,
 287                                Node* dest, Node* dest_offset,
 288                                Node* copy_length, bool dest_uninitialized);
 289   Node* generate_checkcast_arraycopy(const TypePtr* adr_type,
 290                                      Node* dest_elem_klass,
 291                                      Node* src,  Node* src_offset,
 292                                      Node* dest, Node* dest_offset,
 293                                      Node* copy_length, bool dest_uninitialized);
 294   Node* generate_generic_arraycopy(const TypePtr* adr_type,
 295                                    Node* src,  Node* src_offset,
 296                                    Node* dest, Node* dest_offset,
 297                                    Node* copy_length, bool dest_uninitialized);
 298   void generate_unchecked_arraycopy(const TypePtr* adr_type,
 299                                     BasicType basic_elem_type,
 300                                     bool disjoint_bases,
 301                                     Node* src,  Node* src_offset,
 302                                     Node* dest, Node* dest_offset,
 303                                     Node* copy_length, bool dest_uninitialized);
 304   typedef enum { LS_xadd, LS_xchg, LS_cmpxchg } LoadStoreKind;
 305   bool inline_unsafe_load_store(BasicType type,  LoadStoreKind kind);
 306   bool inline_unsafe_ordered_store(BasicType type);
 307   bool inline_unsafe_fence(vmIntrinsics::ID id);
 308   bool inline_fp_conversions(vmIntrinsics::ID id);
 309   bool inline_number_methods(vmIntrinsics::ID id);
 310   bool inline_reference_get();
 311   bool inline_aescrypt_Block(vmIntrinsics::ID id);
 312   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
 313   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
 314   Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);
 315   Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);
 316   bool inline_encodeISOArray();
 317   bool inline_updateCRC32();
 318   bool inline_updateBytesCRC32();
 319   bool inline_updateByteBufferCRC32();
 320 };
 321 
 322 
 323 //---------------------------make_vm_intrinsic----------------------------
 324 CallGenerator* Compile::make_vm_intrinsic(ciMethod* m, bool is_virtual) {
 325   vmIntrinsics::ID id = m-&gt;intrinsic_id();
 326   assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
 327 
 328   if (DisableIntrinsic[0] != '\0'
 329       &amp;&amp; strstr(DisableIntrinsic, vmIntrinsics::name_at(id)) != NULL) {
 330     // disabled by a user request on the command line:
 331     // example: -XX:DisableIntrinsic=_hashCode,_getClass
 332     return NULL;
 333   }
 334 
 335   if (!m-&gt;is_loaded()) {
 336     // do not attempt to inline unloaded methods
 337     return NULL;
 338   }
 339 
 340   // Only a few intrinsics implement a virtual dispatch.
 341   // They are expensive calls which are also frequently overridden.
 342   if (is_virtual) {
 343     switch (id) {
 344     case vmIntrinsics::_hashCode:
 345     case vmIntrinsics::_clone:
 346       // OK, Object.hashCode and Object.clone intrinsics come in both flavors
 347       break;
 348     default:
 349       return NULL;
 350     }
 351   }
 352 
 353   // -XX:-InlineNatives disables nearly all intrinsics:
 354   if (!InlineNatives) {
 355     switch (id) {
 356     case vmIntrinsics::_indexOf:
 357     case vmIntrinsics::_compareTo:
 358     case vmIntrinsics::_equals:
 359     case vmIntrinsics::_equalsC:
 360     case vmIntrinsics::_getAndAddInt:
 361     case vmIntrinsics::_getAndAddLong:
 362     case vmIntrinsics::_getAndSetInt:
 363     case vmIntrinsics::_getAndSetLong:
 364     case vmIntrinsics::_getAndSetObject:
 365     case vmIntrinsics::_loadFence:
 366     case vmIntrinsics::_storeFence:
 367     case vmIntrinsics::_fullFence:
 368       break;  // InlineNatives does not control String.compareTo
 369     case vmIntrinsics::_Reference_get:
 370       break;  // InlineNatives does not control Reference.get
 371     default:
 372       return NULL;
 373     }
 374   }
 375 
 376   bool is_predicted = false;
 377   bool does_virtual_dispatch = false;
 378 
 379   switch (id) {
 380   case vmIntrinsics::_compareTo:
 381     if (!SpecialStringCompareTo)  return NULL;
 382     if (!Matcher::match_rule_supported(Op_StrComp))  return NULL;
 383     break;
 384   case vmIntrinsics::_indexOf:
 385     if (!SpecialStringIndexOf)  return NULL;
 386     break;
 387   case vmIntrinsics::_equals:
 388     if (!SpecialStringEquals)  return NULL;
 389     if (!Matcher::match_rule_supported(Op_StrEquals))  return NULL;
 390     break;
 391   case vmIntrinsics::_equalsC:
 392     if (!SpecialArraysEquals)  return NULL;
 393     if (!Matcher::match_rule_supported(Op_AryEq))  return NULL;
 394     break;
 395   case vmIntrinsics::_arraycopy:
 396     if (!InlineArrayCopy)  return NULL;
 397     break;
 398   case vmIntrinsics::_copyMemory:
 399     if (StubRoutines::unsafe_arraycopy() == NULL)  return NULL;
 400     if (!InlineArrayCopy)  return NULL;
 401     break;
 402   case vmIntrinsics::_hashCode:
 403     if (!InlineObjectHash)  return NULL;
 404     does_virtual_dispatch = true;
 405     break;
 406   case vmIntrinsics::_clone:
 407     does_virtual_dispatch = true;
 408   case vmIntrinsics::_copyOf:
 409   case vmIntrinsics::_copyOfRange:
 410     if (!InlineObjectCopy)  return NULL;
 411     // These also use the arraycopy intrinsic mechanism:
 412     if (!InlineArrayCopy)  return NULL;
 413     break;
 414   case vmIntrinsics::_encodeISOArray:
 415     if (!SpecialEncodeISOArray)  return NULL;
 416     if (!Matcher::match_rule_supported(Op_EncodeISOArray))  return NULL;
 417     break;
 418   case vmIntrinsics::_checkIndex:
 419     // We do not intrinsify this.  The optimizer does fine with it.
 420     return NULL;
 421 
 422   case vmIntrinsics::_getCallerClass:
 423     if (!UseNewReflection)  return NULL;
 424     if (!InlineReflectionGetCallerClass)  return NULL;
 425     if (SystemDictionary::reflect_CallerSensitive_klass() == NULL)  return NULL;
 426     break;
 427 
 428   case vmIntrinsics::_bitCount_i:
 429     if (!Matcher::match_rule_supported(Op_PopCountI)) return NULL;
 430     break;
 431 
 432   case vmIntrinsics::_bitCount_l:
 433     if (!Matcher::match_rule_supported(Op_PopCountL)) return NULL;
 434     break;
 435 
 436   case vmIntrinsics::_numberOfLeadingZeros_i:
 437     if (!Matcher::match_rule_supported(Op_CountLeadingZerosI)) return NULL;
 438     break;
 439 
 440   case vmIntrinsics::_numberOfLeadingZeros_l:
 441     if (!Matcher::match_rule_supported(Op_CountLeadingZerosL)) return NULL;
 442     break;
 443 
 444   case vmIntrinsics::_numberOfTrailingZeros_i:
 445     if (!Matcher::match_rule_supported(Op_CountTrailingZerosI)) return NULL;
 446     break;
 447 
 448   case vmIntrinsics::_numberOfTrailingZeros_l:
 449     if (!Matcher::match_rule_supported(Op_CountTrailingZerosL)) return NULL;
 450     break;
 451 
 452   case vmIntrinsics::_reverseBytes_c:
 453     if (!Matcher::match_rule_supported(Op_ReverseBytesUS)) return NULL;
 454     break;
 455   case vmIntrinsics::_reverseBytes_s:
 456     if (!Matcher::match_rule_supported(Op_ReverseBytesS))  return NULL;
 457     break;
 458   case vmIntrinsics::_reverseBytes_i:
 459     if (!Matcher::match_rule_supported(Op_ReverseBytesI))  return NULL;
 460     break;
 461   case vmIntrinsics::_reverseBytes_l:
 462     if (!Matcher::match_rule_supported(Op_ReverseBytesL))  return NULL;
 463     break;
 464 
 465   case vmIntrinsics::_Reference_get:
 466     // Use the intrinsic version of Reference.get() so that the value in
 467     // the referent field can be registered by the G1 pre-barrier code.
 468     // Also add memory barrier to prevent commoning reads from this field
 469     // across safepoint since GC can change it value.
 470     break;
 471 
 472   case vmIntrinsics::_compareAndSwapObject:
 473 #ifdef _LP64
 474     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_CompareAndSwapP)) return NULL;
 475 #endif
 476     break;
 477 
 478   case vmIntrinsics::_compareAndSwapLong:
 479     if (!Matcher::match_rule_supported(Op_CompareAndSwapL)) return NULL;
 480     break;
 481 
 482   case vmIntrinsics::_getAndAddInt:
 483     if (!Matcher::match_rule_supported(Op_GetAndAddI)) return NULL;
 484     break;
 485 
 486   case vmIntrinsics::_getAndAddLong:
 487     if (!Matcher::match_rule_supported(Op_GetAndAddL)) return NULL;
 488     break;
 489 
 490   case vmIntrinsics::_getAndSetInt:
 491     if (!Matcher::match_rule_supported(Op_GetAndSetI)) return NULL;
 492     break;
 493 
 494   case vmIntrinsics::_getAndSetLong:
 495     if (!Matcher::match_rule_supported(Op_GetAndSetL)) return NULL;
 496     break;
 497 
 498   case vmIntrinsics::_getAndSetObject:
 499 #ifdef _LP64
 500     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 501     if (UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetN)) return NULL;
 502     break;
 503 #else
 504     if (!Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 505     break;
 506 #endif
 507 
 508   case vmIntrinsics::_aescrypt_encryptBlock:
 509   case vmIntrinsics::_aescrypt_decryptBlock:
 510     if (!UseAESIntrinsics) return NULL;
 511     break;
 512 
 513   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 514   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 515     if (!UseAESIntrinsics) return NULL;
 516     // these two require the predicated logic
 517     is_predicted = true;
 518     break;
 519 
 520   case vmIntrinsics::_updateCRC32:
 521   case vmIntrinsics::_updateBytesCRC32:
 522   case vmIntrinsics::_updateByteBufferCRC32:
 523     if (!UseCRC32Intrinsics) return NULL;
 524     break;
 525 
 526   case vmIntrinsics::_incrementExactI:
 527   case vmIntrinsics::_addExactI:
 528     if (!Matcher::match_rule_supported(Op_OverflowAddI) || !UseMathExactIntrinsics) return NULL;
 529     break;
 530   case vmIntrinsics::_incrementExactL:
 531   case vmIntrinsics::_addExactL:
 532     if (!Matcher::match_rule_supported(Op_OverflowAddL) || !UseMathExactIntrinsics) return NULL;
 533     break;
 534   case vmIntrinsics::_decrementExactI:
 535   case vmIntrinsics::_subtractExactI:
 536     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 537     break;
 538   case vmIntrinsics::_decrementExactL:
 539   case vmIntrinsics::_subtractExactL:
 540     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 541     break;
 542   case vmIntrinsics::_negateExactI:
 543     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 544     break;
 545   case vmIntrinsics::_negateExactL:
 546     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 547     break;
 548   case vmIntrinsics::_multiplyExactI:
 549     if (!Matcher::match_rule_supported(Op_OverflowMulI) || !UseMathExactIntrinsics) return NULL;
 550     break;
 551   case vmIntrinsics::_multiplyExactL:
 552     if (!Matcher::match_rule_supported(Op_OverflowMulL) || !UseMathExactIntrinsics) return NULL;
 553     break;
 554 
 555  default:
 556     assert(id &lt;= vmIntrinsics::LAST_COMPILER_INLINE, "caller responsibility");
 557     assert(id != vmIntrinsics::_Object_init &amp;&amp; id != vmIntrinsics::_invoke, "enum out of order?");
 558     break;
 559   }
 560 
 561   // -XX:-InlineClassNatives disables natives from the Class class.
 562   // The flag applies to all reflective calls, notably Array.newArray
 563   // (visible to Java programmers as Array.newInstance).
 564   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Class() ||
 565       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_reflect_Array()) {
 566     if (!InlineClassNatives)  return NULL;
 567   }
 568 
 569   // -XX:-InlineThreadNatives disables natives from the Thread class.
 570   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Thread()) {
 571     if (!InlineThreadNatives)  return NULL;
 572   }
 573 
 574   // -XX:-InlineMathNatives disables natives from the Math,Float and Double classes.
 575   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Math() ||
 576       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Float() ||
 577       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Double()) {
 578     if (!InlineMathNatives)  return NULL;
 579   }
 580 
 581   // -XX:-InlineUnsafeOps disables natives from the Unsafe class.
 582   if (m-&gt;holder()-&gt;name() == ciSymbol::sun_misc_Unsafe()) {
 583     if (!InlineUnsafeOps)  return NULL;
 584   }
 585 
 586   return new LibraryIntrinsic(m, is_virtual, is_predicted, does_virtual_dispatch, (vmIntrinsics::ID) id);
 587 }
 588 
 589 //----------------------register_library_intrinsics-----------------------
 590 // Initialize this file's data structures, for each Compile instance.
 591 void Compile::register_library_intrinsics() {
 592   // Nothing to do here.
 593 }
 594 
 595 JVMState* LibraryIntrinsic::generate(JVMState* jvms, Parse* parent_parser) {
 596   LibraryCallKit kit(jvms, this);
 597   Compile* C = kit.C;
 598   int nodes = C-&gt;unique();
 599 #ifndef PRODUCT
 600   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 601     char buf[1000];
 602     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 603     tty-&gt;print_cr("Intrinsic %s", str);
 604   }
 605 #endif
 606   ciMethod* callee = kit.callee();
 607   const int bci    = kit.bci();
 608 
 609   // Try to inline the intrinsic.
 610   if (kit.try_to_inline()) {
 611     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 612       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 613     }
 614     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 615     if (C-&gt;log()) {
 616       C-&gt;log()-&gt;elem("intrinsic id='%s'%s nodes='%d'",
 617                      vmIntrinsics::name_at(intrinsic_id()),
 618                      (is_virtual() ? " virtual='1'" : ""),
 619                      C-&gt;unique() - nodes);
 620     }
 621     // Push the result from the inlined method onto the stack.
 622     kit.push_result();
 623     C-&gt;print_inlining_update(this);
 624     return kit.transfer_exceptions_into_jvms();
 625   }
 626 
 627   // The intrinsic bailed out
 628   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 629     if (jvms-&gt;has_method()) {
 630       // Not a root compile.
 631       const char* msg = is_virtual() ? "failed to inline (intrinsic, virtual)" : "failed to inline (intrinsic)";
 632       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, msg);
 633     } else {
 634       // Root compile
 635       tty-&gt;print("Did not generate intrinsic %s%s at bci:%d in",
 636                vmIntrinsics::name_at(intrinsic_id()),
 637                (is_virtual() ? " (virtual)" : ""), bci);
 638     }
 639   }
 640   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 641   C-&gt;print_inlining_update(this);
 642   return NULL;
 643 }
 644 
 645 Node* LibraryIntrinsic::generate_predicate(JVMState* jvms) {
 646   LibraryCallKit kit(jvms, this);
 647   Compile* C = kit.C;
 648   int nodes = C-&gt;unique();
 649 #ifndef PRODUCT
 650   assert(is_predicted(), "sanity");
 651   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 652     char buf[1000];
 653     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 654     tty-&gt;print_cr("Predicate for intrinsic %s", str);
 655   }
 656 #endif
 657   ciMethod* callee = kit.callee();
 658   const int bci    = kit.bci();
 659 
 660   Node* slow_ctl = kit.try_to_predicate();
 661   if (!kit.failing()) {
 662     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 663       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 664     }
 665     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 666     if (C-&gt;log()) {
 667       C-&gt;log()-&gt;elem("predicate_intrinsic id='%s'%s nodes='%d'",
 668                      vmIntrinsics::name_at(intrinsic_id()),
 669                      (is_virtual() ? " virtual='1'" : ""),
 670                      C-&gt;unique() - nodes);
 671     }
 672     return slow_ctl; // Could be NULL if the check folds.
 673   }
 674 
 675   // The intrinsic bailed out
 676   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 677     if (jvms-&gt;has_method()) {
 678       // Not a root compile.
 679       const char* msg = "failed to generate predicate for intrinsic";
 680       C-&gt;print_inlining(kit.callee(), jvms-&gt;depth() - 1, bci, msg);
 681     } else {
 682       // Root compile
 683       C-&gt;print_inlining_stream()-&gt;print("Did not generate predicate for intrinsic %s%s at bci:%d in",
 684                                         vmIntrinsics::name_at(intrinsic_id()),
 685                                         (is_virtual() ? " (virtual)" : ""), bci);
 686     }
 687   }
 688   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 689   return NULL;
 690 }
 691 
 692 bool LibraryCallKit::try_to_inline() {
 693   // Handle symbolic names for otherwise undistinguished boolean switches:
 694   const bool is_store       = true;
 695   const bool is_native_ptr  = true;
 696   const bool is_static      = true;
 697   const bool is_volatile    = true;
 698 
 699   if (!jvms()-&gt;has_method()) {
 700     // Root JVMState has a null method.
 701     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 702     // Insert the memory aliasing node
 703     set_all_memory(reset_memory());
 704   }
 705   assert(merged_memory(), "");
 706 
 707 
 708   switch (intrinsic_id()) {
 709   case vmIntrinsics::_hashCode:                 return inline_native_hashcode(intrinsic()-&gt;is_virtual(), !is_static);
 710   case vmIntrinsics::_identityHashCode:         return inline_native_hashcode(/*!virtual*/ false,         is_static);
 711   case vmIntrinsics::_getClass:                 return inline_native_getClass();
 712 
 713   case vmIntrinsics::_dsin:
 714   case vmIntrinsics::_dcos:
 715   case vmIntrinsics::_dtan:
 716   case vmIntrinsics::_dabs:
 717   case vmIntrinsics::_datan2:
 718   case vmIntrinsics::_dsqrt:
 719   case vmIntrinsics::_dexp:
 720   case vmIntrinsics::_dlog:
 721   case vmIntrinsics::_dlog10:
 722   case vmIntrinsics::_dpow:                     return inline_math_native(intrinsic_id());
 723 
 724   case vmIntrinsics::_min:
 725   case vmIntrinsics::_max:                      return inline_min_max(intrinsic_id());
 726 
 727   case vmIntrinsics::_addExactI:                return inline_math_addExactI(false /* add */);
 728   case vmIntrinsics::_addExactL:                return inline_math_addExactL(false /* add */);
 729   case vmIntrinsics::_decrementExactI:          return inline_math_subtractExactI(true /* decrement */);
 730   case vmIntrinsics::_decrementExactL:          return inline_math_subtractExactL(true /* decrement */);
 731   case vmIntrinsics::_incrementExactI:          return inline_math_addExactI(true /* increment */);
 732   case vmIntrinsics::_incrementExactL:          return inline_math_addExactL(true /* increment */);
 733   case vmIntrinsics::_multiplyExactI:           return inline_math_multiplyExactI();
 734   case vmIntrinsics::_multiplyExactL:           return inline_math_multiplyExactL();
 735   case vmIntrinsics::_negateExactI:             return inline_math_negateExactI();
 736   case vmIntrinsics::_negateExactL:             return inline_math_negateExactL();
 737   case vmIntrinsics::_subtractExactI:           return inline_math_subtractExactI(false /* subtract */);
 738   case vmIntrinsics::_subtractExactL:           return inline_math_subtractExactL(false /* subtract */);
 739 
 740   case vmIntrinsics::_arraycopy:                return inline_arraycopy();
 741 
 742   case vmIntrinsics::_compareTo:                return inline_string_compareTo();
 743   case vmIntrinsics::_indexOf:                  return inline_string_indexOf();
 744   case vmIntrinsics::_equals:                   return inline_string_equals();
 745 
 746   case vmIntrinsics::_getObject:                return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,  !is_volatile);
 747   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN, !is_volatile);
 748   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 749   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 750   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 751   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile);
 752   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile);
 753   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 754   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 755 
 756   case vmIntrinsics::_putObject:                return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,  !is_volatile);
 757   case vmIntrinsics::_putBoolean:               return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN, !is_volatile);
 758   case vmIntrinsics::_putByte:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 759   case vmIntrinsics::_putShort:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 760   case vmIntrinsics::_putChar:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 761   case vmIntrinsics::_putInt:                   return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile);
 762   case vmIntrinsics::_putLong:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile);
 763   case vmIntrinsics::_putFloat:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 764   case vmIntrinsics::_putDouble:                return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 765 
 766   case vmIntrinsics::_getByte_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 767   case vmIntrinsics::_getShort_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 768   case vmIntrinsics::_getChar_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 769   case vmIntrinsics::_getInt_raw:               return inline_unsafe_access( is_native_ptr, !is_store, T_INT,     !is_volatile);
 770   case vmIntrinsics::_getLong_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_LONG,    !is_volatile);
 771   case vmIntrinsics::_getFloat_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 772   case vmIntrinsics::_getDouble_raw:            return inline_unsafe_access( is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 773   case vmIntrinsics::_getAddress_raw:           return inline_unsafe_access( is_native_ptr, !is_store, T_ADDRESS, !is_volatile);
 774 
 775   case vmIntrinsics::_putByte_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 776   case vmIntrinsics::_putShort_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 777   case vmIntrinsics::_putChar_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 778   case vmIntrinsics::_putInt_raw:               return inline_unsafe_access( is_native_ptr,  is_store, T_INT,     !is_volatile);
 779   case vmIntrinsics::_putLong_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_LONG,    !is_volatile);
 780   case vmIntrinsics::_putFloat_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 781   case vmIntrinsics::_putDouble_raw:            return inline_unsafe_access( is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 782   case vmIntrinsics::_putAddress_raw:           return inline_unsafe_access( is_native_ptr,  is_store, T_ADDRESS, !is_volatile);
 783 
 784   case vmIntrinsics::_getObjectVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   is_volatile);
 785   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  is_volatile);
 786   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     is_volatile);
 787   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    is_volatile);
 788   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     is_volatile);
 789   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      is_volatile);
 790   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     is_volatile);
 791   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    is_volatile);
 792   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   is_volatile);
 793 
 794   case vmIntrinsics::_putObjectVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   is_volatile);
 795   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  is_volatile);
 796   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     is_volatile);
 797   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    is_volatile);
 798   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     is_volatile);
 799   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      is_volatile);
 800   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     is_volatile);
 801   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    is_volatile);
 802   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   is_volatile);
 803 
 804   case vmIntrinsics::_prefetchRead:             return inline_unsafe_prefetch(!is_native_ptr, !is_store, !is_static);
 805   case vmIntrinsics::_prefetchWrite:            return inline_unsafe_prefetch(!is_native_ptr,  is_store, !is_static);
 806   case vmIntrinsics::_prefetchReadStatic:       return inline_unsafe_prefetch(!is_native_ptr, !is_store,  is_static);
 807   case vmIntrinsics::_prefetchWriteStatic:      return inline_unsafe_prefetch(!is_native_ptr,  is_store,  is_static);
 808 
 809   case vmIntrinsics::_compareAndSwapObject:     return inline_unsafe_load_store(T_OBJECT, LS_cmpxchg);
 810   case vmIntrinsics::_compareAndSwapInt:        return inline_unsafe_load_store(T_INT,    LS_cmpxchg);
 811   case vmIntrinsics::_compareAndSwapLong:       return inline_unsafe_load_store(T_LONG,   LS_cmpxchg);
 812 
 813   case vmIntrinsics::_putOrderedObject:         return inline_unsafe_ordered_store(T_OBJECT);
 814   case vmIntrinsics::_putOrderedInt:            return inline_unsafe_ordered_store(T_INT);
 815   case vmIntrinsics::_putOrderedLong:           return inline_unsafe_ordered_store(T_LONG);
 816 
 817   case vmIntrinsics::_getAndAddInt:             return inline_unsafe_load_store(T_INT,    LS_xadd);
 818   case vmIntrinsics::_getAndAddLong:            return inline_unsafe_load_store(T_LONG,   LS_xadd);
 819   case vmIntrinsics::_getAndSetInt:             return inline_unsafe_load_store(T_INT,    LS_xchg);
 820   case vmIntrinsics::_getAndSetLong:            return inline_unsafe_load_store(T_LONG,   LS_xchg);
 821   case vmIntrinsics::_getAndSetObject:          return inline_unsafe_load_store(T_OBJECT, LS_xchg);
 822 
 823   case vmIntrinsics::_loadFence:
 824   case vmIntrinsics::_storeFence:
 825   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 826 
 827   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 828   case vmIntrinsics::_isInterrupted:            return inline_native_isInterrupted();
 829 
 830 #ifdef TRACE_HAVE_INTRINSICS
 831   case vmIntrinsics::_classID:                  return inline_native_classID();
 832   case vmIntrinsics::_threadID:                 return inline_native_threadID();
 833   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, TRACE_TIME_METHOD), "counterTime");
 834 #endif
 835   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), "currentTimeMillis");
 836   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), "nanoTime");
 837   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 838   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 839   case vmIntrinsics::_newArray:                 return inline_native_newArray();
 840   case vmIntrinsics::_getLength:                return inline_native_getLength();
 841   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 842   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 843   case vmIntrinsics::_equalsC:                  return inline_array_equals();
 844   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 845 
 846   case vmIntrinsics::_isAssignableFrom:         return inline_native_subtype_check();
 847 
 848   case vmIntrinsics::_isInstance:
 849   case vmIntrinsics::_getModifiers:
 850   case vmIntrinsics::_isInterface:
 851   case vmIntrinsics::_isArray:
 852   case vmIntrinsics::_isPrimitive:
 853   case vmIntrinsics::_getSuperclass:
 854   case vmIntrinsics::_getComponentType:
 855   case vmIntrinsics::_getClassAccessFlags:      return inline_native_Class_query(intrinsic_id());
 856 
 857   case vmIntrinsics::_floatToRawIntBits:
 858   case vmIntrinsics::_floatToIntBits:
 859   case vmIntrinsics::_intBitsToFloat:
 860   case vmIntrinsics::_doubleToRawLongBits:
 861   case vmIntrinsics::_doubleToLongBits:
 862   case vmIntrinsics::_longBitsToDouble:         return inline_fp_conversions(intrinsic_id());
 863 
 864   case vmIntrinsics::_numberOfLeadingZeros_i:
 865   case vmIntrinsics::_numberOfLeadingZeros_l:
 866   case vmIntrinsics::_numberOfTrailingZeros_i:
 867   case vmIntrinsics::_numberOfTrailingZeros_l:
 868   case vmIntrinsics::_bitCount_i:
 869   case vmIntrinsics::_bitCount_l:
 870   case vmIntrinsics::_reverseBytes_i:
 871   case vmIntrinsics::_reverseBytes_l:
 872   case vmIntrinsics::_reverseBytes_s:
 873   case vmIntrinsics::_reverseBytes_c:           return inline_number_methods(intrinsic_id());
 874 
 875   case vmIntrinsics::_getCallerClass:           return inline_native_Reflection_getCallerClass();
 876 
 877   case vmIntrinsics::_Reference_get:            return inline_reference_get();
 878 
 879   case vmIntrinsics::_aescrypt_encryptBlock:
 880   case vmIntrinsics::_aescrypt_decryptBlock:    return inline_aescrypt_Block(intrinsic_id());
 881 
 882   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 883   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 884     return inline_cipherBlockChaining_AESCrypt(intrinsic_id());
 885 
 886   case vmIntrinsics::_encodeISOArray:
 887     return inline_encodeISOArray();
 888 
 889   case vmIntrinsics::_updateCRC32:
 890     return inline_updateCRC32();
 891   case vmIntrinsics::_updateBytesCRC32:
 892     return inline_updateBytesCRC32();
 893   case vmIntrinsics::_updateByteBufferCRC32:
 894     return inline_updateByteBufferCRC32();
 895 
 896   default:
 897     // If you get here, it may be that someone has added a new intrinsic
 898     // to the list in vmSymbols.hpp without implementing it here.
 899 #ifndef PRODUCT
 900     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 901       tty-&gt;print_cr("*** Warning: Unimplemented intrinsic %s(%d)",
 902                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 903     }
 904 #endif
 905     return false;
 906   }
 907 }
 908 
 909 Node* LibraryCallKit::try_to_predicate() {
 910   if (!jvms()-&gt;has_method()) {
 911     // Root JVMState has a null method.
 912     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 913     // Insert the memory aliasing node
 914     set_all_memory(reset_memory());
 915   }
 916   assert(merged_memory(), "");
 917 
 918   switch (intrinsic_id()) {
 919   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 920     return inline_cipherBlockChaining_AESCrypt_predicate(false);
 921   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 922     return inline_cipherBlockChaining_AESCrypt_predicate(true);
 923 
 924   default:
 925     // If you get here, it may be that someone has added a new intrinsic
 926     // to the list in vmSymbols.hpp without implementing it here.
 927 #ifndef PRODUCT
 928     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 929       tty-&gt;print_cr("*** Warning: Unimplemented predicate for intrinsic %s(%d)",
 930                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 931     }
 932 #endif
 933     Node* slow_ctl = control();
 934     set_control(top()); // No fast path instrinsic
 935     return slow_ctl;
 936   }
 937 }
 938 
 939 //------------------------------set_result-------------------------------
 940 // Helper function for finishing intrinsics.
 941 void LibraryCallKit::set_result(RegionNode* region, PhiNode* value) {
 942   record_for_igvn(region);
 943   set_control(_gvn.transform(region));
 944   set_result( _gvn.transform(value));
 945   assert(value-&gt;type()-&gt;basic_type() == result()-&gt;bottom_type()-&gt;basic_type(), "sanity");
 946 }
 947 
 948 //------------------------------generate_guard---------------------------
 949 // Helper function for generating guarded fast-slow graph structures.
 950 // The given 'test', if true, guards a slow path.  If the test fails
 951 // then a fast path can be taken.  (We generally hope it fails.)
 952 // In all cases, GraphKit::control() is updated to the fast path.
 953 // The returned value represents the control for the slow path.
 954 // The return value is never 'top'; it is either a valid control
 955 // or NULL if it is obvious that the slow path can never be taken.
 956 // Also, if region and the slow control are not NULL, the slow edge
 957 // is appended to the region.
 958 Node* LibraryCallKit::generate_guard(Node* test, RegionNode* region, float true_prob) {
 959   if (stopped()) {
 960     // Already short circuited.
 961     return NULL;
 962   }
 963 
 964   // Build an if node and its projections.
 965   // If test is true we take the slow path, which we assume is uncommon.
 966   if (_gvn.type(test) == TypeInt::ZERO) {
 967     // The slow branch is never taken.  No need to build this guard.
 968     return NULL;
 969   }
 970 
 971   IfNode* iff = create_and_map_if(control(), test, true_prob, COUNT_UNKNOWN);
 972 
 973   Node* if_slow = _gvn.transform(new (C) IfTrueNode(iff));
 974   if (if_slow == top()) {
 975     // The slow branch is never taken.  No need to build this guard.
 976     return NULL;
 977   }
 978 
 979   if (region != NULL)
 980     region-&gt;add_req(if_slow);
 981 
 982   Node* if_fast = _gvn.transform(new (C) IfFalseNode(iff));
 983   set_control(if_fast);
 984 
 985   return if_slow;
 986 }
 987 
 988 inline Node* LibraryCallKit::generate_slow_guard(Node* test, RegionNode* region) {
 989   return generate_guard(test, region, PROB_UNLIKELY_MAG(3));
 990 }
 991 inline Node* LibraryCallKit::generate_fair_guard(Node* test, RegionNode* region) {
 992   return generate_guard(test, region, PROB_FAIR);
 993 }
 994 
 995 inline Node* LibraryCallKit::generate_negative_guard(Node* index, RegionNode* region,
 996                                                      Node* *pos_index) {
 997   if (stopped())
 998     return NULL;                // already stopped
 999   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS)) // [0,maxint]
1000     return NULL;                // index is already adequately typed
1001   Node* cmp_lt = _gvn.transform(new (C) CmpINode(index, intcon(0)));
1002   Node* bol_lt = _gvn.transform(new (C) BoolNode(cmp_lt, BoolTest::lt));
1003   Node* is_neg = generate_guard(bol_lt, region, PROB_MIN);
1004   if (is_neg != NULL &amp;&amp; pos_index != NULL) {
1005     // Emulate effect of Parse::adjust_map_after_if.
1006     Node* ccast = new (C) CastIINode(index, TypeInt::POS);
1007     ccast-&gt;set_req(0, control());
1008     (*pos_index) = _gvn.transform(ccast);
1009   }
1010   return is_neg;
1011 }
1012 
1013 inline Node* LibraryCallKit::generate_nonpositive_guard(Node* index, bool never_negative,
1014                                                         Node* *pos_index) {
1015   if (stopped())
1016     return NULL;                // already stopped
1017   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS1)) // [1,maxint]
1018     return NULL;                // index is already adequately typed
1019   Node* cmp_le = _gvn.transform(new (C) CmpINode(index, intcon(0)));
1020   BoolTest::mask le_or_eq = (never_negative ? BoolTest::eq : BoolTest::le);
1021   Node* bol_le = _gvn.transform(new (C) BoolNode(cmp_le, le_or_eq));
1022   Node* is_notp = generate_guard(bol_le, NULL, PROB_MIN);
1023   if (is_notp != NULL &amp;&amp; pos_index != NULL) {
1024     // Emulate effect of Parse::adjust_map_after_if.
1025     Node* ccast = new (C) CastIINode(index, TypeInt::POS1);
1026     ccast-&gt;set_req(0, control());
1027     (*pos_index) = _gvn.transform(ccast);
1028   }
1029   return is_notp;
1030 }
1031 
1032 // Make sure that 'position' is a valid limit index, in [0..length].
1033 // There are two equivalent plans for checking this:
1034 //   A. (offset + copyLength)  unsigned&lt;=  arrayLength
1035 //   B. offset  &lt;=  (arrayLength - copyLength)
1036 // We require that all of the values above, except for the sum and
1037 // difference, are already known to be non-negative.
1038 // Plan A is robust in the face of overflow, if offset and copyLength
1039 // are both hugely positive.
1040 //
1041 // Plan B is less direct and intuitive, but it does not overflow at
1042 // all, since the difference of two non-negatives is always
1043 // representable.  Whenever Java methods must perform the equivalent
1044 // check they generally use Plan B instead of Plan A.
1045 // For the moment we use Plan A.
1046 inline Node* LibraryCallKit::generate_limit_guard(Node* offset,
1047                                                   Node* subseq_length,
1048                                                   Node* array_length,
1049                                                   RegionNode* region) {
1050   if (stopped())
1051     return NULL;                // already stopped
1052   bool zero_offset = _gvn.type(offset) == TypeInt::ZERO;
1053   if (zero_offset &amp;&amp; subseq_length-&gt;eqv_uncast(array_length))
1054     return NULL;                // common case of whole-array copy
1055   Node* last = subseq_length;
1056   if (!zero_offset)             // last += offset
1057     last = _gvn.transform(new (C) AddINode(last, offset));
1058   Node* cmp_lt = _gvn.transform(new (C) CmpUNode(array_length, last));
1059   Node* bol_lt = _gvn.transform(new (C) BoolNode(cmp_lt, BoolTest::lt));
1060   Node* is_over = generate_guard(bol_lt, region, PROB_MIN);
1061   return is_over;
1062 }
1063 
1064 
1065 //--------------------------generate_current_thread--------------------
1066 Node* LibraryCallKit::generate_current_thread(Node* &amp;tls_output) {
1067   ciKlass*    thread_klass = env()-&gt;Thread_klass();
1068   const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
1069   Node* thread = _gvn.transform(new (C) ThreadLocalNode());
1070   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::threadObj_offset()));
1071   Node* threadObj = make_load(NULL, p, thread_type, T_OBJECT, MemNode::unordered);
1072   tls_output = thread;
1073   return threadObj;
1074 }
1075 
1076 
1077 //------------------------------make_string_method_node------------------------
1078 // Helper method for String intrinsic functions. This version is called
1079 // with str1 and str2 pointing to String object nodes.
1080 //
1081 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1, Node* str2) {
1082   Node* no_ctrl = NULL;
1083 
1084   // Get start addr of string
1085   Node* str1_value   = load_String_value(no_ctrl, str1);
1086   Node* str1_offset  = load_String_offset(no_ctrl, str1);
1087   Node* str1_start   = array_element_address(str1_value, str1_offset, T_CHAR);
1088 
1089   // Get length of string 1
1090   Node* str1_len  = load_String_length(no_ctrl, str1);
1091 
1092   Node* str2_value   = load_String_value(no_ctrl, str2);
1093   Node* str2_offset  = load_String_offset(no_ctrl, str2);
1094   Node* str2_start   = array_element_address(str2_value, str2_offset, T_CHAR);
1095 
1096   Node* str2_len = NULL;
1097   Node* result = NULL;
1098 
1099   switch (opcode) {
1100   case Op_StrIndexOf:
1101     // Get length of string 2
1102     str2_len = load_String_length(no_ctrl, str2);
1103 
1104     result = new (C) StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1105                                  str1_start, str1_len, str2_start, str2_len);
1106     break;
1107   case Op_StrComp:
1108     // Get length of string 2
1109     str2_len = load_String_length(no_ctrl, str2);
1110 
1111     result = new (C) StrCompNode(control(), memory(TypeAryPtr::CHARS),
1112                                  str1_start, str1_len, str2_start, str2_len);
1113     break;
1114   case Op_StrEquals:
1115     result = new (C) StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1116                                str1_start, str2_start, str1_len);
1117     break;
1118   default:
1119     ShouldNotReachHere();
1120     return NULL;
1121   }
1122 
1123   // All these intrinsics have checks.
1124   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1125 
1126   return _gvn.transform(result);
1127 }
1128 
1129 // Helper method for String intrinsic functions. This version is called
1130 // with str1 and str2 pointing to char[] nodes, with cnt1 and cnt2 pointing
1131 // to Int nodes containing the lenghts of str1 and str2.
1132 //
1133 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2) {
1134   Node* result = NULL;
1135   switch (opcode) {
1136   case Op_StrIndexOf:
1137     result = new (C) StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1138                                  str1_start, cnt1, str2_start, cnt2);
1139     break;
1140   case Op_StrComp:
1141     result = new (C) StrCompNode(control(), memory(TypeAryPtr::CHARS),
1142                                  str1_start, cnt1, str2_start, cnt2);
1143     break;
1144   case Op_StrEquals:
1145     result = new (C) StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1146                                  str1_start, str2_start, cnt1);
1147     break;
1148   default:
1149     ShouldNotReachHere();
1150     return NULL;
1151   }
1152 
1153   // All these intrinsics have checks.
1154   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1155 
1156   return _gvn.transform(result);
1157 }
1158 
1159 //------------------------------inline_string_compareTo------------------------
1160 // public int java.lang.String.compareTo(String anotherString);
1161 bool LibraryCallKit::inline_string_compareTo() {
1162   Node* receiver = null_check(argument(0));
1163   Node* arg      = null_check(argument(1));
1164   if (stopped()) {
1165     return true;
1166   }
1167   set_result(make_string_method_node(Op_StrComp, receiver, arg));
1168   return true;
1169 }
1170 
1171 //------------------------------inline_string_equals------------------------
1172 bool LibraryCallKit::inline_string_equals() {
1173   Node* receiver = null_check_receiver();
1174   // NOTE: Do not null check argument for String.equals() because spec
1175   // allows to specify NULL as argument.
1176   Node* argument = this-&gt;argument(1);
1177   if (stopped()) {
1178     return true;
1179   }
1180 
1181   // paths (plus control) merge
1182   RegionNode* region = new (C) RegionNode(5);
1183   Node* phi = new (C) PhiNode(region, TypeInt::BOOL);
1184 
1185   // does source == target string?
1186   Node* cmp = _gvn.transform(new (C) CmpPNode(receiver, argument));
1187   Node* bol = _gvn.transform(new (C) BoolNode(cmp, BoolTest::eq));
1188 
1189   Node* if_eq = generate_slow_guard(bol, NULL);
1190   if (if_eq != NULL) {
1191     // receiver == argument
1192     phi-&gt;init_req(2, intcon(1));
1193     region-&gt;init_req(2, if_eq);
1194   }
1195 
1196   // get String klass for instanceOf
1197   ciInstanceKlass* klass = env()-&gt;String_klass();
1198 
1199   if (!stopped()) {
1200     Node* inst = gen_instanceof(argument, makecon(TypeKlassPtr::make(klass)));
1201     Node* cmp  = _gvn.transform(new (C) CmpINode(inst, intcon(1)));
1202     Node* bol  = _gvn.transform(new (C) BoolNode(cmp, BoolTest::ne));
1203 
1204     Node* inst_false = generate_guard(bol, NULL, PROB_MIN);
1205     //instanceOf == true, fallthrough
1206 
1207     if (inst_false != NULL) {
1208       phi-&gt;init_req(3, intcon(0));
1209       region-&gt;init_req(3, inst_false);
1210     }
1211   }
1212 
1213   if (!stopped()) {
1214     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(klass);
1215 
1216     // Properly cast the argument to String
1217     argument = _gvn.transform(new (C) CheckCastPPNode(control(), argument, string_type));
1218     // This path is taken only when argument's type is String:NotNull.
1219     argument = cast_not_null(argument, false);
1220 
1221     Node* no_ctrl = NULL;
1222 
1223     // Get start addr of receiver
1224     Node* receiver_val    = load_String_value(no_ctrl, receiver);
1225     Node* receiver_offset = load_String_offset(no_ctrl, receiver);
1226     Node* receiver_start = array_element_address(receiver_val, receiver_offset, T_CHAR);
1227 
1228     // Get length of receiver
1229     Node* receiver_cnt  = load_String_length(no_ctrl, receiver);
1230 
1231     // Get start addr of argument
1232     Node* argument_val    = load_String_value(no_ctrl, argument);
1233     Node* argument_offset = load_String_offset(no_ctrl, argument);
1234     Node* argument_start = array_element_address(argument_val, argument_offset, T_CHAR);
1235 
1236     // Get length of argument
1237     Node* argument_cnt  = load_String_length(no_ctrl, argument);
1238 
1239     // Check for receiver count != argument count
1240     Node* cmp = _gvn.transform(new(C) CmpINode(receiver_cnt, argument_cnt));
1241     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::ne));
1242     Node* if_ne = generate_slow_guard(bol, NULL);
1243     if (if_ne != NULL) {
1244       phi-&gt;init_req(4, intcon(0));
1245       region-&gt;init_req(4, if_ne);
1246     }
1247 
1248     // Check for count == 0 is done by assembler code for StrEquals.
1249 
1250     if (!stopped()) {
1251       Node* equals = make_string_method_node(Op_StrEquals, receiver_start, receiver_cnt, argument_start, argument_cnt);
1252       phi-&gt;init_req(1, equals);
1253       region-&gt;init_req(1, control());
1254     }
1255   }
1256 
1257   // post merge
1258   set_control(_gvn.transform(region));
1259   record_for_igvn(region);
1260 
1261   set_result(_gvn.transform(phi));
1262   return true;
1263 }
1264 
1265 //------------------------------inline_array_equals----------------------------
1266 bool LibraryCallKit::inline_array_equals() {
1267   Node* arg1 = argument(0);
1268   Node* arg2 = argument(1);
1269   set_result(_gvn.transform(new (C) AryEqNode(control(), memory(TypeAryPtr::CHARS), arg1, arg2)));
1270   return true;
1271 }
1272 
1273 // Java version of String.indexOf(constant string)
1274 // class StringDecl {
1275 //   StringDecl(char[] ca) {
1276 //     offset = 0;
1277 //     count = ca.length;
1278 //     value = ca;
1279 //   }
1280 //   int offset;
1281 //   int count;
1282 //   char[] value;
1283 // }
1284 //
1285 // static int string_indexOf_J(StringDecl string_object, char[] target_object,
1286 //                             int targetOffset, int cache_i, int md2) {
1287 //   int cache = cache_i;
1288 //   int sourceOffset = string_object.offset;
1289 //   int sourceCount = string_object.count;
1290 //   int targetCount = target_object.length;
1291 //
1292 //   int targetCountLess1 = targetCount - 1;
1293 //   int sourceEnd = sourceOffset + sourceCount - targetCountLess1;
1294 //
1295 //   char[] source = string_object.value;
1296 //   char[] target = target_object;
1297 //   int lastChar = target[targetCountLess1];
1298 //
1299 //  outer_loop:
1300 //   for (int i = sourceOffset; i &lt; sourceEnd; ) {
1301 //     int src = source[i + targetCountLess1];
1302 //     if (src == lastChar) {
1303 //       // With random strings and a 4-character alphabet,
1304 //       // reverse matching at this point sets up 0.8% fewer
1305 //       // frames, but (paradoxically) makes 0.3% more probes.
1306 //       // Since those probes are nearer the lastChar probe,
1307 //       // there is may be a net D$ win with reverse matching.
1308 //       // But, reversing loop inhibits unroll of inner loop
1309 //       // for unknown reason.  So, does running outer loop from
1310 //       // (sourceOffset - targetCountLess1) to (sourceOffset + sourceCount)
1311 //       for (int j = 0; j &lt; targetCountLess1; j++) {
1312 //         if (target[targetOffset + j] != source[i+j]) {
1313 //           if ((cache &amp; (1 &lt;&lt; source[i+j])) == 0) {
1314 //             if (md2 &lt; j+1) {
1315 //               i += j+1;
1316 //               continue outer_loop;
1317 //             }
1318 //           }
1319 //           i += md2;
1320 //           continue outer_loop;
1321 //         }
1322 //       }
1323 //       return i - sourceOffset;
1324 //     }
1325 //     if ((cache &amp; (1 &lt;&lt; src)) == 0) {
1326 //       i += targetCountLess1;
1327 //     } // using "i += targetCount;" and an "else i++;" causes a jump to jump.
1328 //     i++;
1329 //   }
1330 //   return -1;
1331 // }
1332 
1333 //------------------------------string_indexOf------------------------
1334 Node* LibraryCallKit::string_indexOf(Node* string_object, ciTypeArray* target_array, jint targetOffset_i,
1335                                      jint cache_i, jint md2_i) {
1336 
1337   Node* no_ctrl  = NULL;
1338   float likely   = PROB_LIKELY(0.9);
1339   float unlikely = PROB_UNLIKELY(0.9);
1340 
1341   const int nargs = 0; // no arguments to push back for uncommon trap in predicate
1342 
1343   Node* source        = load_String_value(no_ctrl, string_object);
1344   Node* sourceOffset  = load_String_offset(no_ctrl, string_object);
1345   Node* sourceCount   = load_String_length(no_ctrl, string_object);
1346 
1347   Node* target = _gvn.transform( makecon(TypeOopPtr::make_from_constant(target_array, true)));
1348   jint target_length = target_array-&gt;length();
1349   const TypeAry* target_array_type = TypeAry::make(TypeInt::CHAR, TypeInt::make(0, target_length, Type::WidenMin));
1350   const TypeAryPtr* target_type = TypeAryPtr::make(TypePtr::BotPTR, target_array_type, target_array-&gt;klass(), true, Type::OffsetBot);
1351 
1352   // String.value field is known to be @Stable.
1353   if (UseImplicitStableValues) {
1354     target = cast_array_to_stable(target, target_type);
1355   }
1356 
1357   IdealKit kit(this, false, true);
1358 #define __ kit.
1359   Node* zero             = __ ConI(0);
1360   Node* one              = __ ConI(1);
1361   Node* cache            = __ ConI(cache_i);
1362   Node* md2              = __ ConI(md2_i);
1363   Node* lastChar         = __ ConI(target_array-&gt;char_at(target_length - 1));
1364   Node* targetCount      = __ ConI(target_length);
1365   Node* targetCountLess1 = __ ConI(target_length - 1);
1366   Node* targetOffset     = __ ConI(targetOffset_i);
1367   Node* sourceEnd        = __ SubI(__ AddI(sourceOffset, sourceCount), targetCountLess1);
1368 
1369   IdealVariable rtn(kit), i(kit), j(kit); __ declarations_done();
1370   Node* outer_loop = __ make_label(2 /* goto */);
1371   Node* return_    = __ make_label(1);
1372 
1373   __ set(rtn,__ ConI(-1));
1374   __ loop(this, nargs, i, sourceOffset, BoolTest::lt, sourceEnd); {
1375        Node* i2  = __ AddI(__ value(i), targetCountLess1);
1376        // pin to prohibit loading of "next iteration" value which may SEGV (rare)
1377        Node* src = load_array_element(__ ctrl(), source, i2, TypeAryPtr::CHARS);
1378        __ if_then(src, BoolTest::eq, lastChar, unlikely); {
1379          __ loop(this, nargs, j, zero, BoolTest::lt, targetCountLess1); {
1380               Node* tpj = __ AddI(targetOffset, __ value(j));
1381               Node* targ = load_array_element(no_ctrl, target, tpj, target_type);
1382               Node* ipj  = __ AddI(__ value(i), __ value(j));
1383               Node* src2 = load_array_element(no_ctrl, source, ipj, TypeAryPtr::CHARS);
1384               __ if_then(targ, BoolTest::ne, src2); {
1385                 __ if_then(__ AndI(cache, __ LShiftI(one, src2)), BoolTest::eq, zero); {
1386                   __ if_then(md2, BoolTest::lt, __ AddI(__ value(j), one)); {
1387                     __ increment(i, __ AddI(__ value(j), one));
1388                     __ goto_(outer_loop);
1389                   } __ end_if(); __ dead(j);
1390                 }__ end_if(); __ dead(j);
1391                 __ increment(i, md2);
1392                 __ goto_(outer_loop);
1393               }__ end_if();
1394               __ increment(j, one);
1395          }__ end_loop(); __ dead(j);
1396          __ set(rtn, __ SubI(__ value(i), sourceOffset)); __ dead(i);
1397          __ goto_(return_);
1398        }__ end_if();
1399        __ if_then(__ AndI(cache, __ LShiftI(one, src)), BoolTest::eq, zero, likely); {
1400          __ increment(i, targetCountLess1);
1401        }__ end_if();
1402        __ increment(i, one);
1403        __ bind(outer_loop);
1404   }__ end_loop(); __ dead(i);
1405   __ bind(return_);
1406 
1407   // Final sync IdealKit and GraphKit.
1408   final_sync(kit);
1409   Node* result = __ value(rtn);
1410 #undef __
1411   C-&gt;set_has_loops(true);
1412   return result;
1413 }
1414 
1415 //------------------------------inline_string_indexOf------------------------
1416 bool LibraryCallKit::inline_string_indexOf() {
1417   Node* receiver = argument(0);
1418   Node* arg      = argument(1);
1419 
1420   Node* result;
1421   // Disable the use of pcmpestri until it can be guaranteed that
1422   // the load doesn't cross into the uncommited space.
1423   if (Matcher::has_match_rule(Op_StrIndexOf) &amp;&amp;
1424       UseSSE42Intrinsics) {
1425     // Generate SSE4.2 version of indexOf
1426     // We currently only have match rules that use SSE4.2
1427 
1428     receiver = null_check(receiver);
1429     arg      = null_check(arg);
1430     if (stopped()) {
1431       return true;
1432     }
1433 
1434     ciInstanceKlass* str_klass = env()-&gt;String_klass();
1435     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(str_klass);
1436 
1437     // Make the merge point
1438     RegionNode* result_rgn = new (C) RegionNode(4);
1439     Node*       result_phi = new (C) PhiNode(result_rgn, TypeInt::INT);
1440     Node* no_ctrl  = NULL;
1441 
1442     // Get start addr of source string
1443     Node* source = load_String_value(no_ctrl, receiver);
1444     Node* source_offset = load_String_offset(no_ctrl, receiver);
1445     Node* source_start = array_element_address(source, source_offset, T_CHAR);
1446 
1447     // Get length of source string
1448     Node* source_cnt  = load_String_length(no_ctrl, receiver);
1449 
1450     // Get start addr of substring
1451     Node* substr = load_String_value(no_ctrl, arg);
1452     Node* substr_offset = load_String_offset(no_ctrl, arg);
1453     Node* substr_start = array_element_address(substr, substr_offset, T_CHAR);
1454 
1455     // Get length of source string
1456     Node* substr_cnt  = load_String_length(no_ctrl, arg);
1457 
1458     // Check for substr count &gt; string count
1459     Node* cmp = _gvn.transform(new(C) CmpINode(substr_cnt, source_cnt));
1460     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::gt));
1461     Node* if_gt = generate_slow_guard(bol, NULL);
1462     if (if_gt != NULL) {
1463       result_phi-&gt;init_req(2, intcon(-1));
1464       result_rgn-&gt;init_req(2, if_gt);
1465     }
1466 
1467     if (!stopped()) {
1468       // Check for substr count == 0
1469       cmp = _gvn.transform(new(C) CmpINode(substr_cnt, intcon(0)));
1470       bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::eq));
1471       Node* if_zero = generate_slow_guard(bol, NULL);
1472       if (if_zero != NULL) {
1473         result_phi-&gt;init_req(3, intcon(0));
1474         result_rgn-&gt;init_req(3, if_zero);
1475       }
1476     }
1477 
1478     if (!stopped()) {
1479       result = make_string_method_node(Op_StrIndexOf, source_start, source_cnt, substr_start, substr_cnt);
1480       result_phi-&gt;init_req(1, result);
1481       result_rgn-&gt;init_req(1, control());
1482     }
1483     set_control(_gvn.transform(result_rgn));
1484     record_for_igvn(result_rgn);
1485     result = _gvn.transform(result_phi);
1486 
1487   } else { // Use LibraryCallKit::string_indexOf
1488     // don't intrinsify if argument isn't a constant string.
1489     if (!arg-&gt;is_Con()) {
1490      return false;
1491     }
1492     const TypeOopPtr* str_type = _gvn.type(arg)-&gt;isa_oopptr();
1493     if (str_type == NULL) {
1494       return false;
1495     }
1496     ciInstanceKlass* klass = env()-&gt;String_klass();
1497     ciObject* str_const = str_type-&gt;const_oop();
1498     if (str_const == NULL || str_const-&gt;klass() != klass) {
1499       return false;
1500     }
1501     ciInstance* str = str_const-&gt;as_instance();
1502     assert(str != NULL, "must be instance");
1503 
1504     ciObject* v = str-&gt;field_value_by_offset(java_lang_String::value_offset_in_bytes()).as_object();
1505     ciTypeArray* pat = v-&gt;as_type_array(); // pattern (argument) character array
1506 
1507     int o;
1508     int c;
1509     if (java_lang_String::has_offset_field()) {
1510       o = str-&gt;field_value_by_offset(java_lang_String::offset_offset_in_bytes()).as_int();
1511       c = str-&gt;field_value_by_offset(java_lang_String::count_offset_in_bytes()).as_int();
1512     } else {
1513       o = 0;
1514       c = pat-&gt;length();
1515     }
1516 
1517     // constant strings have no offset and count == length which
1518     // simplifies the resulting code somewhat so lets optimize for that.
1519     if (o != 0 || c != pat-&gt;length()) {
1520      return false;
1521     }
1522 
1523     receiver = null_check(receiver, T_OBJECT);
1524     // NOTE: No null check on the argument is needed since it's a constant String oop.
1525     if (stopped()) {
1526       return true;
1527     }
1528 
1529     // The null string as a pattern always returns 0 (match at beginning of string)
1530     if (c == 0) {
1531       set_result(intcon(0));
1532       return true;
1533     }
1534 
1535     // Generate default indexOf
1536     jchar lastChar = pat-&gt;char_at(o + (c - 1));
1537     int cache = 0;
1538     int i;
1539     for (i = 0; i &lt; c - 1; i++) {
1540       assert(i &lt; pat-&gt;length(), "out of range");
1541       cache |= (1 &lt;&lt; (pat-&gt;char_at(o + i) &amp; (sizeof(cache) * BitsPerByte - 1)));
1542     }
1543 
1544     int md2 = c;
1545     for (i = 0; i &lt; c - 1; i++) {
1546       assert(i &lt; pat-&gt;length(), "out of range");
1547       if (pat-&gt;char_at(o + i) == lastChar) {
1548         md2 = (c - 1) - i;
1549       }
1550     }
1551 
1552     result = string_indexOf(receiver, pat, o, cache, md2);
1553   }
1554   set_result(result);
1555   return true;
1556 }
1557 
1558 //--------------------------round_double_node--------------------------------
1559 // Round a double node if necessary.
1560 Node* LibraryCallKit::round_double_node(Node* n) {
1561   if (Matcher::strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt;= 1)
1562     n = _gvn.transform(new (C) RoundDoubleNode(0, n));
1563   return n;
1564 }
1565 
1566 //------------------------------inline_math-----------------------------------
1567 // public static double Math.abs(double)
1568 // public static double Math.sqrt(double)
1569 // public static double Math.log(double)
1570 // public static double Math.log10(double)
1571 bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
1572   Node* arg = round_double_node(argument(0));
1573   Node* n;
1574   switch (id) {
1575   case vmIntrinsics::_dabs:   n = new (C) AbsDNode(                arg);  break;
1576   case vmIntrinsics::_dsqrt:  n = new (C) SqrtDNode(C, control(),  arg);  break;
1577   case vmIntrinsics::_dlog:   n = new (C) LogDNode(C, control(),   arg);  break;
1578   case vmIntrinsics::_dlog10: n = new (C) Log10DNode(C, control(), arg);  break;
1579   default:  fatal_unexpected_iid(id);  break;
1580   }
1581   set_result(_gvn.transform(n));
1582   return true;
1583 }
1584 
1585 //------------------------------inline_trig----------------------------------
1586 // Inline sin/cos/tan instructions, if possible.  If rounding is required, do
1587 // argument reduction which will turn into a fast/slow diamond.
1588 bool LibraryCallKit::inline_trig(vmIntrinsics::ID id) {
1589   Node* arg = round_double_node(argument(0));
1590   Node* n = NULL;
1591 
1592   switch (id) {
1593   case vmIntrinsics::_dsin:  n = new (C) SinDNode(C, control(), arg);  break;
1594   case vmIntrinsics::_dcos:  n = new (C) CosDNode(C, control(), arg);  break;
1595   case vmIntrinsics::_dtan:  n = new (C) TanDNode(C, control(), arg);  break;
1596   default:  fatal_unexpected_iid(id);  break;
1597   }
1598   n = _gvn.transform(n);
1599 
1600   // Rounding required?  Check for argument reduction!
1601   if (Matcher::strict_fp_requires_explicit_rounding) {
1602     static const double     pi_4 =  0.7853981633974483;
1603     static const double neg_pi_4 = -0.7853981633974483;
1604     // pi/2 in 80-bit extended precision
1605     // static const unsigned char pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0x3f,0x00,0x00,0x00,0x00,0x00,0x00};
1606     // -pi/2 in 80-bit extended precision
1607     // static const unsigned char neg_pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0xbf,0x00,0x00,0x00,0x00,0x00,0x00};
1608     // Cutoff value for using this argument reduction technique
1609     //static const double    pi_2_minus_epsilon =  1.564660403643354;
1610     //static const double neg_pi_2_plus_epsilon = -1.564660403643354;
1611 
1612     // Pseudocode for sin:
1613     // if (x &lt;= Math.PI / 4.0) {
1614     //   if (x &gt;= -Math.PI / 4.0) return  fsin(x);
1615     //   if (x &gt;= -Math.PI / 2.0) return -fcos(x + Math.PI / 2.0);
1616     // } else {
1617     //   if (x &lt;=  Math.PI / 2.0) return  fcos(x - Math.PI / 2.0);
1618     // }
1619     // return StrictMath.sin(x);
1620 
1621     // Pseudocode for cos:
1622     // if (x &lt;= Math.PI / 4.0) {
1623     //   if (x &gt;= -Math.PI / 4.0) return  fcos(x);
1624     //   if (x &gt;= -Math.PI / 2.0) return  fsin(x + Math.PI / 2.0);
1625     // } else {
1626     //   if (x &lt;=  Math.PI / 2.0) return -fsin(x - Math.PI / 2.0);
1627     // }
1628     // return StrictMath.cos(x);
1629 
1630     // Actually, sticking in an 80-bit Intel value into C2 will be tough; it
1631     // requires a special machine instruction to load it.  Instead we'll try
1632     // the 'easy' case.  If we really need the extra range +/- PI/2 we'll
1633     // probably do the math inside the SIN encoding.
1634 
1635     // Make the merge point
1636     RegionNode* r = new (C) RegionNode(3);
1637     Node* phi = new (C) PhiNode(r, Type::DOUBLE);
1638 
1639     // Flatten arg so we need only 1 test
1640     Node *abs = _gvn.transform(new (C) AbsDNode(arg));
1641     // Node for PI/4 constant
1642     Node *pi4 = makecon(TypeD::make(pi_4));
1643     // Check PI/4 : abs(arg)
1644     Node *cmp = _gvn.transform(new (C) CmpDNode(pi4,abs));
1645     // Check: If PI/4 &lt; abs(arg) then go slow
1646     Node *bol = _gvn.transform(new (C) BoolNode( cmp, BoolTest::lt ));
1647     // Branch either way
1648     IfNode *iff = create_and_xform_if(control(),bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1649     set_control(opt_iff(r,iff));
1650 
1651     // Set fast path result
1652     phi-&gt;init_req(2, n);
1653 
1654     // Slow path - non-blocking leaf call
1655     Node* call = NULL;
1656     switch (id) {
1657     case vmIntrinsics::_dsin:
1658       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1659                                CAST_FROM_FN_PTR(address, SharedRuntime::dsin),
1660                                "Sin", NULL, arg, top());
1661       break;
1662     case vmIntrinsics::_dcos:
1663       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1664                                CAST_FROM_FN_PTR(address, SharedRuntime::dcos),
1665                                "Cos", NULL, arg, top());
1666       break;
1667     case vmIntrinsics::_dtan:
1668       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1669                                CAST_FROM_FN_PTR(address, SharedRuntime::dtan),
1670                                "Tan", NULL, arg, top());
1671       break;
1672     }
1673     assert(control()-&gt;in(0) == call, "");
1674     Node* slow_result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
1675     r-&gt;init_req(1, control());
1676     phi-&gt;init_req(1, slow_result);
1677 
1678     // Post-merge
1679     set_control(_gvn.transform(r));
1680     record_for_igvn(r);
1681     n = _gvn.transform(phi);
1682 
1683     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1684   }
1685   set_result(n);
1686   return true;
1687 }
1688 
1689 void LibraryCallKit::finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName) {
1690   //-------------------
1691   //result=(result.isNaN())? funcAddr():result;
1692   // Check: If isNaN() by checking result!=result? then either trap
1693   // or go to runtime
1694   Node* cmpisnan = _gvn.transform(new (C) CmpDNode(result, result));
1695   // Build the boolean node
1696   Node* bolisnum = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::eq));
1697 
1698   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1699     { BuildCutout unless(this, bolisnum, PROB_STATIC_FREQUENT);
1700       // The pow or exp intrinsic returned a NaN, which requires a call
1701       // to the runtime.  Recompile with the runtime call.
1702       uncommon_trap(Deoptimization::Reason_intrinsic,
1703                     Deoptimization::Action_make_not_entrant);
1704     }
1705     set_result(result);
1706   } else {
1707     // If this inlining ever returned NaN in the past, we compile a call
1708     // to the runtime to properly handle corner cases
1709 
1710     IfNode* iff = create_and_xform_if(control(), bolisnum, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1711     Node* if_slow = _gvn.transform(new (C) IfFalseNode(iff));
1712     Node* if_fast = _gvn.transform(new (C) IfTrueNode(iff));
1713 
1714     if (!if_slow-&gt;is_top()) {
1715       RegionNode* result_region = new (C) RegionNode(3);
1716       PhiNode*    result_val = new (C) PhiNode(result_region, Type::DOUBLE);
1717 
1718       result_region-&gt;init_req(1, if_fast);
1719       result_val-&gt;init_req(1, result);
1720 
1721       set_control(if_slow);
1722 
1723       const TypePtr* no_memory_effects = NULL;
1724       Node* rt = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1725                                    no_memory_effects,
1726                                    x, top(), y, y ? top() : NULL);
1727       Node* value = _gvn.transform(new (C) ProjNode(rt, TypeFunc::Parms+0));
1728 #ifdef ASSERT
1729       Node* value_top = _gvn.transform(new (C) ProjNode(rt, TypeFunc::Parms+1));
1730       assert(value_top == top(), "second value must be top");
1731 #endif
1732 
1733       result_region-&gt;init_req(2, control());
1734       result_val-&gt;init_req(2, value);
1735       set_result(result_region, result_val);
1736     } else {
1737       set_result(result);
1738     }
1739   }
1740 }
1741 
1742 //------------------------------inline_exp-------------------------------------
1743 // Inline exp instructions, if possible.  The Intel hardware only misses
1744 // really odd corner cases (+/- Infinity).  Just uncommon-trap them.
1745 bool LibraryCallKit::inline_exp() {
1746   Node* arg = round_double_node(argument(0));
1747   Node* n   = _gvn.transform(new (C) ExpDNode(C, control(), arg));
1748 
1749   finish_pow_exp(n, arg, NULL, OptoRuntime::Math_D_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dexp), "EXP");
1750 
1751   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1752   return true;
1753 }
1754 
1755 //------------------------------inline_pow-------------------------------------
1756 // Inline power instructions, if possible.
1757 bool LibraryCallKit::inline_pow() {
1758   // Pseudocode for pow
1759   // if (x &lt;= 0.0) {
1760   //   long longy = (long)y;
1761   //   if ((double)longy == y) { // if y is long
1762   //     if (y + 1 == y) longy = 0; // huge number: even
1763   //     result = ((1&amp;longy) == 0)?-DPow(abs(x), y):DPow(abs(x), y);
1764   //   } else {
1765   //     result = NaN;
1766   //   }
1767   // } else {
1768   //   result = DPow(x,y);
1769   // }
1770   // if (result != result)?  {
1771   //   result = uncommon_trap() or runtime_call();
1772   // }
1773   // return result;
1774 
1775   Node* x = round_double_node(argument(0));
1776   Node* y = round_double_node(argument(2));
1777 
1778   Node* result = NULL;
1779 
1780   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1781     // Short form: skip the fancy tests and just check for NaN result.
1782     result = _gvn.transform(new (C) PowDNode(C, control(), x, y));
1783   } else {
1784     // If this inlining ever returned NaN in the past, include all
1785     // checks + call to the runtime.
1786 
1787     // Set the merge point for If node with condition of (x &lt;= 0.0)
1788     // There are four possible paths to region node and phi node
1789     RegionNode *r = new (C) RegionNode(4);
1790     Node *phi = new (C) PhiNode(r, Type::DOUBLE);
1791 
1792     // Build the first if node: if (x &lt;= 0.0)
1793     // Node for 0 constant
1794     Node *zeronode = makecon(TypeD::ZERO);
1795     // Check x:0
1796     Node *cmp = _gvn.transform(new (C) CmpDNode(x, zeronode));
1797     // Check: If (x&lt;=0) then go complex path
1798     Node *bol1 = _gvn.transform(new (C) BoolNode( cmp, BoolTest::le ));
1799     // Branch either way
1800     IfNode *if1 = create_and_xform_if(control(),bol1, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1801     // Fast path taken; set region slot 3
1802     Node *fast_taken = _gvn.transform(new (C) IfFalseNode(if1));
1803     r-&gt;init_req(3,fast_taken); // Capture fast-control
1804 
1805     // Fast path not-taken, i.e. slow path
1806     Node *complex_path = _gvn.transform(new (C) IfTrueNode(if1));
1807 
1808     // Set fast path result
1809     Node *fast_result = _gvn.transform(new (C) PowDNode(C, control(), x, y));
1810     phi-&gt;init_req(3, fast_result);
1811 
1812     // Complex path
1813     // Build the second if node (if y is long)
1814     // Node for (long)y
1815     Node *longy = _gvn.transform(new (C) ConvD2LNode(y));
1816     // Node for (double)((long) y)
1817     Node *doublelongy= _gvn.transform(new (C) ConvL2DNode(longy));
1818     // Check (double)((long) y) : y
1819     Node *cmplongy= _gvn.transform(new (C) CmpDNode(doublelongy, y));
1820     // Check if (y isn't long) then go to slow path
1821 
1822     Node *bol2 = _gvn.transform(new (C) BoolNode( cmplongy, BoolTest::ne ));
1823     // Branch either way
1824     IfNode *if2 = create_and_xform_if(complex_path,bol2, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1825     Node* ylong_path = _gvn.transform(new (C) IfFalseNode(if2));
1826 
1827     Node *slow_path = _gvn.transform(new (C) IfTrueNode(if2));
1828 
1829     // Calculate DPow(abs(x), y)*(1 &amp; (long)y)
1830     // Node for constant 1
1831     Node *conone = longcon(1);
1832     // 1&amp; (long)y
1833     Node *signnode= _gvn.transform(new (C) AndLNode(conone, longy));
1834 
1835     // A huge number is always even. Detect a huge number by checking
1836     // if y + 1 == y and set integer to be tested for parity to 0.
1837     // Required for corner case:
1838     // (long)9.223372036854776E18 = max_jlong
1839     // (double)(long)9.223372036854776E18 = 9.223372036854776E18
1840     // max_jlong is odd but 9.223372036854776E18 is even
1841     Node* yplus1 = _gvn.transform(new (C) AddDNode(y, makecon(TypeD::make(1))));
1842     Node *cmpyplus1= _gvn.transform(new (C) CmpDNode(yplus1, y));
1843     Node *bolyplus1 = _gvn.transform(new (C) BoolNode( cmpyplus1, BoolTest::eq ));
1844     Node* correctedsign = NULL;
1845     if (ConditionalMoveLimit != 0) {
1846       correctedsign = _gvn.transform( CMoveNode::make(C, NULL, bolyplus1, signnode, longcon(0), TypeLong::LONG));
1847     } else {
1848       IfNode *ifyplus1 = create_and_xform_if(ylong_path,bolyplus1, PROB_FAIR, COUNT_UNKNOWN);
1849       RegionNode *r = new (C) RegionNode(3);
1850       Node *phi = new (C) PhiNode(r, TypeLong::LONG);
1851       r-&gt;init_req(1, _gvn.transform(new (C) IfFalseNode(ifyplus1)));
1852       r-&gt;init_req(2, _gvn.transform(new (C) IfTrueNode(ifyplus1)));
1853       phi-&gt;init_req(1, signnode);
1854       phi-&gt;init_req(2, longcon(0));
1855       correctedsign = _gvn.transform(phi);
1856       ylong_path = _gvn.transform(r);
1857       record_for_igvn(r);
1858     }
1859 
1860     // zero node
1861     Node *conzero = longcon(0);
1862     // Check (1&amp;(long)y)==0?
1863     Node *cmpeq1 = _gvn.transform(new (C) CmpLNode(correctedsign, conzero));
1864     // Check if (1&amp;(long)y)!=0?, if so the result is negative
1865     Node *bol3 = _gvn.transform(new (C) BoolNode( cmpeq1, BoolTest::ne ));
1866     // abs(x)
1867     Node *absx=_gvn.transform(new (C) AbsDNode(x));
1868     // abs(x)^y
1869     Node *absxpowy = _gvn.transform(new (C) PowDNode(C, control(), absx, y));
1870     // -abs(x)^y
1871     Node *negabsxpowy = _gvn.transform(new (C) NegDNode (absxpowy));
1872     // (1&amp;(long)y)==1?-DPow(abs(x), y):DPow(abs(x), y)
1873     Node *signresult = NULL;
1874     if (ConditionalMoveLimit != 0) {
1875       signresult = _gvn.transform( CMoveNode::make(C, NULL, bol3, absxpowy, negabsxpowy, Type::DOUBLE));
1876     } else {
1877       IfNode *ifyeven = create_and_xform_if(ylong_path,bol3, PROB_FAIR, COUNT_UNKNOWN);
1878       RegionNode *r = new (C) RegionNode(3);
1879       Node *phi = new (C) PhiNode(r, Type::DOUBLE);
1880       r-&gt;init_req(1, _gvn.transform(new (C) IfFalseNode(ifyeven)));
1881       r-&gt;init_req(2, _gvn.transform(new (C) IfTrueNode(ifyeven)));
1882       phi-&gt;init_req(1, absxpowy);
1883       phi-&gt;init_req(2, negabsxpowy);
1884       signresult = _gvn.transform(phi);
1885       ylong_path = _gvn.transform(r);
1886       record_for_igvn(r);
1887     }
1888     // Set complex path fast result
1889     r-&gt;init_req(2, ylong_path);
1890     phi-&gt;init_req(2, signresult);
1891 
1892     static const jlong nan_bits = CONST64(0x7ff8000000000000);
1893     Node *slow_result = makecon(TypeD::make(*(double*)&amp;nan_bits)); // return NaN
1894     r-&gt;init_req(1,slow_path);
1895     phi-&gt;init_req(1,slow_result);
1896 
1897     // Post merge
1898     set_control(_gvn.transform(r));
1899     record_for_igvn(r);
1900     result = _gvn.transform(phi);
1901   }
1902 
1903   finish_pow_exp(result, x, y, OptoRuntime::Math_DD_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dpow), "POW");
1904 
1905   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1906   return true;
1907 }
1908 
1909 //------------------------------runtime_math-----------------------------
1910 bool LibraryCallKit::runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName) {
1911   assert(call_type == OptoRuntime::Math_DD_D_Type() || call_type == OptoRuntime::Math_D_D_Type(),
1912          "must be (DD)D or (D)D type");
1913 
1914   // Inputs
1915   Node* a = round_double_node(argument(0));
1916   Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : NULL;
1917 
1918   const TypePtr* no_memory_effects = NULL;
1919   Node* trig = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1920                                  no_memory_effects,
1921                                  a, top(), b, b ? top() : NULL);
1922   Node* value = _gvn.transform(new (C) ProjNode(trig, TypeFunc::Parms+0));
1923 #ifdef ASSERT
1924   Node* value_top = _gvn.transform(new (C) ProjNode(trig, TypeFunc::Parms+1));
1925   assert(value_top == top(), "second value must be top");
1926 #endif
1927 
1928   set_result(value);
1929   return true;
1930 }
1931 
1932 //------------------------------inline_math_native-----------------------------
1933 bool LibraryCallKit::inline_math_native(vmIntrinsics::ID id) {
1934 #define FN_PTR(f) CAST_FROM_FN_PTR(address, f)
1935   switch (id) {
1936     // These intrinsics are not properly supported on all hardware
1937   case vmIntrinsics::_dcos:   return Matcher::has_match_rule(Op_CosD)   ? inline_trig(id) :
1938     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dcos),   "COS");
1939   case vmIntrinsics::_dsin:   return Matcher::has_match_rule(Op_SinD)   ? inline_trig(id) :
1940     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dsin),   "SIN");
1941   case vmIntrinsics::_dtan:   return Matcher::has_match_rule(Op_TanD)   ? inline_trig(id) :
1942     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dtan),   "TAN");
1943 
1944   case vmIntrinsics::_dlog:   return Matcher::has_match_rule(Op_LogD)   ? inline_math(id) :
1945     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog),   "LOG");
1946   case vmIntrinsics::_dlog10: return Matcher::has_match_rule(Op_Log10D) ? inline_math(id) :
1947     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog10), "LOG10");
1948 
1949     // These intrinsics are supported on all hardware
1950   case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_math(id) : false;
1951   case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_math(id) : false;
1952 
1953   case vmIntrinsics::_dexp:   return Matcher::has_match_rule(Op_ExpD)   ? inline_exp()    :
1954     runtime_math(OptoRuntime::Math_D_D_Type(),  FN_PTR(SharedRuntime::dexp),  "EXP");
1955   case vmIntrinsics::_dpow:   return Matcher::has_match_rule(Op_PowD)   ? inline_pow()    :
1956     runtime_math(OptoRuntime::Math_DD_D_Type(), FN_PTR(SharedRuntime::dpow),  "POW");
1957 #undef FN_PTR
1958 
1959    // These intrinsics are not yet correctly implemented
1960   case vmIntrinsics::_datan2:
1961     return false;
1962 
1963   default:
1964     fatal_unexpected_iid(id);
1965     return false;
1966   }
1967 }
1968 
1969 static bool is_simple_name(Node* n) {
1970   return (n-&gt;req() == 1         // constant
1971           || (n-&gt;is_Type() &amp;&amp; n-&gt;as_Type()-&gt;type()-&gt;singleton())
1972           || n-&gt;is_Proj()       // parameter or return value
1973           || n-&gt;is_Phi()        // local of some sort
1974           );
1975 }
1976 
1977 //----------------------------inline_min_max-----------------------------------
1978 bool LibraryCallKit::inline_min_max(vmIntrinsics::ID id) {
1979   set_result(generate_min_max(id, argument(0), argument(1)));
1980   return true;
1981 }
1982 
1983 void LibraryCallKit::inline_math_mathExact(Node* math, Node *test) {
1984   Node* bol = _gvn.transform( new (C) BoolNode(test, BoolTest::overflow) );
1985   IfNode* check = create_and_map_if(control(), bol, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
1986   Node* fast_path = _gvn.transform( new (C) IfFalseNode(check));
1987   Node* slow_path = _gvn.transform( new (C) IfTrueNode(check) );
1988 
1989   {
1990     PreserveJVMState pjvms(this);
1991     PreserveReexecuteState preexecs(this);
1992     jvms()-&gt;set_should_reexecute(true);
1993 
1994     set_control(slow_path);
1995     set_i_o(i_o());
1996 
1997     uncommon_trap(Deoptimization::Reason_intrinsic,
1998                   Deoptimization::Action_none);
1999   }
2000 
2001   set_control(fast_path);
2002   set_result(math);
2003 }
2004 
2005 template &lt;typename OverflowOp&gt;
2006 bool LibraryCallKit::inline_math_overflow(Node* arg1, Node* arg2) {
2007   typedef typename OverflowOp::MathOp MathOp;
2008 
2009   MathOp* mathOp = new(C) MathOp(arg1, arg2);
2010   Node* operation = _gvn.transform( mathOp );
2011   Node* ofcheck = _gvn.transform( new(C) OverflowOp(arg1, arg2) );
2012   inline_math_mathExact(operation, ofcheck);
2013   return true;
2014 }
2015 
2016 bool LibraryCallKit::inline_math_addExactI(bool is_increment) {
2017   return inline_math_overflow&lt;OverflowAddINode&gt;(argument(0), is_increment ? intcon(1) : argument(1));
2018 }
2019 
2020 bool LibraryCallKit::inline_math_addExactL(bool is_increment) {
2021   return inline_math_overflow&lt;OverflowAddLNode&gt;(argument(0), is_increment ? longcon(1) : argument(2));
2022 }
2023 
2024 bool LibraryCallKit::inline_math_subtractExactI(bool is_decrement) {
2025   return inline_math_overflow&lt;OverflowSubINode&gt;(argument(0), is_decrement ? intcon(1) : argument(1));
2026 }
2027 
2028 bool LibraryCallKit::inline_math_subtractExactL(bool is_decrement) {
2029   return inline_math_overflow&lt;OverflowSubLNode&gt;(argument(0), is_decrement ? longcon(1) : argument(2));
2030 }
2031 
2032 bool LibraryCallKit::inline_math_negateExactI() {
2033   return inline_math_overflow&lt;OverflowSubINode&gt;(intcon(0), argument(0));
2034 }
2035 
2036 bool LibraryCallKit::inline_math_negateExactL() {
2037   return inline_math_overflow&lt;OverflowSubLNode&gt;(longcon(0), argument(0));
2038 }
2039 
2040 bool LibraryCallKit::inline_math_multiplyExactI() {
2041   return inline_math_overflow&lt;OverflowMulINode&gt;(argument(0), argument(1));
2042 }
2043 
2044 bool LibraryCallKit::inline_math_multiplyExactL() {
2045   return inline_math_overflow&lt;OverflowMulLNode&gt;(argument(0), argument(2));
2046 }
2047 
2048 Node*
2049 LibraryCallKit::generate_min_max(vmIntrinsics::ID id, Node* x0, Node* y0) {
2050   // These are the candidate return value:
2051   Node* xvalue = x0;
2052   Node* yvalue = y0;
2053 
2054   if (xvalue == yvalue) {
2055     return xvalue;
2056   }
2057 
2058   bool want_max = (id == vmIntrinsics::_max);
2059 
2060   const TypeInt* txvalue = _gvn.type(xvalue)-&gt;isa_int();
2061   const TypeInt* tyvalue = _gvn.type(yvalue)-&gt;isa_int();
2062   if (txvalue == NULL || tyvalue == NULL)  return top();
2063   // This is not really necessary, but it is consistent with a
2064   // hypothetical MaxINode::Value method:
2065   int widen = MAX2(txvalue-&gt;_widen, tyvalue-&gt;_widen);
2066 
2067   // %%% This folding logic should (ideally) be in a different place.
2068   // Some should be inside IfNode, and there to be a more reliable
2069   // transformation of ?: style patterns into cmoves.  We also want
2070   // more powerful optimizations around cmove and min/max.
2071 
2072   // Try to find a dominating comparison of these guys.
2073   // It can simplify the index computation for Arrays.copyOf
2074   // and similar uses of System.arraycopy.
2075   // First, compute the normalized version of CmpI(x, y).
2076   int   cmp_op = Op_CmpI;
2077   Node* xkey = xvalue;
2078   Node* ykey = yvalue;
2079   Node* ideal_cmpxy = _gvn.transform(new(C) CmpINode(xkey, ykey));
2080   if (ideal_cmpxy-&gt;is_Cmp()) {
2081     // E.g., if we have CmpI(length - offset, count),
2082     // it might idealize to CmpI(length, count + offset)
2083     cmp_op = ideal_cmpxy-&gt;Opcode();
2084     xkey = ideal_cmpxy-&gt;in(1);
2085     ykey = ideal_cmpxy-&gt;in(2);
2086   }
2087 
2088   // Start by locating any relevant comparisons.
2089   Node* start_from = (xkey-&gt;outcnt() &lt; ykey-&gt;outcnt()) ? xkey : ykey;
2090   Node* cmpxy = NULL;
2091   Node* cmpyx = NULL;
2092   for (DUIterator_Fast kmax, k = start_from-&gt;fast_outs(kmax); k &lt; kmax; k++) {
2093     Node* cmp = start_from-&gt;fast_out(k);
2094     if (cmp-&gt;outcnt() &gt; 0 &amp;&amp;            // must have prior uses
2095         cmp-&gt;in(0) == NULL &amp;&amp;           // must be context-independent
2096         cmp-&gt;Opcode() == cmp_op) {      // right kind of compare
2097       if (cmp-&gt;in(1) == xkey &amp;&amp; cmp-&gt;in(2) == ykey)  cmpxy = cmp;
2098       if (cmp-&gt;in(1) == ykey &amp;&amp; cmp-&gt;in(2) == xkey)  cmpyx = cmp;
2099     }
2100   }
2101 
2102   const int NCMPS = 2;
2103   Node* cmps[NCMPS] = { cmpxy, cmpyx };
2104   int cmpn;
2105   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2106     if (cmps[cmpn] != NULL)  break;     // find a result
2107   }
2108   if (cmpn &lt; NCMPS) {
2109     // Look for a dominating test that tells us the min and max.
2110     int depth = 0;                // Limit search depth for speed
2111     Node* dom = control();
2112     for (; dom != NULL; dom = IfNode::up_one_dom(dom, true)) {
2113       if (++depth &gt;= 100)  break;
2114       Node* ifproj = dom;
2115       if (!ifproj-&gt;is_Proj())  continue;
2116       Node* iff = ifproj-&gt;in(0);
2117       if (!iff-&gt;is_If())  continue;
2118       Node* bol = iff-&gt;in(1);
2119       if (!bol-&gt;is_Bool())  continue;
2120       Node* cmp = bol-&gt;in(1);
2121       if (cmp == NULL)  continue;
2122       for (cmpn = 0; cmpn &lt; NCMPS; cmpn++)
2123         if (cmps[cmpn] == cmp)  break;
2124       if (cmpn == NCMPS)  continue;
2125       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2126       if (ifproj-&gt;is_IfFalse())  btest = BoolTest(btest).negate();
2127       if (cmp-&gt;in(1) == ykey)    btest = BoolTest(btest).commute();
2128       // At this point, we know that 'x btest y' is true.
2129       switch (btest) {
2130       case BoolTest::eq:
2131         // They are proven equal, so we can collapse the min/max.
2132         // Either value is the answer.  Choose the simpler.
2133         if (is_simple_name(yvalue) &amp;&amp; !is_simple_name(xvalue))
2134           return yvalue;
2135         return xvalue;
2136       case BoolTest::lt:          // x &lt; y
2137       case BoolTest::le:          // x &lt;= y
2138         return (want_max ? yvalue : xvalue);
2139       case BoolTest::gt:          // x &gt; y
2140       case BoolTest::ge:          // x &gt;= y
2141         return (want_max ? xvalue : yvalue);
2142       }
2143     }
2144   }
2145 
2146   // We failed to find a dominating test.
2147   // Let's pick a test that might GVN with prior tests.
2148   Node*          best_bol   = NULL;
2149   BoolTest::mask best_btest = BoolTest::illegal;
2150   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2151     Node* cmp = cmps[cmpn];
2152     if (cmp == NULL)  continue;
2153     for (DUIterator_Fast jmax, j = cmp-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2154       Node* bol = cmp-&gt;fast_out(j);
2155       if (!bol-&gt;is_Bool())  continue;
2156       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2157       if (btest == BoolTest::eq || btest == BoolTest::ne)  continue;
2158       if (cmp-&gt;in(1) == ykey)   btest = BoolTest(btest).commute();
2159       if (bol-&gt;outcnt() &gt; (best_bol == NULL ? 0 : best_bol-&gt;outcnt())) {
2160         best_bol   = bol-&gt;as_Bool();
2161         best_btest = btest;
2162       }
2163     }
2164   }
2165 
2166   Node* answer_if_true  = NULL;
2167   Node* answer_if_false = NULL;
2168   switch (best_btest) {
2169   default:
2170     if (cmpxy == NULL)
2171       cmpxy = ideal_cmpxy;
2172     best_bol = _gvn.transform(new(C) BoolNode(cmpxy, BoolTest::lt));
2173     // and fall through:
2174   case BoolTest::lt:          // x &lt; y
2175   case BoolTest::le:          // x &lt;= y
2176     answer_if_true  = (want_max ? yvalue : xvalue);
2177     answer_if_false = (want_max ? xvalue : yvalue);
2178     break;
2179   case BoolTest::gt:          // x &gt; y
2180   case BoolTest::ge:          // x &gt;= y
2181     answer_if_true  = (want_max ? xvalue : yvalue);
2182     answer_if_false = (want_max ? yvalue : xvalue);
2183     break;
2184   }
2185 
2186   jint hi, lo;
2187   if (want_max) {
2188     // We can sharpen the minimum.
2189     hi = MAX2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2190     lo = MAX2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2191   } else {
2192     // We can sharpen the maximum.
2193     hi = MIN2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2194     lo = MIN2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2195   }
2196 
2197   // Use a flow-free graph structure, to avoid creating excess control edges
2198   // which could hinder other optimizations.
2199   // Since Math.min/max is often used with arraycopy, we want
2200   // tightly_coupled_allocation to be able to see beyond min/max expressions.
2201   Node* cmov = CMoveNode::make(C, NULL, best_bol,
2202                                answer_if_false, answer_if_true,
2203                                TypeInt::make(lo, hi, widen));
2204 
2205   return _gvn.transform(cmov);
2206 
2207   /*
2208   // This is not as desirable as it may seem, since Min and Max
2209   // nodes do not have a full set of optimizations.
2210   // And they would interfere, anyway, with 'if' optimizations
2211   // and with CMoveI canonical forms.
2212   switch (id) {
2213   case vmIntrinsics::_min:
2214     result_val = _gvn.transform(new (C, 3) MinINode(x,y)); break;
2215   case vmIntrinsics::_max:
2216     result_val = _gvn.transform(new (C, 3) MaxINode(x,y)); break;
2217   default:
2218     ShouldNotReachHere();
2219   }
2220   */
2221 }
2222 
2223 inline int
2224 LibraryCallKit::classify_unsafe_addr(Node* &amp;base, Node* &amp;offset) {
2225   const TypePtr* base_type = TypePtr::NULL_PTR;
2226   if (base != NULL)  base_type = _gvn.type(base)-&gt;isa_ptr();
2227   if (base_type == NULL) {
2228     // Unknown type.
2229     return Type::AnyPtr;
2230   } else if (base_type == TypePtr::NULL_PTR) {
2231     // Since this is a NULL+long form, we have to switch to a rawptr.
2232     base   = _gvn.transform(new (C) CastX2PNode(offset));
2233     offset = MakeConX(0);
2234     return Type::RawPtr;
2235   } else if (base_type-&gt;base() == Type::RawPtr) {
2236     return Type::RawPtr;
2237   } else if (base_type-&gt;isa_oopptr()) {
2238     // Base is never null =&gt; always a heap address.
2239     if (base_type-&gt;ptr() == TypePtr::NotNull) {
2240       return Type::OopPtr;
2241     }
2242     // Offset is small =&gt; always a heap address.
2243     const TypeX* offset_type = _gvn.type(offset)-&gt;isa_intptr_t();
2244     if (offset_type != NULL &amp;&amp;
2245         base_type-&gt;offset() == 0 &amp;&amp;     // (should always be?)
2246         offset_type-&gt;_lo &gt;= 0 &amp;&amp;
2247         !MacroAssembler::needs_explicit_null_check(offset_type-&gt;_hi)) {
2248       return Type::OopPtr;
2249     }
2250     // Otherwise, it might either be oop+off or NULL+addr.
2251     return Type::AnyPtr;
2252   } else {
2253     // No information:
2254     return Type::AnyPtr;
2255   }
2256 }
2257 
2258 inline Node* LibraryCallKit::make_unsafe_address(Node* base, Node* offset) {
2259   int kind = classify_unsafe_addr(base, offset);
2260   if (kind == Type::RawPtr) {
2261     return basic_plus_adr(top(), base, offset);
2262   } else {
2263     return basic_plus_adr(base, offset);
2264   }
2265 }
2266 
2267 //--------------------------inline_number_methods-----------------------------
2268 // inline int     Integer.numberOfLeadingZeros(int)
2269 // inline int        Long.numberOfLeadingZeros(long)
2270 //
2271 // inline int     Integer.numberOfTrailingZeros(int)
2272 // inline int        Long.numberOfTrailingZeros(long)
2273 //
2274 // inline int     Integer.bitCount(int)
2275 // inline int        Long.bitCount(long)
2276 //
2277 // inline char  Character.reverseBytes(char)
2278 // inline short     Short.reverseBytes(short)
2279 // inline int     Integer.reverseBytes(int)
2280 // inline long       Long.reverseBytes(long)
2281 bool LibraryCallKit::inline_number_methods(vmIntrinsics::ID id) {
2282   Node* arg = argument(0);
2283   Node* n;
2284   switch (id) {
2285   case vmIntrinsics::_numberOfLeadingZeros_i:   n = new (C) CountLeadingZerosINode( arg);  break;
2286   case vmIntrinsics::_numberOfLeadingZeros_l:   n = new (C) CountLeadingZerosLNode( arg);  break;
2287   case vmIntrinsics::_numberOfTrailingZeros_i:  n = new (C) CountTrailingZerosINode(arg);  break;
2288   case vmIntrinsics::_numberOfTrailingZeros_l:  n = new (C) CountTrailingZerosLNode(arg);  break;
2289   case vmIntrinsics::_bitCount_i:               n = new (C) PopCountINode(          arg);  break;
2290   case vmIntrinsics::_bitCount_l:               n = new (C) PopCountLNode(          arg);  break;
2291   case vmIntrinsics::_reverseBytes_c:           n = new (C) ReverseBytesUSNode(0,   arg);  break;
2292   case vmIntrinsics::_reverseBytes_s:           n = new (C) ReverseBytesSNode( 0,   arg);  break;
2293   case vmIntrinsics::_reverseBytes_i:           n = new (C) ReverseBytesINode( 0,   arg);  break;
2294   case vmIntrinsics::_reverseBytes_l:           n = new (C) ReverseBytesLNode( 0,   arg);  break;
2295   default:  fatal_unexpected_iid(id);  break;
2296   }
2297   set_result(_gvn.transform(n));
2298   return true;
2299 }
2300 
2301 //----------------------------inline_unsafe_access----------------------------
2302 
2303 const static BasicType T_ADDRESS_HOLDER = T_LONG;
2304 
2305 // Helper that guards and inserts a pre-barrier.
2306 void LibraryCallKit::insert_pre_barrier(Node* base_oop, Node* offset,
2307                                         Node* pre_val, bool need_mem_bar) {
2308   // We could be accessing the referent field of a reference object. If so, when G1
2309   // is enabled, we need to log the value in the referent field in an SATB buffer.
2310   // This routine performs some compile time filters and generates suitable
2311   // runtime filters that guard the pre-barrier code.
2312   // Also add memory barrier for non volatile load from the referent field
2313   // to prevent commoning of loads across safepoint.
2314   if (!UseG1GC &amp;&amp; !need_mem_bar)
2315     return;
2316 
2317   // Some compile time checks.
2318 
2319   // If offset is a constant, is it java_lang_ref_Reference::_reference_offset?
2320   const TypeX* otype = offset-&gt;find_intptr_t_type();
2321   if (otype != NULL &amp;&amp; otype-&gt;is_con() &amp;&amp;
2322       otype-&gt;get_con() != java_lang_ref_Reference::referent_offset) {
2323     // Constant offset but not the reference_offset so just return
2324     return;
2325   }
2326 
2327   // We only need to generate the runtime guards for instances.
2328   const TypeOopPtr* btype = base_oop-&gt;bottom_type()-&gt;isa_oopptr();
2329   if (btype != NULL) {
2330     if (btype-&gt;isa_aryptr()) {
2331       // Array type so nothing to do
2332       return;
2333     }
2334 
2335     const TypeInstPtr* itype = btype-&gt;isa_instptr();
2336     if (itype != NULL) {
2337       // Can the klass of base_oop be statically determined to be
2338       // _not_ a sub-class of Reference and _not_ Object?
2339       ciKlass* klass = itype-&gt;klass();
2340       if ( klass-&gt;is_loaded() &amp;&amp;
2341           !klass-&gt;is_subtype_of(env()-&gt;Reference_klass()) &amp;&amp;
2342           !env()-&gt;Object_klass()-&gt;is_subtype_of(klass)) {
2343         return;
2344       }
2345     }
2346   }
2347 
2348   // The compile time filters did not reject base_oop/offset so
2349   // we need to generate the following runtime filters
2350   //
2351   // if (offset == java_lang_ref_Reference::_reference_offset) {
2352   //   if (instance_of(base, java.lang.ref.Reference)) {
2353   //     pre_barrier(_, pre_val, ...);
2354   //   }
2355   // }
2356 
2357   float likely   = PROB_LIKELY(  0.999);
2358   float unlikely = PROB_UNLIKELY(0.999);
2359 
2360   IdealKit ideal(this);
2361 #define __ ideal.
2362 
2363   Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset);
2364 
2365   __ if_then(offset, BoolTest::eq, referent_off, unlikely); {
2366       // Update graphKit memory and control from IdealKit.
2367       sync_kit(ideal);
2368 
2369       Node* ref_klass_con = makecon(TypeKlassPtr::make(env()-&gt;Reference_klass()));
2370       Node* is_instof = gen_instanceof(base_oop, ref_klass_con);
2371 
2372       // Update IdealKit memory and control from graphKit.
2373       __ sync_kit(this);
2374 
2375       Node* one = __ ConI(1);
2376       // is_instof == 0 if base_oop == NULL
2377       __ if_then(is_instof, BoolTest::eq, one, unlikely); {
2378 
2379         // Update graphKit from IdeakKit.
2380         sync_kit(ideal);
2381 
2382         // Use the pre-barrier to record the value in the referent field
2383         pre_barrier(false /* do_load */,
2384                     __ ctrl(),
2385                     NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
2386                     pre_val /* pre_val */,
2387                     T_OBJECT);
2388         if (need_mem_bar) {
2389           // Add memory barrier to prevent commoning reads from this field
2390           // across safepoint since GC can change its value.
2391           insert_mem_bar(Op_MemBarCPUOrder);
2392         }
2393         // Update IdealKit from graphKit.
2394         __ sync_kit(this);
2395 
2396       } __ end_if(); // _ref_type != ref_none
2397   } __ end_if(); // offset == referent_offset
2398 
2399   // Final sync IdealKit and GraphKit.
2400   final_sync(ideal);
2401 #undef __
2402 }
2403 
2404 
2405 // Interpret Unsafe.fieldOffset cookies correctly:
2406 extern jlong Unsafe_field_offset_to_byte_offset(jlong field_offset);
2407 
2408 const TypeOopPtr* LibraryCallKit::sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr) {
2409   // Attempt to infer a sharper value type from the offset and base type.
2410   ciKlass* sharpened_klass = NULL;
2411 
2412   // See if it is an instance field, with an object type.
2413   if (alias_type-&gt;field() != NULL) {
2414     assert(!is_native_ptr, "native pointer op cannot use a java address");
2415     if (alias_type-&gt;field()-&gt;type()-&gt;is_klass()) {
2416       sharpened_klass = alias_type-&gt;field()-&gt;type()-&gt;as_klass();
2417     }
2418   }
2419 
2420   // See if it is a narrow oop array.
2421   if (adr_type-&gt;isa_aryptr()) {
2422     if (adr_type-&gt;offset() &gt;= objArrayOopDesc::base_offset_in_bytes()) {
2423       const TypeOopPtr *elem_type = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_oopptr();
2424       if (elem_type != NULL) {
2425         sharpened_klass = elem_type-&gt;klass();
2426       }
2427     }
2428   }
2429 
2430   // The sharpened class might be unloaded if there is no class loader
2431   // contraint in place.
2432   if (sharpened_klass != NULL &amp;&amp; sharpened_klass-&gt;is_loaded()) {
2433     const TypeOopPtr* tjp = TypeOopPtr::make_from_klass(sharpened_klass);
2434 
2435 #ifndef PRODUCT
2436     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
2437       tty-&gt;print("  from base type: ");  adr_type-&gt;dump();
2438       tty-&gt;print("  sharpened value: ");  tjp-&gt;dump();
2439     }
2440 #endif
2441     // Sharpen the value type.
2442     return tjp;
2443   }
2444   return NULL;
2445 }
2446 
2447 bool LibraryCallKit::inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile) {
2448   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2449 
2450 #ifndef PRODUCT
2451   {
2452     ResourceMark rm;
2453     // Check the signatures.
2454     ciSignature* sig = callee()-&gt;signature();
2455 #ifdef ASSERT
2456     if (!is_store) {
2457       // Object getObject(Object base, int/long offset), etc.
2458       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2459       if (rtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::getAddress_name())
2460           rtype = T_ADDRESS;  // it is really a C void*
2461       assert(rtype == type, "getter must return the expected value");
2462       if (!is_native_ptr) {
2463         assert(sig-&gt;count() == 2, "oop getter has 2 arguments");
2464         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "getter base is object");
2465         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "getter offset is correct");
2466       } else {
2467         assert(sig-&gt;count() == 1, "native getter has 1 argument");
2468         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "getter base is long");
2469       }
2470     } else {
2471       // void putObject(Object base, int/long offset, Object x), etc.
2472       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, "putter must not return a value");
2473       if (!is_native_ptr) {
2474         assert(sig-&gt;count() == 3, "oop putter has 3 arguments");
2475         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "putter base is object");
2476         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "putter offset is correct");
2477       } else {
2478         assert(sig-&gt;count() == 2, "native putter has 2 arguments");
2479         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "putter base is long");
2480       }
2481       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
2482       if (vtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::putAddress_name())
2483         vtype = T_ADDRESS;  // it is really a C void*
2484       assert(vtype == type, "putter must accept the expected value");
2485     }
2486 #endif // ASSERT
2487  }
2488 #endif //PRODUCT
2489 
2490   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2491 
2492   Node* receiver = argument(0);  // type: oop
2493 
2494   // Build address expression.  See the code in inline_unsafe_prefetch.
2495   Node* adr;
2496   Node* heap_base_oop = top();
2497   Node* offset = top();
2498   Node* val;
2499 
2500   if (!is_native_ptr) {
2501     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2502     Node* base = argument(1);  // type: oop
2503     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2504     offset = argument(2);  // type: long
2505     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2506     // to be plain byte offsets, which are also the same as those accepted
2507     // by oopDesc::field_base.
2508     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2509            "fieldOffset must be byte-scaled");
2510     // 32-bit machines ignore the high half!
2511     offset = ConvL2X(offset);
2512     adr = make_unsafe_address(base, offset);
2513     heap_base_oop = base;
2514     val = is_store ? argument(4) : NULL;
2515   } else {
2516     Node* ptr = argument(1);  // type: long
2517     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2518     adr = make_unsafe_address(NULL, ptr);
2519     val = is_store ? argument(3) : NULL;
2520   }
2521 
2522   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2523 
2524   // First guess at the value type.
2525   const Type *value_type = Type::get_const_basic_type(type);
2526 
2527   // Try to categorize the address.  If it comes up as TypeJavaPtr::BOTTOM,
2528   // there was not enough information to nail it down.
2529   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2530   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2531 
2532   // We will need memory barriers unless we can determine a unique
2533   // alias category for this reference.  (Note:  If for some reason
2534   // the barriers get omitted and the unsafe reference begins to "pollute"
2535   // the alias analysis of the rest of the graph, either Compile::can_alias
2536   // or Compile::must_alias will throw a diagnostic assert.)
2537   bool need_mem_bar = (alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM);
2538 
2539   // If we are reading the value of the referent field of a Reference
2540   // object (either by using Unsafe directly or through reflection)
2541   // then, if G1 is enabled, we need to record the referent in an
2542   // SATB log buffer using the pre-barrier mechanism.
2543   // Also we need to add memory barrier to prevent commoning reads
2544   // from this field across safepoint since GC can change its value.
2545   bool need_read_barrier = !is_native_ptr &amp;&amp; !is_store &amp;&amp;
2546                            offset != top() &amp;&amp; heap_base_oop != top();
2547 
2548   if (!is_store &amp;&amp; type == T_OBJECT) {
2549     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type, is_native_ptr);
2550     if (tjp != NULL) {
2551       value_type = tjp;
2552     }
2553   }
2554 
2555   receiver = null_check(receiver);
2556   if (stopped()) {
2557     return true;
2558   }
2559   // Heap pointers get a null-check from the interpreter,
2560   // as a courtesy.  However, this is not guaranteed by Unsafe,
2561   // and it is not possible to fully distinguish unintended nulls
2562   // from intended ones in this API.
2563 
2564   if (is_volatile) {
2565     // We need to emit leading and trailing CPU membars (see below) in
2566     // addition to memory membars when is_volatile. This is a little
2567     // too strong, but avoids the need to insert per-alias-type
2568     // volatile membars (for stores; compare Parse::do_put_xxx), which
2569     // we cannot do effectively here because we probably only have a
2570     // rough approximation of type.
2571     need_mem_bar = true;
2572     // For Stores, place a memory ordering barrier now.
2573     if (is_store) {
2574       insert_mem_bar(Op_MemBarRelease);
2575     } else {
2576       if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2577         insert_mem_bar(Op_MemBarVolatile);
2578       }
2579     }
2580   }
2581 
2582   // Memory barrier to prevent normal and 'unsafe' accesses from
2583   // bypassing each other.  Happens after null checks, so the
2584   // exception paths do not take memory state from the memory barrier,
2585   // so there's no problems making a strong assert about mixing users
2586   // of safe &amp; unsafe memory.  Otherwise fails in a CTW of rt.jar
2587   // around 5701, class sun/reflect/UnsafeBooleanFieldAccessorImpl.
2588   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2589 
2590   if (!is_store) {
2591     Node* p = make_load(control(), adr, value_type, type, adr_type, MemNode::unordered, is_volatile);
2592     // load value
2593     switch (type) {
2594     case T_BOOLEAN:
2595     case T_CHAR:
2596     case T_BYTE:
2597     case T_SHORT:
2598     case T_INT:
2599     case T_LONG:
2600     case T_FLOAT:
2601     case T_DOUBLE:
2602       break;
2603     case T_OBJECT:
2604       if (need_read_barrier) {
2605         insert_pre_barrier(heap_base_oop, offset, p, !(is_volatile || need_mem_bar));
2606       }
2607       break;
2608     case T_ADDRESS:
2609       // Cast to an int type.
2610       p = _gvn.transform(new (C) CastP2XNode(NULL, p));
2611       p = ConvX2UL(p);
2612       break;
2613     default:
2614       fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2615       break;
2616     }
2617     // The load node has the control of the preceding MemBarCPUOrder.  All
2618     // following nodes will have the control of the MemBarCPUOrder inserted at
2619     // the end of this method.  So, pushing the load onto the stack at a later
2620     // point is fine.
2621     set_result(p);
2622   } else {
2623     // place effect of store into memory
2624     switch (type) {
2625     case T_DOUBLE:
2626       val = dstore_rounding(val);
2627       break;
2628     case T_ADDRESS:
2629       // Repackage the long as a pointer.
2630       val = ConvL2X(val);
2631       val = _gvn.transform(new (C) CastX2PNode(val));
2632       break;
2633     }
2634 
2635     MemNode::MemOrd mo = is_volatile ? MemNode::release : MemNode::unordered;
2636     if (type != T_OBJECT ) {
2637       (void) store_to_memory(control(), adr, val, type, adr_type, mo, is_volatile);
2638     } else {
2639       // Possibly an oop being stored to Java heap or native memory
2640       if (!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(heap_base_oop))) {
2641         // oop to Java heap.
2642         (void) store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2643       } else {
2644         // We can't tell at compile time if we are storing in the Java heap or outside
2645         // of it. So we need to emit code to conditionally do the proper type of
2646         // store.
2647 
2648         IdealKit ideal(this);
2649 #define __ ideal.
2650         // QQQ who knows what probability is here??
2651         __ if_then(heap_base_oop, BoolTest::ne, null(), PROB_UNLIKELY(0.999)); {
2652           // Sync IdealKit and graphKit.
2653           sync_kit(ideal);
2654           Node* st = store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2655           // Update IdealKit memory.
2656           __ sync_kit(this);
2657         } __ else_(); {
2658           __ store(__ ctrl(), adr, val, type, alias_type-&gt;index(), mo, is_volatile);
2659         } __ end_if();
2660         // Final sync IdealKit and GraphKit.
2661         final_sync(ideal);
2662 #undef __
2663       }
2664     }
2665   }
2666 
2667   if (is_volatile) {
2668     if (!is_store) {
2669       insert_mem_bar(Op_MemBarAcquire);
2670     } else {
2671       if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2672         insert_mem_bar(Op_MemBarVolatile);
2673       }
2674     }
2675   }
2676 
2677   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2678 
2679   return true;
2680 }
2681 
2682 //----------------------------inline_unsafe_prefetch----------------------------
2683 
2684 bool LibraryCallKit::inline_unsafe_prefetch(bool is_native_ptr, bool is_store, bool is_static) {
2685 #ifndef PRODUCT
2686   {
2687     ResourceMark rm;
2688     // Check the signatures.
2689     ciSignature* sig = callee()-&gt;signature();
2690 #ifdef ASSERT
2691     // Object getObject(Object base, int/long offset), etc.
2692     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2693     if (!is_native_ptr) {
2694       assert(sig-&gt;count() == 2, "oop prefetch has 2 arguments");
2695       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "prefetch base is object");
2696       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "prefetcha offset is correct");
2697     } else {
2698       assert(sig-&gt;count() == 1, "native prefetch has 1 argument");
2699       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "prefetch base is long");
2700     }
2701 #endif // ASSERT
2702   }
2703 #endif // !PRODUCT
2704 
2705   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2706 
2707   const int idx = is_static ? 0 : 1;
2708   if (!is_static) {
2709     null_check_receiver();
2710     if (stopped()) {
2711       return true;
2712     }
2713   }
2714 
2715   // Build address expression.  See the code in inline_unsafe_access.
2716   Node *adr;
2717   if (!is_native_ptr) {
2718     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2719     Node* base   = argument(idx + 0);  // type: oop
2720     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2721     Node* offset = argument(idx + 1);  // type: long
2722     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2723     // to be plain byte offsets, which are also the same as those accepted
2724     // by oopDesc::field_base.
2725     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2726            "fieldOffset must be byte-scaled");
2727     // 32-bit machines ignore the high half!
2728     offset = ConvL2X(offset);
2729     adr = make_unsafe_address(base, offset);
2730   } else {
2731     Node* ptr = argument(idx + 0);  // type: long
2732     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2733     adr = make_unsafe_address(NULL, ptr);
2734   }
2735 
2736   // Generate the read or write prefetch
2737   Node *prefetch;
2738   if (is_store) {
2739     prefetch = new (C) PrefetchWriteNode(i_o(), adr);
2740   } else {
2741     prefetch = new (C) PrefetchReadNode(i_o(), adr);
2742   }
2743   prefetch-&gt;init_req(0, control());
2744   set_i_o(_gvn.transform(prefetch));
2745 
2746   return true;
2747 }
2748 
2749 //----------------------------inline_unsafe_load_store----------------------------
2750 // This method serves a couple of different customers (depending on LoadStoreKind):
2751 //
2752 // LS_cmpxchg:
2753 //   public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object x);
2754 //   public final native boolean compareAndSwapInt(   Object o, long offset, int    expected, int    x);
2755 //   public final native boolean compareAndSwapLong(  Object o, long offset, long   expected, long   x);
2756 //
2757 // LS_xadd:
2758 //   public int  getAndAddInt( Object o, long offset, int  delta)
2759 //   public long getAndAddLong(Object o, long offset, long delta)
2760 //
2761 // LS_xchg:
2762 //   int    getAndSet(Object o, long offset, int    newValue)
2763 //   long   getAndSet(Object o, long offset, long   newValue)
2764 //   Object getAndSet(Object o, long offset, Object newValue)
2765 //
2766 bool LibraryCallKit::inline_unsafe_load_store(BasicType type, LoadStoreKind kind) {
2767   // This basic scheme here is the same as inline_unsafe_access, but
2768   // differs in enough details that combining them would make the code
2769   // overly confusing.  (This is a true fact! I originally combined
2770   // them, but even I was confused by it!) As much code/comments as
2771   // possible are retained from inline_unsafe_access though to make
2772   // the correspondences clearer. - dl
2773 
2774   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2775 
2776 #ifndef PRODUCT
2777   BasicType rtype;
2778   {
2779     ResourceMark rm;
2780     // Check the signatures.
2781     ciSignature* sig = callee()-&gt;signature();
2782     rtype = sig-&gt;return_type()-&gt;basic_type();
2783     if (kind == LS_xadd || kind == LS_xchg) {
2784       // Check the signatures.
2785 #ifdef ASSERT
2786       assert(rtype == type, "get and set must return the expected type");
2787       assert(sig-&gt;count() == 3, "get and set has 3 arguments");
2788       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "get and set base is object");
2789       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "get and set offset is long");
2790       assert(sig-&gt;type_at(2)-&gt;basic_type() == type, "get and set must take expected type as new value/delta");
2791 #endif // ASSERT
2792     } else if (kind == LS_cmpxchg) {
2793       // Check the signatures.
2794 #ifdef ASSERT
2795       assert(rtype == T_BOOLEAN, "CAS must return boolean");
2796       assert(sig-&gt;count() == 4, "CAS has 4 arguments");
2797       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "CAS base is object");
2798       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "CAS offset is long");
2799 #endif // ASSERT
2800     } else {
2801       ShouldNotReachHere();
2802     }
2803   }
2804 #endif //PRODUCT
2805 
2806   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2807 
2808   // Get arguments:
2809   Node* receiver = NULL;
2810   Node* base     = NULL;
2811   Node* offset   = NULL;
2812   Node* oldval   = NULL;
2813   Node* newval   = NULL;
2814   if (kind == LS_cmpxchg) {
2815     const bool two_slot_type = type2size[type] == 2;
2816     receiver = argument(0);  // type: oop
2817     base     = argument(1);  // type: oop
2818     offset   = argument(2);  // type: long
2819     oldval   = argument(4);  // type: oop, int, or long
2820     newval   = argument(two_slot_type ? 6 : 5);  // type: oop, int, or long
2821   } else if (kind == LS_xadd || kind == LS_xchg){
2822     receiver = argument(0);  // type: oop
2823     base     = argument(1);  // type: oop
2824     offset   = argument(2);  // type: long
2825     oldval   = NULL;
2826     newval   = argument(4);  // type: oop, int, or long
2827   }
2828 
2829   // Null check receiver.
2830   receiver = null_check(receiver);
2831   if (stopped()) {
2832     return true;
2833   }
2834 
2835   // Build field offset expression.
2836   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2837   // to be plain byte offsets, which are also the same as those accepted
2838   // by oopDesc::field_base.
2839   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2840   // 32-bit machines ignore the high half of long offsets
2841   offset = ConvL2X(offset);
2842   Node* adr = make_unsafe_address(base, offset);
2843   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2844 
2845   // For CAS, unlike inline_unsafe_access, there seems no point in
2846   // trying to refine types. Just use the coarse types here.
2847   const Type *value_type = Type::get_const_basic_type(type);
2848   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2849   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2850 
2851   if (kind == LS_xchg &amp;&amp; type == T_OBJECT) {
2852     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
2853     if (tjp != NULL) {
2854       value_type = tjp;
2855     }
2856   }
2857 
2858   int alias_idx = C-&gt;get_alias_index(adr_type);
2859 
2860   // Memory-model-wise, a LoadStore acts like a little synchronized
2861   // block, so needs barriers on each side.  These don't translate
2862   // into actual barriers on most machines, but we still need rest of
2863   // compiler to respect ordering.
2864 
2865   insert_mem_bar(Op_MemBarRelease);
2866   insert_mem_bar(Op_MemBarCPUOrder);
2867 
2868   // 4984716: MemBars must be inserted before this
2869   //          memory node in order to avoid a false
2870   //          dependency which will confuse the scheduler.
2871   Node *mem = memory(alias_idx);
2872 
2873   // For now, we handle only those cases that actually exist: ints,
2874   // longs, and Object. Adding others should be straightforward.
2875   Node* load_store;
2876   switch(type) {
2877   case T_INT:
2878     if (kind == LS_xadd) {
2879       load_store = _gvn.transform(new (C) GetAndAddINode(control(), mem, adr, newval, adr_type));
2880     } else if (kind == LS_xchg) {
2881       load_store = _gvn.transform(new (C) GetAndSetINode(control(), mem, adr, newval, adr_type));
2882     } else if (kind == LS_cmpxchg) {
2883       load_store = _gvn.transform(new (C) CompareAndSwapINode(control(), mem, adr, newval, oldval));
2884     } else {
2885       ShouldNotReachHere();
2886     }
2887     break;
2888   case T_LONG:
2889     if (kind == LS_xadd) {
2890       load_store = _gvn.transform(new (C) GetAndAddLNode(control(), mem, adr, newval, adr_type));
2891     } else if (kind == LS_xchg) {
2892       load_store = _gvn.transform(new (C) GetAndSetLNode(control(), mem, adr, newval, adr_type));
2893     } else if (kind == LS_cmpxchg) {
2894       load_store = _gvn.transform(new (C) CompareAndSwapLNode(control(), mem, adr, newval, oldval));
2895     } else {
2896       ShouldNotReachHere();
2897     }
2898     break;
2899   case T_OBJECT:
2900     // Transformation of a value which could be NULL pointer (CastPP #NULL)
2901     // could be delayed during Parse (for example, in adjust_map_after_if()).
2902     // Execute transformation here to avoid barrier generation in such case.
2903     if (_gvn.type(newval) == TypePtr::NULL_PTR)
2904       newval = _gvn.makecon(TypePtr::NULL_PTR);
2905 
2906     // Reference stores need a store barrier.
2907     if (kind == LS_xchg) {
2908       // If pre-barrier must execute before the oop store, old value will require do_load here.
2909       if (!can_move_pre_barrier()) {
2910         pre_barrier(true /* do_load*/,
2911                     control(), base, adr, alias_idx, newval, value_type-&gt;make_oopptr(),
2912                     NULL /* pre_val*/,
2913                     T_OBJECT);
2914       } // Else move pre_barrier to use load_store value, see below.
2915     } else if (kind == LS_cmpxchg) {
2916       // Same as for newval above:
2917       if (_gvn.type(oldval) == TypePtr::NULL_PTR) {
2918         oldval = _gvn.makecon(TypePtr::NULL_PTR);
2919       }
2920       // The only known value which might get overwritten is oldval.
2921       pre_barrier(false /* do_load */,
2922                   control(), NULL, NULL, max_juint, NULL, NULL,
2923                   oldval /* pre_val */,
2924                   T_OBJECT);
2925     } else {
2926       ShouldNotReachHere();
2927     }
2928 
2929 #ifdef _LP64
2930     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2931       Node *newval_enc = _gvn.transform(new (C) EncodePNode(newval, newval-&gt;bottom_type()-&gt;make_narrowoop()));
2932       if (kind == LS_xchg) {
2933         load_store = _gvn.transform(new (C) GetAndSetNNode(control(), mem, adr,
2934                                                            newval_enc, adr_type, value_type-&gt;make_narrowoop()));
2935       } else {
2936         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2937         Node *oldval_enc = _gvn.transform(new (C) EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
2938         load_store = _gvn.transform(new (C) CompareAndSwapNNode(control(), mem, adr,
2939                                                                 newval_enc, oldval_enc));
2940       }
2941     } else
2942 #endif
2943     {
2944       if (kind == LS_xchg) {
2945         load_store = _gvn.transform(new (C) GetAndSetPNode(control(), mem, adr, newval, adr_type, value_type-&gt;is_oopptr()));
2946       } else {
2947         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2948         load_store = _gvn.transform(new (C) CompareAndSwapPNode(control(), mem, adr, newval, oldval));
2949       }
2950     }
2951     post_barrier(control(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
2952     break;
2953   default:
2954     fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2955     break;
2956   }
2957 
2958   // SCMemProjNodes represent the memory state of a LoadStore. Their
2959   // main role is to prevent LoadStore nodes from being optimized away
2960   // when their results aren't used.
2961   Node* proj = _gvn.transform(new (C) SCMemProjNode(load_store));
2962   set_memory(proj, alias_idx);
2963 
2964   if (type == T_OBJECT &amp;&amp; kind == LS_xchg) {
2965 #ifdef _LP64
2966     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2967       load_store = _gvn.transform(new (C) DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
2968     }
2969 #endif
2970     if (can_move_pre_barrier()) {
2971       // Don't need to load pre_val. The old value is returned by load_store.
2972       // The pre_barrier can execute after the xchg as long as no safepoint
2973       // gets inserted between them.
2974       pre_barrier(false /* do_load */,
2975                   control(), NULL, NULL, max_juint, NULL, NULL,
2976                   load_store /* pre_val */,
2977                   T_OBJECT);
2978     }
2979   }
2980 
2981   // Add the trailing membar surrounding the access
2982   insert_mem_bar(Op_MemBarCPUOrder);
2983   insert_mem_bar(Op_MemBarAcquire);
2984 
2985   assert(type2size[load_store-&gt;bottom_type()-&gt;basic_type()] == type2size[rtype], "result type should match");
2986   set_result(load_store);
2987   return true;
2988 }
2989 
2990 //----------------------------inline_unsafe_ordered_store----------------------
2991 // public native void sun.misc.Unsafe.putOrderedObject(Object o, long offset, Object x);
2992 // public native void sun.misc.Unsafe.putOrderedInt(Object o, long offset, int x);
2993 // public native void sun.misc.Unsafe.putOrderedLong(Object o, long offset, long x);
2994 bool LibraryCallKit::inline_unsafe_ordered_store(BasicType type) {
2995   // This is another variant of inline_unsafe_access, differing in
2996   // that it always issues store-store ("release") barrier and ensures
2997   // store-atomicity (which only matters for "long").
2998 
2999   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3000 
3001 #ifndef PRODUCT
3002   {
3003     ResourceMark rm;
3004     // Check the signatures.
3005     ciSignature* sig = callee()-&gt;signature();
3006 #ifdef ASSERT
3007     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
3008     assert(rtype == T_VOID, "must return void");
3009     assert(sig-&gt;count() == 3, "has 3 arguments");
3010     assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "base is object");
3011     assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "offset is long");
3012 #endif // ASSERT
3013   }
3014 #endif //PRODUCT
3015 
3016   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
3017 
3018   // Get arguments:
3019   Node* receiver = argument(0);  // type: oop
3020   Node* base     = argument(1);  // type: oop
3021   Node* offset   = argument(2);  // type: long
3022   Node* val      = argument(4);  // type: oop, int, or long
3023 
3024   // Null check receiver.
3025   receiver = null_check(receiver);
3026   if (stopped()) {
3027     return true;
3028   }
3029 
3030   // Build field offset expression.
3031   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
3032   // 32-bit machines ignore the high half of long offsets
3033   offset = ConvL2X(offset);
3034   Node* adr = make_unsafe_address(base, offset);
3035   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
3036   const Type *value_type = Type::get_const_basic_type(type);
3037   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
3038 
3039   insert_mem_bar(Op_MemBarRelease);
3040   insert_mem_bar(Op_MemBarCPUOrder);
3041   // Ensure that the store is atomic for longs:
3042   const bool require_atomic_access = true;
3043   Node* store;
3044   if (type == T_OBJECT) // reference stores need a store barrier.
3045     store = store_oop_to_unknown(control(), base, adr, adr_type, val, type, MemNode::release);
3046   else {
3047     store = store_to_memory(control(), adr, val, type, adr_type, MemNode::release, require_atomic_access);
3048   }
3049   insert_mem_bar(Op_MemBarCPUOrder);
3050   return true;
3051 }
3052 
3053 bool LibraryCallKit::inline_unsafe_fence(vmIntrinsics::ID id) {
3054   // Regardless of form, don't allow previous ld/st to move down,
3055   // then issue acquire, release, or volatile mem_bar.
3056   insert_mem_bar(Op_MemBarCPUOrder);
3057   switch(id) {
3058     case vmIntrinsics::_loadFence:
3059       insert_mem_bar(Op_LoadFence);
3060       return true;
3061     case vmIntrinsics::_storeFence:
3062       insert_mem_bar(Op_StoreFence);
3063       return true;
3064     case vmIntrinsics::_fullFence:
3065       insert_mem_bar(Op_MemBarVolatile);
3066       return true;
3067     default:
3068       fatal_unexpected_iid(id);
3069       return false;
3070   }
3071 }
3072 
3073 bool LibraryCallKit::klass_needs_init_guard(Node* kls) {
3074   if (!kls-&gt;is_Con()) {
3075     return true;
3076   }
3077   const TypeKlassPtr* klsptr = kls-&gt;bottom_type()-&gt;isa_klassptr();
3078   if (klsptr == NULL) {
3079     return true;
3080   }
3081   ciInstanceKlass* ik = klsptr-&gt;klass()-&gt;as_instance_klass();
3082   // don't need a guard for a klass that is already initialized
3083   return !ik-&gt;is_initialized();
3084 }
3085 
3086 //----------------------------inline_unsafe_allocate---------------------------
3087 // public native Object sun.misc.Unsafe.allocateInstance(Class&lt;?&gt; cls);
3088 bool LibraryCallKit::inline_unsafe_allocate() {
3089   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3090 
3091   null_check_receiver();  // null-check, then ignore
3092   Node* cls = null_check(argument(1));
3093   if (stopped())  return true;
3094 
3095   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3096   kls = null_check(kls);
3097   if (stopped())  return true;  // argument was like int.class
3098 
3099   Node* test = NULL;
3100   if (LibraryCallKit::klass_needs_init_guard(kls)) {
3101     // Note:  The argument might still be an illegal value like
3102     // Serializable.class or Object[].class.   The runtime will handle it.
3103     // But we must make an explicit check for initialization.
3104     Node* insp = basic_plus_adr(kls, in_bytes(InstanceKlass::init_state_offset()));
3105     // Use T_BOOLEAN for InstanceKlass::_init_state so the compiler
3106     // can generate code to load it as unsigned byte.
3107     Node* inst = make_load(NULL, insp, TypeInt::UBYTE, T_BOOLEAN, MemNode::unordered);
3108     Node* bits = intcon(InstanceKlass::fully_initialized);
3109     test = _gvn.transform(new (C) SubINode(inst, bits));
3110     // The 'test' is non-zero if we need to take a slow path.
3111   }
3112 
3113   Node* obj = new_instance(kls, test);
3114   set_result(obj);
3115   return true;
3116 }
3117 
3118 #ifdef TRACE_HAVE_INTRINSICS
3119 /*
3120  * oop -&gt; myklass
3121  * myklass-&gt;trace_id |= USED
3122  * return myklass-&gt;trace_id &amp; ~0x3
3123  */
3124 bool LibraryCallKit::inline_native_classID() {
3125   null_check_receiver();  // null-check, then ignore
3126   Node* cls = null_check(argument(1), T_OBJECT);
3127   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3128   kls = null_check(kls, T_OBJECT);
3129   ByteSize offset = TRACE_ID_OFFSET;
3130   Node* insp = basic_plus_adr(kls, in_bytes(offset));
3131   Node* tvalue = make_load(NULL, insp, TypeLong::LONG, T_LONG, MemNode::unordered);
3132   Node* bits = longcon(~0x03l); // ignore bit 0 &amp; 1
3133   Node* andl = _gvn.transform(new (C) AndLNode(tvalue, bits));
3134   Node* clsused = longcon(0x01l); // set the class bit
3135   Node* orl = _gvn.transform(new (C) OrLNode(tvalue, clsused));
3136 
3137   const TypePtr *adr_type = _gvn.type(insp)-&gt;isa_ptr();
3138   store_to_memory(control(), insp, orl, T_LONG, adr_type, MemNode::unordered);
3139   set_result(andl);
3140   return true;
3141 }
3142 
3143 bool LibraryCallKit::inline_native_threadID() {
3144   Node* tls_ptr = NULL;
3145   Node* cur_thr = generate_current_thread(tls_ptr);
3146   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3147   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3148   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::thread_id_offset()));
3149 
3150   Node* threadid = NULL;
3151   size_t thread_id_size = OSThread::thread_id_size();
3152   if (thread_id_size == (size_t) BytesPerLong) {
3153     threadid = ConvL2I(make_load(control(), p, TypeLong::LONG, T_LONG, MemNode::unordered));
3154   } else if (thread_id_size == (size_t) BytesPerInt) {
3155     threadid = make_load(control(), p, TypeInt::INT, T_INT, MemNode::unordered);
3156   } else {
3157     ShouldNotReachHere();
3158   }
3159   set_result(threadid);
3160   return true;
3161 }
3162 #endif
3163 
3164 //------------------------inline_native_time_funcs--------------
3165 // inline code for System.currentTimeMillis() and System.nanoTime()
3166 // these have the same type and signature
3167 bool LibraryCallKit::inline_native_time_funcs(address funcAddr, const char* funcName) {
3168   const TypeFunc* tf = OptoRuntime::void_long_Type();
3169   const TypePtr* no_memory_effects = NULL;
3170   Node* time = make_runtime_call(RC_LEAF, tf, funcAddr, funcName, no_memory_effects);
3171   Node* value = _gvn.transform(new (C) ProjNode(time, TypeFunc::Parms+0));
3172 #ifdef ASSERT
3173   Node* value_top = _gvn.transform(new (C) ProjNode(time, TypeFunc::Parms+1));
3174   assert(value_top == top(), "second value must be top");
3175 #endif
3176   set_result(value);
3177   return true;
3178 }
3179 
3180 //------------------------inline_native_currentThread------------------
3181 bool LibraryCallKit::inline_native_currentThread() {
3182   Node* junk = NULL;
3183   set_result(generate_current_thread(junk));
3184   return true;
3185 }
3186 
3187 //------------------------inline_native_isInterrupted------------------
3188 // private native boolean java.lang.Thread.isInterrupted(boolean ClearInterrupted);
3189 bool LibraryCallKit::inline_native_isInterrupted() {
3190   // Add a fast path to t.isInterrupted(clear_int):
3191   //   (t == Thread.current() &amp;&amp;
3192   //    (!TLS._osthread._interrupted || WINDOWS_ONLY(false) NOT_WINDOWS(!clear_int)))
3193   //   ? TLS._osthread._interrupted : /*slow path:*/ t.isInterrupted(clear_int)
3194   // So, in the common case that the interrupt bit is false,
3195   // we avoid making a call into the VM.  Even if the interrupt bit
3196   // is true, if the clear_int argument is false, we avoid the VM call.
3197   // However, if the receiver is not currentThread, we must call the VM,
3198   // because there must be some locking done around the operation.
3199 
3200   // We only go to the fast case code if we pass two guards.
3201   // Paths which do not pass are accumulated in the slow_region.
3202 
3203   enum {
3204     no_int_result_path   = 1, // t == Thread.current() &amp;&amp; !TLS._osthread._interrupted
3205     no_clear_result_path = 2, // t == Thread.current() &amp;&amp;  TLS._osthread._interrupted &amp;&amp; !clear_int
3206     slow_result_path     = 3, // slow path: t.isInterrupted(clear_int)
3207     PATH_LIMIT
3208   };
3209 
3210   // Ensure that it's not possible to move the load of TLS._osthread._interrupted flag
3211   // out of the function.
3212   insert_mem_bar(Op_MemBarCPUOrder);
3213 
3214   RegionNode* result_rgn = new (C) RegionNode(PATH_LIMIT);
3215   PhiNode*    result_val = new (C) PhiNode(result_rgn, TypeInt::BOOL);
3216 
3217   RegionNode* slow_region = new (C) RegionNode(1);
3218   record_for_igvn(slow_region);
3219 
3220   // (a) Receiving thread must be the current thread.
3221   Node* rec_thr = argument(0);
3222   Node* tls_ptr = NULL;
3223   Node* cur_thr = generate_current_thread(tls_ptr);
3224   Node* cmp_thr = _gvn.transform(new (C) CmpPNode(cur_thr, rec_thr));
3225   Node* bol_thr = _gvn.transform(new (C) BoolNode(cmp_thr, BoolTest::ne));
3226 
3227   generate_slow_guard(bol_thr, slow_region);
3228 
3229   // (b) Interrupt bit on TLS must be false.
3230   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3231   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3232   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::interrupted_offset()));
3233 
3234   // Set the control input on the field _interrupted read to prevent it floating up.
3235   Node* int_bit = make_load(control(), p, TypeInt::BOOL, T_INT, MemNode::unordered);
3236   Node* cmp_bit = _gvn.transform(new (C) CmpINode(int_bit, intcon(0)));
3237   Node* bol_bit = _gvn.transform(new (C) BoolNode(cmp_bit, BoolTest::ne));
3238 
3239   IfNode* iff_bit = create_and_map_if(control(), bol_bit, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
3240 
3241   // First fast path:  if (!TLS._interrupted) return false;
3242   Node* false_bit = _gvn.transform(new (C) IfFalseNode(iff_bit));
3243   result_rgn-&gt;init_req(no_int_result_path, false_bit);
3244   result_val-&gt;init_req(no_int_result_path, intcon(0));
3245 
3246   // drop through to next case
3247   set_control( _gvn.transform(new (C) IfTrueNode(iff_bit)));
3248 
3249 #ifndef TARGET_OS_FAMILY_windows
3250   // (c) Or, if interrupt bit is set and clear_int is false, use 2nd fast path.
3251   Node* clr_arg = argument(1);
3252   Node* cmp_arg = _gvn.transform(new (C) CmpINode(clr_arg, intcon(0)));
3253   Node* bol_arg = _gvn.transform(new (C) BoolNode(cmp_arg, BoolTest::ne));
3254   IfNode* iff_arg = create_and_map_if(control(), bol_arg, PROB_FAIR, COUNT_UNKNOWN);
3255 
3256   // Second fast path:  ... else if (!clear_int) return true;
3257   Node* false_arg = _gvn.transform(new (C) IfFalseNode(iff_arg));
3258   result_rgn-&gt;init_req(no_clear_result_path, false_arg);
3259   result_val-&gt;init_req(no_clear_result_path, intcon(1));
3260 
3261   // drop through to next case
3262   set_control( _gvn.transform(new (C) IfTrueNode(iff_arg)));
3263 #else
3264   // To return true on Windows you must read the _interrupted field
3265   // and check the the event state i.e. take the slow path.
3266 #endif // TARGET_OS_FAMILY_windows
3267 
3268   // (d) Otherwise, go to the slow path.
3269   slow_region-&gt;add_req(control());
3270   set_control( _gvn.transform(slow_region));
3271 
3272   if (stopped()) {
3273     // There is no slow path.
3274     result_rgn-&gt;init_req(slow_result_path, top());
3275     result_val-&gt;init_req(slow_result_path, top());
3276   } else {
3277     // non-virtual because it is a private non-static
3278     CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_isInterrupted);
3279 
3280     Node* slow_val = set_results_for_java_call(slow_call);
3281     // this-&gt;control() comes from set_results_for_java_call
3282 
3283     Node* fast_io  = slow_call-&gt;in(TypeFunc::I_O);
3284     Node* fast_mem = slow_call-&gt;in(TypeFunc::Memory);
3285 
3286     // These two phis are pre-filled with copies of of the fast IO and Memory
3287     PhiNode* result_mem  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
3288     PhiNode* result_io   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
3289 
3290     result_rgn-&gt;init_req(slow_result_path, control());
3291     result_io -&gt;init_req(slow_result_path, i_o());
3292     result_mem-&gt;init_req(slow_result_path, reset_memory());
3293     result_val-&gt;init_req(slow_result_path, slow_val);
3294 
3295     set_all_memory(_gvn.transform(result_mem));
3296     set_i_o(       _gvn.transform(result_io));
3297   }
3298 
3299   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3300   set_result(result_rgn, result_val);
3301   return true;
3302 }
3303 
3304 //---------------------------load_mirror_from_klass----------------------------
3305 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3306 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3307   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3308   return make_load(NULL, p, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3309 }
3310 
3311 //-----------------------load_klass_from_mirror_common-------------------------
3312 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3313 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3314 // and branch to the given path on the region.
3315 // If never_see_null, take an uncommon trap on null, so we can optimistically
3316 // compile for the non-null case.
3317 // If the region is NULL, force never_see_null = true.
3318 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3319                                                     bool never_see_null,
3320                                                     RegionNode* region,
3321                                                     int null_path,
3322                                                     int offset) {
3323   if (region == NULL)  never_see_null = true;
3324   Node* p = basic_plus_adr(mirror, offset);
3325   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3326   Node* kls = _gvn.transform( LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3327   Node* null_ctl = top();
3328   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3329   if (region != NULL) {
3330     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
3331     region-&gt;init_req(null_path, null_ctl);
3332   } else {
3333     assert(null_ctl == top(), "no loose ends");
3334   }
3335   return kls;
3336 }
3337 
3338 //--------------------(inline_native_Class_query helpers)---------------------
3339 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE, JVM_ACC_HAS_FINALIZER.
3340 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3341 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3342   // Branch around if the given klass has the given modifier bit set.
3343   // Like generate_guard, adds a new path onto the region.
3344   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3345   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3346   Node* mask = intcon(modifier_mask);
3347   Node* bits = intcon(modifier_bits);
3348   Node* mbit = _gvn.transform(new (C) AndINode(mods, mask));
3349   Node* cmp  = _gvn.transform(new (C) CmpINode(mbit, bits));
3350   Node* bol  = _gvn.transform(new (C) BoolNode(cmp, BoolTest::ne));
3351   return generate_fair_guard(bol, region);
3352 }
3353 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3354   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3355 }
3356 
3357 //-------------------------inline_native_Class_query-------------------
3358 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3359   const Type* return_type = TypeInt::BOOL;
3360   Node* prim_return_value = top();  // what happens if it's a primitive class?
3361   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3362   bool expect_prim = false;     // most of these guys expect to work on refs
3363 
3364   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3365 
3366   Node* mirror = argument(0);
3367   Node* obj    = top();
3368 
3369   switch (id) {
3370   case vmIntrinsics::_isInstance:
3371     // nothing is an instance of a primitive type
3372     prim_return_value = intcon(0);
3373     obj = argument(1);
3374     break;
3375   case vmIntrinsics::_getModifiers:
3376     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3377     assert(is_power_of_2((int)JVM_ACC_WRITTEN_FLAGS+1), "change next line");
3378     return_type = TypeInt::make(0, JVM_ACC_WRITTEN_FLAGS, Type::WidenMin);
3379     break;
3380   case vmIntrinsics::_isInterface:
3381     prim_return_value = intcon(0);
3382     break;
3383   case vmIntrinsics::_isArray:
3384     prim_return_value = intcon(0);
3385     expect_prim = true;  // cf. ObjectStreamClass.getClassSignature
3386     break;
3387   case vmIntrinsics::_isPrimitive:
3388     prim_return_value = intcon(1);
3389     expect_prim = true;  // obviously
3390     break;
3391   case vmIntrinsics::_getSuperclass:
3392     prim_return_value = null();
3393     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3394     break;
3395   case vmIntrinsics::_getComponentType:
3396     prim_return_value = null();
3397     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3398     break;
3399   case vmIntrinsics::_getClassAccessFlags:
3400     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3401     return_type = TypeInt::INT;  // not bool!  6297094
3402     break;
3403   default:
3404     fatal_unexpected_iid(id);
3405     break;
3406   }
3407 
3408   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3409   if (mirror_con == NULL)  return false;  // cannot happen?
3410 
3411 #ifndef PRODUCT
3412   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
3413     ciType* k = mirror_con-&gt;java_mirror_type();
3414     if (k) {
3415       tty-&gt;print("Inlining %s on constant Class ", vmIntrinsics::name_at(intrinsic_id()));
3416       k-&gt;print_name();
3417       tty-&gt;cr();
3418     }
3419   }
3420 #endif
3421 
3422   // Null-check the mirror, and the mirror's klass ptr (in case it is a primitive).
3423   RegionNode* region = new (C) RegionNode(PATH_LIMIT);
3424   record_for_igvn(region);
3425   PhiNode* phi = new (C) PhiNode(region, return_type);
3426 
3427   // The mirror will never be null of Reflection.getClassAccessFlags, however
3428   // it may be null for Class.isInstance or Class.getModifiers. Throw a NPE
3429   // if it is. See bug 4774291.
3430 
3431   // For Reflection.getClassAccessFlags(), the null check occurs in
3432   // the wrong place; see inline_unsafe_access(), above, for a similar
3433   // situation.
3434   mirror = null_check(mirror);
3435   // If mirror or obj is dead, only null-path is taken.
3436   if (stopped())  return true;
3437 
3438   if (expect_prim)  never_see_null = false;  // expect nulls (meaning prims)
3439 
3440   // Now load the mirror's klass metaobject, and null-check it.
3441   // Side-effects region with the control path if the klass is null.
3442   Node* kls = load_klass_from_mirror(mirror, never_see_null, region, _prim_path);
3443   // If kls is null, we have a primitive mirror.
3444   phi-&gt;init_req(_prim_path, prim_return_value);
3445   if (stopped()) { set_result(region, phi); return true; }
3446   bool safe_for_replace = (region-&gt;in(_prim_path) == top());
3447 
3448   Node* p;  // handy temp
3449   Node* null_ctl;
3450 
3451   // Now that we have the non-null klass, we can perform the real query.
3452   // For constant classes, the query will constant-fold in LoadNode::Value.
3453   Node* query_value = top();
3454   switch (id) {
3455   case vmIntrinsics::_isInstance:
3456     // nothing is an instance of a primitive type
3457     query_value = gen_instanceof(obj, kls, safe_for_replace);
3458     break;
3459 
3460   case vmIntrinsics::_getModifiers:
3461     p = basic_plus_adr(kls, in_bytes(Klass::modifier_flags_offset()));
3462     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3463     break;
3464 
3465   case vmIntrinsics::_isInterface:
3466     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3467     if (generate_interface_guard(kls, region) != NULL)
3468       // A guard was added.  If the guard is taken, it was an interface.
3469       phi-&gt;add_req(intcon(1));
3470     // If we fall through, it's a plain class.
3471     query_value = intcon(0);
3472     break;
3473 
3474   case vmIntrinsics::_isArray:
3475     // (To verify this code sequence, check the asserts in JVM_IsArrayClass.)
3476     if (generate_array_guard(kls, region) != NULL)
3477       // A guard was added.  If the guard is taken, it was an array.
3478       phi-&gt;add_req(intcon(1));
3479     // If we fall through, it's a plain class.
3480     query_value = intcon(0);
3481     break;
3482 
3483   case vmIntrinsics::_isPrimitive:
3484     query_value = intcon(0); // "normal" path produces false
3485     break;
3486 
3487   case vmIntrinsics::_getSuperclass:
3488     // The rules here are somewhat unfortunate, but we can still do better
3489     // with random logic than with a JNI call.
3490     // Interfaces store null or Object as _super, but must report null.
3491     // Arrays store an intermediate super as _super, but must report Object.
3492     // Other types can report the actual _super.
3493     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3494     if (generate_interface_guard(kls, region) != NULL)
3495       // A guard was added.  If the guard is taken, it was an interface.
3496       phi-&gt;add_req(null());
3497     if (generate_array_guard(kls, region) != NULL)
3498       // A guard was added.  If the guard is taken, it was an array.
3499       phi-&gt;add_req(makecon(TypeInstPtr::make(env()-&gt;Object_klass()-&gt;java_mirror())));
3500     // If we fall through, it's a plain class.  Get its _super.
3501     p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));
3502     kls = _gvn.transform( LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeKlassPtr::OBJECT_OR_NULL));
3503     null_ctl = top();
3504     kls = null_check_oop(kls, &amp;null_ctl);
3505     if (null_ctl != top()) {
3506       // If the guard is taken, Object.superClass is null (both klass and mirror).
3507       region-&gt;add_req(null_ctl);
3508       phi   -&gt;add_req(null());
3509     }
3510     if (!stopped()) {
3511       query_value = load_mirror_from_klass(kls);
3512     }
3513     break;
3514 
3515   case vmIntrinsics::_getComponentType:
3516     if (generate_array_guard(kls, region) != NULL) {
3517       // Be sure to pin the oop load to the guard edge just created:
3518       Node* is_array_ctrl = region-&gt;in(region-&gt;req()-1);
3519       Node* cma = basic_plus_adr(kls, in_bytes(ArrayKlass::component_mirror_offset()));
3520       Node* cmo = make_load(is_array_ctrl, cma, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3521       phi-&gt;add_req(cmo);
3522     }
3523     query_value = null();  // non-array case is null
3524     break;
3525 
3526   case vmIntrinsics::_getClassAccessFlags:
3527     p = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3528     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3529     break;
3530 
3531   default:
3532     fatal_unexpected_iid(id);
3533     break;
3534   }
3535 
3536   // Fall-through is the normal case of a query to a real class.
3537   phi-&gt;init_req(1, query_value);
3538   region-&gt;init_req(1, control());
3539 
3540   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3541   set_result(region, phi);
3542   return true;
3543 }
3544 
3545 //--------------------------inline_native_subtype_check------------------------
3546 // This intrinsic takes the JNI calls out of the heart of
3547 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3548 bool LibraryCallKit::inline_native_subtype_check() {
3549   // Pull both arguments off the stack.
3550   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3551   args[0] = argument(0);
3552   args[1] = argument(1);
3553   Node* klasses[2];             // corresponding Klasses: superk, subk
3554   klasses[0] = klasses[1] = top();
3555 
3556   enum {
3557     // A full decision tree on {superc is prim, subc is prim}:
3558     _prim_0_path = 1,           // {P,N} =&gt; false
3559                                 // {P,P} &amp; superc!=subc =&gt; false
3560     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3561     _prim_1_path,               // {N,P} =&gt; false
3562     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3563     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3564     PATH_LIMIT
3565   };
3566 
3567   RegionNode* region = new (C) RegionNode(PATH_LIMIT);
3568   Node*       phi    = new (C) PhiNode(region, TypeInt::BOOL);
3569   record_for_igvn(region);
3570 
3571   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3572   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3573   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3574 
3575   // First null-check both mirrors and load each mirror's klass metaobject.
3576   int which_arg;
3577   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3578     Node* arg = args[which_arg];
3579     arg = null_check(arg);
3580     if (stopped())  break;
3581     args[which_arg] = arg;
3582 
3583     Node* p = basic_plus_adr(arg, class_klass_offset);
3584     Node* kls = LoadKlassNode::make(_gvn, immutable_memory(), p, adr_type, kls_type);
3585     klasses[which_arg] = _gvn.transform(kls);
3586   }
3587 
3588   // Having loaded both klasses, test each for null.
3589   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3590   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3591     Node* kls = klasses[which_arg];
3592     Node* null_ctl = top();
3593     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3594     int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);
3595     region-&gt;init_req(prim_path, null_ctl);
3596     if (stopped())  break;
3597     klasses[which_arg] = kls;
3598   }
3599 
3600   if (!stopped()) {
3601     // now we have two reference types, in klasses[0..1]
3602     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3603     Node* superk = klasses[0];  // the receiver
3604     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3605     // now we have a successful reference subtype check
3606     region-&gt;set_req(_ref_subtype_path, control());
3607   }
3608 
3609   // If both operands are primitive (both klasses null), then
3610   // we must return true when they are identical primitives.
3611   // It is convenient to test this after the first null klass check.
3612   set_control(region-&gt;in(_prim_0_path)); // go back to first null check
3613   if (!stopped()) {
3614     // Since superc is primitive, make a guard for the superc==subc case.
3615     Node* cmp_eq = _gvn.transform(new (C) CmpPNode(args[0], args[1]));
3616     Node* bol_eq = _gvn.transform(new (C) BoolNode(cmp_eq, BoolTest::eq));
3617     generate_guard(bol_eq, region, PROB_FAIR);
3618     if (region-&gt;req() == PATH_LIMIT+1) {
3619       // A guard was added.  If the added guard is taken, superc==subc.
3620       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3621       region-&gt;del_req(PATH_LIMIT);
3622     }
3623     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3624   }
3625 
3626   // these are the only paths that produce 'true':
3627   phi-&gt;set_req(_prim_same_path,   intcon(1));
3628   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3629 
3630   // pull together the cases:
3631   assert(region-&gt;req() == PATH_LIMIT, "sane region");
3632   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3633     Node* ctl = region-&gt;in(i);
3634     if (ctl == NULL || ctl == top()) {
3635       region-&gt;set_req(i, top());
3636       phi   -&gt;set_req(i, top());
3637     } else if (phi-&gt;in(i) == NULL) {
3638       phi-&gt;set_req(i, intcon(0)); // all other paths produce 'false'
3639     }
3640   }
3641 
3642   set_control(_gvn.transform(region));
3643   set_result(_gvn.transform(phi));
3644   return true;
3645 }
3646 
3647 //---------------------generate_array_guard_common------------------------
3648 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,
3649                                                   bool obj_array, bool not_array) {
3650   // If obj_array/non_array==false/false:
3651   // Branch around if the given klass is in fact an array (either obj or prim).
3652   // If obj_array/non_array==false/true:
3653   // Branch around if the given klass is not an array klass of any kind.
3654   // If obj_array/non_array==true/true:
3655   // Branch around if the kls is not an oop array (kls is int[], String, etc.)
3656   // If obj_array/non_array==true/false:
3657   // Branch around if the kls is an oop array (Object[] or subtype)
3658   //
3659   // Like generate_guard, adds a new path onto the region.
3660   jint  layout_con = 0;
3661   Node* layout_val = get_layout_helper(kls, layout_con);
3662   if (layout_val == NULL) {
3663     bool query = (obj_array
3664                   ? Klass::layout_helper_is_objArray(layout_con)
3665                   : Klass::layout_helper_is_array(layout_con));
3666     if (query == not_array) {
3667       return NULL;                       // never a branch
3668     } else {                             // always a branch
3669       Node* always_branch = control();
3670       if (region != NULL)
3671         region-&gt;add_req(always_branch);
3672       set_control(top());
3673       return always_branch;
3674     }
3675   }
3676   // Now test the correct condition.
3677   jint  nval = (obj_array
3678                 ? ((jint)Klass::_lh_array_tag_type_value
3679                    &lt;&lt;    Klass::_lh_array_tag_shift)
3680                 : Klass::_lh_neutral_value);
3681   Node* cmp = _gvn.transform(new(C) CmpINode(layout_val, intcon(nval)));
3682   BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array
3683   // invert the test if we are looking for a non-array
3684   if (not_array)  btest = BoolTest(btest).negate();
3685   Node* bol = _gvn.transform(new(C) BoolNode(cmp, btest));
3686   return generate_fair_guard(bol, region);
3687 }
3688 
3689 
3690 //-----------------------inline_native_newArray--------------------------
3691 // private static native Object java.lang.reflect.newArray(Class&lt;?&gt; componentType, int length);
3692 bool LibraryCallKit::inline_native_newArray() {
3693   Node* mirror    = argument(0);
3694   Node* count_val = argument(1);
3695 
3696   mirror = null_check(mirror);
3697   // If mirror or obj is dead, only null-path is taken.
3698   if (stopped())  return true;
3699 
3700   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3701   RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
3702   PhiNode*    result_val = new(C) PhiNode(result_reg,
3703                                           TypeInstPtr::NOTNULL);
3704   PhiNode*    result_io  = new(C) PhiNode(result_reg, Type::ABIO);
3705   PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY,
3706                                           TypePtr::BOTTOM);
3707 
3708   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3709   Node* klass_node = load_array_klass_from_mirror(mirror, never_see_null,
3710                                                   result_reg, _slow_path);
3711   Node* normal_ctl   = control();
3712   Node* no_array_ctl = result_reg-&gt;in(_slow_path);
3713 
3714   // Generate code for the slow case.  We make a call to newArray().
3715   set_control(no_array_ctl);
3716   if (!stopped()) {
3717     // Either the input type is void.class, or else the
3718     // array klass has not yet been cached.  Either the
3719     // ensuing call will throw an exception, or else it
3720     // will cache the array klass for next time.
3721     PreserveJVMState pjvms(this);
3722     CallJavaNode* slow_call = generate_method_call_static(vmIntrinsics::_newArray);
3723     Node* slow_result = set_results_for_java_call(slow_call);
3724     // this-&gt;control() comes from set_results_for_java_call
3725     result_reg-&gt;set_req(_slow_path, control());
3726     result_val-&gt;set_req(_slow_path, slow_result);
3727     result_io -&gt;set_req(_slow_path, i_o());
3728     result_mem-&gt;set_req(_slow_path, reset_memory());
3729   }
3730 
3731   set_control(normal_ctl);
3732   if (!stopped()) {
3733     // Normal case:  The array type has been cached in the java.lang.Class.
3734     // The following call works fine even if the array type is polymorphic.
3735     // It could be a dynamic mix of int[], boolean[], Object[], etc.
3736     Node* obj = new_array(klass_node, count_val, 0);  // no arguments to push
3737     result_reg-&gt;init_req(_normal_path, control());
3738     result_val-&gt;init_req(_normal_path, obj);
3739     result_io -&gt;init_req(_normal_path, i_o());
3740     result_mem-&gt;init_req(_normal_path, reset_memory());
3741   }
3742 
3743   // Return the combined state.
3744   set_i_o(        _gvn.transform(result_io)  );
3745   set_all_memory( _gvn.transform(result_mem));
3746 
3747   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3748   set_result(result_reg, result_val);
3749   return true;
3750 }
3751 
3752 //----------------------inline_native_getLength--------------------------
3753 // public static native int java.lang.reflect.Array.getLength(Object array);
3754 bool LibraryCallKit::inline_native_getLength() {
3755   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3756 
3757   Node* array = null_check(argument(0));
3758   // If array is dead, only null-path is taken.
3759   if (stopped())  return true;
3760 
3761   // Deoptimize if it is a non-array.
3762   Node* non_array = generate_non_array_guard(load_object_klass(array), NULL);
3763 
3764   if (non_array != NULL) {
3765     PreserveJVMState pjvms(this);
3766     set_control(non_array);
3767     uncommon_trap(Deoptimization::Reason_intrinsic,
3768                   Deoptimization::Action_maybe_recompile);
3769   }
3770 
3771   // If control is dead, only non-array-path is taken.
3772   if (stopped())  return true;
3773 
3774   // The works fine even if the array type is polymorphic.
3775   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3776   Node* result = load_array_length(array);
3777 
3778   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3779   set_result(result);
3780   return true;
3781 }
3782 
3783 //------------------------inline_array_copyOf----------------------------
3784 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3785 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3786 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3787   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3788 
3789   // Get the arguments.
3790   Node* original          = argument(0);
3791   Node* start             = is_copyOfRange? argument(1): intcon(0);
3792   Node* end               = is_copyOfRange? argument(2): argument(1);
3793   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3794 
3795   Node* newcopy;
3796 
3797   // Set the original stack and the reexecute bit for the interpreter to reexecute
3798   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3799   { PreserveReexecuteState preexecs(this);
3800     jvms()-&gt;set_should_reexecute(true);
3801 
3802     array_type_mirror = null_check(array_type_mirror);
3803     original          = null_check(original);
3804 
3805     // Check if a null path was taken unconditionally.
3806     if (stopped())  return true;
3807 
3808     Node* orig_length = load_array_length(original);
3809 
3810     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3811     klass_node = null_check(klass_node);
3812 
3813     RegionNode* bailout = new (C) RegionNode(1);
3814     record_for_igvn(bailout);
3815 
3816     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3817     // Bail out if that is so.
3818     Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);
3819     if (not_objArray != NULL) {
3820       // Improve the klass node's type from the new optimistic assumption:
3821       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3822       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
3823       Node* cast = new (C) CastPPNode(klass_node, akls);
3824       cast-&gt;init_req(0, control());
3825       klass_node = _gvn.transform(cast);
3826     }
3827 
3828     // Bail out if either start or end is negative.
3829     generate_negative_guard(start, bailout, &amp;start);
3830     generate_negative_guard(end,   bailout, &amp;end);
3831 
3832     Node* length = end;
3833     if (_gvn.type(start) != TypeInt::ZERO) {
3834       length = _gvn.transform(new (C) SubINode(end, start));
3835     }
3836 
3837     // Bail out if length is negative.
3838     // Without this the new_array would throw
3839     // NegativeArraySizeException but IllegalArgumentException is what
3840     // should be thrown
3841     generate_negative_guard(length, bailout, &amp;length);
3842 
3843     if (bailout-&gt;req() &gt; 1) {
3844       PreserveJVMState pjvms(this);
3845       set_control(_gvn.transform(bailout));
3846       uncommon_trap(Deoptimization::Reason_intrinsic,
3847                     Deoptimization::Action_maybe_recompile);
3848     }
3849 
3850     if (!stopped()) {
3851       // How many elements will we copy from the original?
3852       // The answer is MinI(orig_length - start, length).
3853       Node* orig_tail = _gvn.transform(new (C) SubINode(orig_length, start));
3854       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
3855 
3856       newcopy = new_array(klass_node, length, 0);  // no argments to push
3857 
3858       // Generate a direct call to the right arraycopy function(s).
3859       // We know the copy is disjoint but we might not know if the
3860       // oop stores need checking.
3861       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
3862       // This will fail a store-check if x contains any non-nulls.
3863       bool disjoint_bases = true;
3864       // if start &gt; orig_length then the length of the copy may be
3865       // negative.
3866       bool length_never_negative = !is_copyOfRange;
3867       generate_arraycopy(TypeAryPtr::OOPS, T_OBJECT,
3868                          original, start, newcopy, intcon(0), moved,
3869                          disjoint_bases, length_never_negative);
3870     }
3871   } // original reexecute is set back here
3872 
3873   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3874   if (!stopped()) {
3875     set_result(newcopy);
3876   }
3877   return true;
3878 }
3879 
3880 
3881 //----------------------generate_virtual_guard---------------------------
3882 // Helper for hashCode and clone.  Peeks inside the vtable to avoid a call.
3883 Node* LibraryCallKit::generate_virtual_guard(Node* obj_klass,
3884                                              RegionNode* slow_region) {
3885   ciMethod* method = callee();
3886   int vtable_index = method-&gt;vtable_index();
3887   assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
3888          err_msg_res("bad index %d", vtable_index));
3889   // Get the Method* out of the appropriate vtable entry.
3890   int entry_offset  = (InstanceKlass::vtable_start_offset() +
3891                      vtable_index*vtableEntry::size()) * wordSize +
3892                      vtableEntry::method_offset_in_bytes();
3893   Node* entry_addr  = basic_plus_adr(obj_klass, entry_offset);
3894   Node* target_call = make_load(NULL, entry_addr, TypePtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3895 
3896   // Compare the target method with the expected method (e.g., Object.hashCode).
3897   const TypePtr* native_call_addr = TypeMetadataPtr::make(method);
3898 
3899   Node* native_call = makecon(native_call_addr);
3900   Node* chk_native  = _gvn.transform(new(C) CmpPNode(target_call, native_call));
3901   Node* test_native = _gvn.transform(new(C) BoolNode(chk_native, BoolTest::ne));
3902 
3903   return generate_slow_guard(test_native, slow_region);
3904 }
3905 
3906 //-----------------------generate_method_call----------------------------
3907 // Use generate_method_call to make a slow-call to the real
3908 // method if the fast path fails.  An alternative would be to
3909 // use a stub like OptoRuntime::slow_arraycopy_Java.
3910 // This only works for expanding the current library call,
3911 // not another intrinsic.  (E.g., don't use this for making an
3912 // arraycopy call inside of the copyOf intrinsic.)
3913 CallJavaNode*
3914 LibraryCallKit::generate_method_call(vmIntrinsics::ID method_id, bool is_virtual, bool is_static) {
3915   // When compiling the intrinsic method itself, do not use this technique.
3916   guarantee(callee() != C-&gt;method(), "cannot make slow-call to self");
3917 
3918   ciMethod* method = callee();
3919   // ensure the JVMS we have will be correct for this call
3920   guarantee(method_id == method-&gt;intrinsic_id(), "must match");
3921 
3922   const TypeFunc* tf = TypeFunc::make(method);
3923   CallJavaNode* slow_call;
3924   if (is_static) {
3925     assert(!is_virtual, "");
3926     slow_call = new(C) CallStaticJavaNode(C, tf,
3927                            SharedRuntime::get_resolve_static_call_stub(),
3928                            method, bci());
3929   } else if (is_virtual) {
3930     null_check_receiver();
3931     int vtable_index = Method::invalid_vtable_index;
3932     if (UseInlineCaches) {
3933       // Suppress the vtable call
3934     } else {
3935       // hashCode and clone are not a miranda methods,
3936       // so the vtable index is fixed.
3937       // No need to use the linkResolver to get it.
3938        vtable_index = method-&gt;vtable_index();
3939        assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
3940               err_msg_res("bad index %d", vtable_index));
3941     }
3942     slow_call = new(C) CallDynamicJavaNode(tf,
3943                           SharedRuntime::get_resolve_virtual_call_stub(),
3944                           method, vtable_index, bci());
3945   } else {  // neither virtual nor static:  opt_virtual
3946     null_check_receiver();
3947     slow_call = new(C) CallStaticJavaNode(C, tf,
3948                                 SharedRuntime::get_resolve_opt_virtual_call_stub(),
3949                                 method, bci());
3950     slow_call-&gt;set_optimized_virtual(true);
3951   }
3952   set_arguments_for_java_call(slow_call);
3953   set_edges_for_java_call(slow_call);
3954   return slow_call;
3955 }
3956 
3957 
3958 //------------------------------inline_native_hashcode--------------------
3959 // Build special case code for calls to hashCode on an object.
3960 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
3961   assert(is_static == callee()-&gt;is_static(), "correct intrinsic selection");
3962   assert(!(is_virtual &amp;&amp; is_static), "either virtual, special, or static");
3963 
3964   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
3965 
3966   RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
3967   PhiNode*    result_val = new(C) PhiNode(result_reg,
3968                                           TypeInt::INT);
3969   PhiNode*    result_io  = new(C) PhiNode(result_reg, Type::ABIO);
3970   PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY,
3971                                           TypePtr::BOTTOM);
3972   Node* obj = NULL;
3973   if (!is_static) {
3974     // Check for hashing null object
3975     obj = null_check_receiver();
3976     if (stopped())  return true;        // unconditionally null
3977     result_reg-&gt;init_req(_null_path, top());
3978     result_val-&gt;init_req(_null_path, top());
3979   } else {
3980     // Do a null check, and return zero if null.
3981     // System.identityHashCode(null) == 0
3982     obj = argument(0);
3983     Node* null_ctl = top();
3984     obj = null_check_oop(obj, &amp;null_ctl);
3985     result_reg-&gt;init_req(_null_path, null_ctl);
3986     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
3987   }
3988 
3989   // Unconditionally null?  Then return right away.
3990   if (stopped()) {
3991     set_control( result_reg-&gt;in(_null_path));
3992     if (!stopped())
3993       set_result(result_val-&gt;in(_null_path));
3994     return true;
3995   }
3996 
3997   // After null check, get the object's klass.
3998   Node* obj_klass = load_object_klass(obj);
3999 
4000   // This call may be virtual (invokevirtual) or bound (invokespecial).
4001   // For each case we generate slightly different code.
4002 
4003   // We only go to the fast case code if we pass a number of guards.  The
4004   // paths which do not pass are accumulated in the slow_region.
4005   RegionNode* slow_region = new (C) RegionNode(1);
4006   record_for_igvn(slow_region);
4007 
4008   // If this is a virtual call, we generate a funny guard.  We pull out
4009   // the vtable entry corresponding to hashCode() from the target object.
4010   // If the target method which we are calling happens to be the native
4011   // Object hashCode() method, we pass the guard.  We do not need this
4012   // guard for non-virtual calls -- the caller is known to be the native
4013   // Object hashCode().
4014   if (is_virtual) {
4015     generate_virtual_guard(obj_klass, slow_region);
4016   }
4017 
4018   // Get the header out of the object, use LoadMarkNode when available
4019   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
4020   Node* header = make_load(control(), header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
4021 
4022   // Test the header to see if it is unlocked.
4023   Node *lock_mask      = _gvn.MakeConX(markOopDesc::biased_lock_mask_in_place);
4024   Node *lmasked_header = _gvn.transform(new (C) AndXNode(header, lock_mask));
4025   Node *unlocked_val   = _gvn.MakeConX(markOopDesc::unlocked_value);
4026   Node *chk_unlocked   = _gvn.transform(new (C) CmpXNode( lmasked_header, unlocked_val));
4027   Node *test_unlocked  = _gvn.transform(new (C) BoolNode( chk_unlocked, BoolTest::ne));
4028 
4029   generate_slow_guard(test_unlocked, slow_region);
4030 
4031   // Get the hash value and check to see that it has been properly assigned.
4032   // We depend on hash_mask being at most 32 bits and avoid the use of
4033   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
4034   // vm: see markOop.hpp.
4035   Node *hash_mask      = _gvn.intcon(markOopDesc::hash_mask);
4036   Node *hash_shift     = _gvn.intcon(markOopDesc::hash_shift);
4037   Node *hshifted_header= _gvn.transform(new (C) URShiftXNode(header, hash_shift));
4038   // This hack lets the hash bits live anywhere in the mark object now, as long
4039   // as the shift drops the relevant bits into the low 32 bits.  Note that
4040   // Java spec says that HashCode is an int so there's no point in capturing
4041   // an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
4042   hshifted_header      = ConvX2I(hshifted_header);
4043   Node *hash_val       = _gvn.transform(new (C) AndINode(hshifted_header, hash_mask));
4044 
4045   Node *no_hash_val    = _gvn.intcon(markOopDesc::no_hash);
4046   Node *chk_assigned   = _gvn.transform(new (C) CmpINode( hash_val, no_hash_val));
4047   Node *test_assigned  = _gvn.transform(new (C) BoolNode( chk_assigned, BoolTest::eq));
4048 
4049   generate_slow_guard(test_assigned, slow_region);
4050 
4051   Node* init_mem = reset_memory();
4052   // fill in the rest of the null path:
4053   result_io -&gt;init_req(_null_path, i_o());
4054   result_mem-&gt;init_req(_null_path, init_mem);
4055 
4056   result_val-&gt;init_req(_fast_path, hash_val);
4057   result_reg-&gt;init_req(_fast_path, control());
4058   result_io -&gt;init_req(_fast_path, i_o());
4059   result_mem-&gt;init_req(_fast_path, init_mem);
4060 
4061   // Generate code for the slow case.  We make a call to hashCode().
4062   set_control(_gvn.transform(slow_region));
4063   if (!stopped()) {
4064     // No need for PreserveJVMState, because we're using up the present state.
4065     set_all_memory(init_mem);
4066     vmIntrinsics::ID hashCode_id = is_static ? vmIntrinsics::_identityHashCode : vmIntrinsics::_hashCode;
4067     CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static);
4068     Node* slow_result = set_results_for_java_call(slow_call);
4069     // this-&gt;control() comes from set_results_for_java_call
4070     result_reg-&gt;init_req(_slow_path, control());
4071     result_val-&gt;init_req(_slow_path, slow_result);
4072     result_io  -&gt;set_req(_slow_path, i_o());
4073     result_mem -&gt;set_req(_slow_path, reset_memory());
4074   }
4075 
4076   // Return the combined state.
4077   set_i_o(        _gvn.transform(result_io)  );
4078   set_all_memory( _gvn.transform(result_mem));
4079 
4080   set_result(result_reg, result_val);
4081   return true;
4082 }
4083 
4084 //---------------------------inline_native_getClass----------------------------
4085 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4086 //
4087 // Build special case code for calls to getClass on an object.
4088 bool LibraryCallKit::inline_native_getClass() {
4089   Node* obj = null_check_receiver();
4090   if (stopped())  return true;
4091   set_result(load_mirror_from_klass(load_object_klass(obj)));
4092   return true;
4093 }
4094 
4095 //-----------------inline_native_Reflection_getCallerClass---------------------
4096 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4097 //
4098 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4099 //
4100 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4101 // in that it must skip particular security frames and checks for
4102 // caller sensitive methods.
4103 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4104 #ifndef PRODUCT
4105   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4106     tty-&gt;print_cr("Attempting to inline sun.reflect.Reflection.getCallerClass");
4107   }
4108 #endif
4109 
4110   if (!jvms()-&gt;has_method()) {
4111 #ifndef PRODUCT
4112     if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4113       tty-&gt;print_cr("  Bailing out because intrinsic was inlined at top level");
4114     }
4115 #endif
4116     return false;
4117   }
4118 
4119   // Walk back up the JVM state to find the caller at the required
4120   // depth.
4121   JVMState* caller_jvms = jvms();
4122 
4123   // Cf. JVM_GetCallerClass
4124   // NOTE: Start the loop at depth 1 because the current JVM state does
4125   // not include the Reflection.getCallerClass() frame.
4126   for (int n = 1; caller_jvms != NULL; caller_jvms = caller_jvms-&gt;caller(), n++) {
4127     ciMethod* m = caller_jvms-&gt;method();
4128     switch (n) {
4129     case 0:
4130       fatal("current JVM state does not include the Reflection.getCallerClass frame");
4131       break;
4132     case 1:
4133       // Frame 0 and 1 must be caller sensitive (see JVM_GetCallerClass).
4134       if (!m-&gt;caller_sensitive()) {
4135 #ifndef PRODUCT
4136         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4137           tty-&gt;print_cr("  Bailing out: CallerSensitive annotation expected at frame %d", n);
4138         }
4139 #endif
4140         return false;  // bail-out; let JVM_GetCallerClass do the work
4141       }
4142       break;
4143     default:
4144       if (!m-&gt;is_ignored_by_security_stack_walk()) {
4145         // We have reached the desired frame; return the holder class.
4146         // Acquire method holder as java.lang.Class and push as constant.
4147         ciInstanceKlass* caller_klass = caller_jvms-&gt;method()-&gt;holder();
4148         ciInstance* caller_mirror = caller_klass-&gt;java_mirror();
4149         set_result(makecon(TypeInstPtr::make(caller_mirror)));
4150 
4151 #ifndef PRODUCT
4152         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4153           tty-&gt;print_cr("  Succeeded: caller = %d) %s.%s, JVMS depth = %d", n, caller_klass-&gt;name()-&gt;as_utf8(), caller_jvms-&gt;method()-&gt;name()-&gt;as_utf8(), jvms()-&gt;depth());
4154           tty-&gt;print_cr("  JVM state at this point:");
4155           for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4156             ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4157             tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4158           }
4159         }
4160 #endif
4161         return true;
4162       }
4163       break;
4164     }
4165   }
4166 
4167 #ifndef PRODUCT
4168   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4169     tty-&gt;print_cr("  Bailing out because caller depth exceeded inlining depth = %d", jvms()-&gt;depth());
4170     tty-&gt;print_cr("  JVM state at this point:");
4171     for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4172       ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4173       tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4174     }
4175   }
4176 #endif
4177 
4178   return false;  // bail-out; let JVM_GetCallerClass do the work
4179 }
4180 
4181 bool LibraryCallKit::inline_fp_conversions(vmIntrinsics::ID id) {
4182   Node* arg = argument(0);
4183   Node* result;
4184 
4185   switch (id) {
4186   case vmIntrinsics::_floatToRawIntBits:    result = new (C) MoveF2INode(arg);  break;
4187   case vmIntrinsics::_intBitsToFloat:       result = new (C) MoveI2FNode(arg);  break;
4188   case vmIntrinsics::_doubleToRawLongBits:  result = new (C) MoveD2LNode(arg);  break;
4189   case vmIntrinsics::_longBitsToDouble:     result = new (C) MoveL2DNode(arg);  break;
4190 
4191   case vmIntrinsics::_doubleToLongBits: {
4192     // two paths (plus control) merge in a wood
4193     RegionNode *r = new (C) RegionNode(3);
4194     Node *phi = new (C) PhiNode(r, TypeLong::LONG);
4195 
4196     Node *cmpisnan = _gvn.transform(new (C) CmpDNode(arg, arg));
4197     // Build the boolean node
4198     Node *bolisnan = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::ne));
4199 
4200     // Branch either way.
4201     // NaN case is less traveled, which makes all the difference.
4202     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4203     Node *opt_isnan = _gvn.transform(ifisnan);
4204     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4205     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4206     Node *iftrue = _gvn.transform(new (C) IfTrueNode(opt_ifisnan));
4207 
4208     set_control(iftrue);
4209 
4210     static const jlong nan_bits = CONST64(0x7ff8000000000000);
4211     Node *slow_result = longcon(nan_bits); // return NaN
4212     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4213     r-&gt;init_req(1, iftrue);
4214 
4215     // Else fall through
4216     Node *iffalse = _gvn.transform(new (C) IfFalseNode(opt_ifisnan));
4217     set_control(iffalse);
4218 
4219     phi-&gt;init_req(2, _gvn.transform(new (C) MoveD2LNode(arg)));
4220     r-&gt;init_req(2, iffalse);
4221 
4222     // Post merge
4223     set_control(_gvn.transform(r));
4224     record_for_igvn(r);
4225 
4226     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4227     result = phi;
4228     assert(result-&gt;bottom_type()-&gt;isa_long(), "must be");
4229     break;
4230   }
4231 
4232   case vmIntrinsics::_floatToIntBits: {
4233     // two paths (plus control) merge in a wood
4234     RegionNode *r = new (C) RegionNode(3);
4235     Node *phi = new (C) PhiNode(r, TypeInt::INT);
4236 
4237     Node *cmpisnan = _gvn.transform(new (C) CmpFNode(arg, arg));
4238     // Build the boolean node
4239     Node *bolisnan = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::ne));
4240 
4241     // Branch either way.
4242     // NaN case is less traveled, which makes all the difference.
4243     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4244     Node *opt_isnan = _gvn.transform(ifisnan);
4245     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4246     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4247     Node *iftrue = _gvn.transform(new (C) IfTrueNode(opt_ifisnan));
4248 
4249     set_control(iftrue);
4250 
4251     static const jint nan_bits = 0x7fc00000;
4252     Node *slow_result = makecon(TypeInt::make(nan_bits)); // return NaN
4253     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4254     r-&gt;init_req(1, iftrue);
4255 
4256     // Else fall through
4257     Node *iffalse = _gvn.transform(new (C) IfFalseNode(opt_ifisnan));
4258     set_control(iffalse);
4259 
4260     phi-&gt;init_req(2, _gvn.transform(new (C) MoveF2INode(arg)));
4261     r-&gt;init_req(2, iffalse);
4262 
4263     // Post merge
4264     set_control(_gvn.transform(r));
4265     record_for_igvn(r);
4266 
4267     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4268     result = phi;
4269     assert(result-&gt;bottom_type()-&gt;isa_int(), "must be");
4270     break;
4271   }
4272 
4273   default:
4274     fatal_unexpected_iid(id);
4275     break;
4276   }
4277   set_result(_gvn.transform(result));
4278   return true;
4279 }
4280 
4281 #ifdef _LP64
4282 #define XTOP ,top() /*additional argument*/
4283 #else  //_LP64
4284 #define XTOP        /*no additional argument*/
4285 #endif //_LP64
4286 
4287 //----------------------inline_unsafe_copyMemory-------------------------
4288 // public native void sun.misc.Unsafe.copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);
4289 bool LibraryCallKit::inline_unsafe_copyMemory() {
4290   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
4291   null_check_receiver();  // null-check receiver
4292   if (stopped())  return true;
4293 
4294   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
4295 
4296   Node* src_ptr =         argument(1);   // type: oop
4297   Node* src_off = ConvL2X(argument(2));  // type: long
4298   Node* dst_ptr =         argument(4);   // type: oop
4299   Node* dst_off = ConvL2X(argument(5));  // type: long
4300   Node* size    = ConvL2X(argument(7));  // type: long
4301 
4302   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
4303          "fieldOffset must be byte-scaled");
4304 
4305   Node* src = make_unsafe_address(src_ptr, src_off);
4306   Node* dst = make_unsafe_address(dst_ptr, dst_off);
4307 
4308   // Conservatively insert a memory barrier on all memory slices.
4309   // Do not let writes of the copy source or destination float below the copy.
4310   insert_mem_bar(Op_MemBarCPUOrder);
4311 
4312   // Call it.  Note that the length argument is not scaled.
4313   make_runtime_call(RC_LEAF|RC_NO_FP,
4314                     OptoRuntime::fast_arraycopy_Type(),
4315                     StubRoutines::unsafe_arraycopy(),
4316                     "unsafe_arraycopy",
4317                     TypeRawPtr::BOTTOM,
4318                     src, dst, size XTOP);
4319 
4320   // Do not let reads of the copy destination float above the copy.
4321   insert_mem_bar(Op_MemBarCPUOrder);
4322 
4323   return true;
4324 }
4325 
4326 //------------------------clone_coping-----------------------------------
4327 // Helper function for inline_native_clone.
4328 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark) {
4329   assert(obj_size != NULL, "");
4330   Node* raw_obj = alloc_obj-&gt;in(1);
4331   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), "");
4332 
4333   AllocateNode* alloc = NULL;
4334   if (ReduceBulkZeroing) {
4335     // We will be completely responsible for initializing this object -
4336     // mark Initialize node as complete.
4337     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4338     // The object was just allocated - there should be no any stores!
4339     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), "");
4340     // Mark as complete_with_arraycopy so that on AllocateNode
4341     // expansion, we know this AllocateNode is initialized by an array
4342     // copy and a StoreStore barrier exists after the array copy.
4343     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4344   }
4345 
4346   // Copy the fastest available way.
4347   // TODO: generate fields copies for small objects instead.
4348   Node* src  = obj;
4349   Node* dest = alloc_obj;
4350   Node* size = _gvn.transform(obj_size);
4351 
4352   // Exclude the header but include array length to copy by 8 bytes words.
4353   // Can't use base_offset_in_bytes(bt) since basic type is unknown.
4354   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
4355                             instanceOopDesc::base_offset_in_bytes();
4356   // base_off:
4357   // 8  - 32-bit VM
4358   // 12 - 64-bit VM, compressed klass
4359   // 16 - 64-bit VM, normal klass
4360   if (base_off % BytesPerLong != 0) {
4361     assert(UseCompressedClassPointers, "");
4362     if (is_array) {
4363       // Exclude length to copy by 8 bytes words.
4364       base_off += sizeof(int);
4365     } else {
4366       // Include klass to copy by 8 bytes words.
4367       base_off = instanceOopDesc::klass_offset_in_bytes();
4368     }
4369     assert(base_off % BytesPerLong == 0, "expect 8 bytes alignment");
4370   }
4371   src  = basic_plus_adr(src,  base_off);
4372   dest = basic_plus_adr(dest, base_off);
4373 
4374   // Compute the length also, if needed:
4375   Node* countx = size;
4376   countx = _gvn.transform(new (C) SubXNode(countx, MakeConX(base_off)));
4377   countx = _gvn.transform(new (C) URShiftXNode(countx, intcon(LogBytesPerLong) ));
4378 
4379   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4380   bool disjoint_bases = true;
4381   generate_unchecked_arraycopy(raw_adr_type, T_LONG, disjoint_bases,
4382                                src, NULL, dest, NULL, countx,
4383                                /*dest_uninitialized*/true);
4384 
4385   // If necessary, emit some card marks afterwards.  (Non-arrays only.)
4386   if (card_mark) {
4387     assert(!is_array, "");
4388     // Put in store barrier for any and all oops we are sticking
4389     // into this object.  (We could avoid this if we could prove
4390     // that the object type contains no oop fields at all.)
4391     Node* no_particular_value = NULL;
4392     Node* no_particular_field = NULL;
4393     int raw_adr_idx = Compile::AliasIdxRaw;
4394     post_barrier(control(),
4395                  memory(raw_adr_type),
4396                  alloc_obj,
4397                  no_particular_field,
4398                  raw_adr_idx,
4399                  no_particular_value,
4400                  T_OBJECT,
4401                  false);
4402   }
4403 
4404   // Do not let reads from the cloned object float above the arraycopy.
4405   if (alloc != NULL) {
4406     // Do not let stores that initialize this object be reordered with
4407     // a subsequent store that would make this object accessible by
4408     // other threads.
4409     // Record what AllocateNode this StoreStore protects so that
4410     // escape analysis can go from the MemBarStoreStoreNode to the
4411     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4412     // based on the escape status of the AllocateNode.
4413     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
4414   } else {
4415     insert_mem_bar(Op_MemBarCPUOrder);
4416   }
4417 }
4418 
4419 //------------------------inline_native_clone----------------------------
4420 // protected native Object java.lang.Object.clone();
4421 //
4422 // Here are the simple edge cases:
4423 //  null receiver =&gt; normal trap
4424 //  virtual and clone was overridden =&gt; slow path to out-of-line clone
4425 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4426 //
4427 // The general case has two steps, allocation and copying.
4428 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4429 //
4430 // Copying also has two cases, oop arrays and everything else.
4431 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4432 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4433 //
4434 // These steps fold up nicely if and when the cloned object's klass
4435 // can be sharply typed as an object array, a type array, or an instance.
4436 //
4437 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4438   PhiNode* result_val;
4439 
4440   // Set the reexecute bit for the interpreter to reexecute
4441   // the bytecode that invokes Object.clone if deoptimization happens.
4442   { PreserveReexecuteState preexecs(this);
4443     jvms()-&gt;set_should_reexecute(true);
4444 
4445     Node* obj = null_check_receiver();
4446     if (stopped())  return true;
4447 
4448     Node* obj_klass = load_object_klass(obj);
4449     const TypeKlassPtr* tklass = _gvn.type(obj_klass)-&gt;isa_klassptr();
4450     const TypeOopPtr*   toop   = ((tklass != NULL)
4451                                 ? tklass-&gt;as_instance_type()
4452                                 : TypeInstPtr::NOTNULL);
4453 
4454     // Conservatively insert a memory barrier on all memory slices.
4455     // Do not let writes into the original float below the clone.
4456     insert_mem_bar(Op_MemBarCPUOrder);
4457 
4458     // paths into result_reg:
4459     enum {
4460       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4461       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4462       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4463       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4464       PATH_LIMIT
4465     };
4466     RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
4467     result_val             = new(C) PhiNode(result_reg,
4468                                             TypeInstPtr::NOTNULL);
4469     PhiNode*    result_i_o = new(C) PhiNode(result_reg, Type::ABIO);
4470     PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY,
4471                                             TypePtr::BOTTOM);
4472     record_for_igvn(result_reg);
4473 
4474     const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4475     int raw_adr_idx = Compile::AliasIdxRaw;
4476 
4477     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4478     if (array_ctl != NULL) {
4479       // It's an array.
4480       PreserveJVMState pjvms(this);
4481       set_control(array_ctl);
4482       Node* obj_length = load_array_length(obj);
4483       Node* obj_size  = NULL;
4484       Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push
4485 
4486       if (!use_ReduceInitialCardMarks()) {
4487         // If it is an oop array, it requires very special treatment,
4488         // because card marking is required on each card of the array.
4489         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4490         if (is_obja != NULL) {
4491           PreserveJVMState pjvms2(this);
4492           set_control(is_obja);
4493           // Generate a direct call to the right arraycopy function(s).
4494           bool disjoint_bases = true;
4495           bool length_never_negative = true;
4496           generate_arraycopy(TypeAryPtr::OOPS, T_OBJECT,
4497                              obj, intcon(0), alloc_obj, intcon(0),
4498                              obj_length,
4499                              disjoint_bases, length_never_negative);
4500           result_reg-&gt;init_req(_objArray_path, control());
4501           result_val-&gt;init_req(_objArray_path, alloc_obj);
4502           result_i_o -&gt;set_req(_objArray_path, i_o());
4503           result_mem -&gt;set_req(_objArray_path, reset_memory());
4504         }
4505       }
4506       // Otherwise, there are no card marks to worry about.
4507       // (We can dispense with card marks if we know the allocation
4508       //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
4509       //  causes the non-eden paths to take compensating steps to
4510       //  simulate a fresh allocation, so that no further
4511       //  card marks are required in compiled code to initialize
4512       //  the object.)
4513 
4514       if (!stopped()) {
4515         copy_to_clone(obj, alloc_obj, obj_size, true, false);
4516 
4517         // Present the results of the copy.
4518         result_reg-&gt;init_req(_array_path, control());
4519         result_val-&gt;init_req(_array_path, alloc_obj);
4520         result_i_o -&gt;set_req(_array_path, i_o());
4521         result_mem -&gt;set_req(_array_path, reset_memory());
4522       }
4523     }
4524 
4525     // We only go to the instance fast case code if we pass a number of guards.
4526     // The paths which do not pass are accumulated in the slow_region.
4527     RegionNode* slow_region = new (C) RegionNode(1);
4528     record_for_igvn(slow_region);
4529     if (!stopped()) {
4530       // It's an instance (we did array above).  Make the slow-path tests.
4531       // If this is a virtual call, we generate a funny guard.  We grab
4532       // the vtable entry corresponding to clone() from the target object.
4533       // If the target method which we are calling happens to be the
4534       // Object clone() method, we pass the guard.  We do not need this
4535       // guard for non-virtual calls; the caller is known to be the native
4536       // Object clone().
4537       if (is_virtual) {
4538         generate_virtual_guard(obj_klass, slow_region);
4539       }
4540 
4541       // The object must be cloneable and must not have a finalizer.
4542       // Both of these conditions may be checked in a single test.
4543       // We could optimize the cloneable test further, but we don't care.
4544       generate_access_flags_guard(obj_klass,
4545                                   // Test both conditions:
4546                                   JVM_ACC_IS_CLONEABLE | JVM_ACC_HAS_FINALIZER,
4547                                   // Must be cloneable but not finalizer:
4548                                   JVM_ACC_IS_CLONEABLE,
4549                                   slow_region);
4550     }
4551 
4552     if (!stopped()) {
4553       // It's an instance, and it passed the slow-path tests.
4554       PreserveJVMState pjvms(this);
4555       Node* obj_size  = NULL;
4556       Node* alloc_obj = new_instance(obj_klass, NULL, &amp;obj_size);
4557 
4558       copy_to_clone(obj, alloc_obj, obj_size, false, !use_ReduceInitialCardMarks());
4559 
4560       // Present the results of the slow call.
4561       result_reg-&gt;init_req(_instance_path, control());
4562       result_val-&gt;init_req(_instance_path, alloc_obj);
4563       result_i_o -&gt;set_req(_instance_path, i_o());
4564       result_mem -&gt;set_req(_instance_path, reset_memory());
4565     }
4566 
4567     // Generate code for the slow case.  We make a call to clone().
4568     set_control(_gvn.transform(slow_region));
4569     if (!stopped()) {
4570       PreserveJVMState pjvms(this);
4571       CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual);
4572       Node* slow_result = set_results_for_java_call(slow_call);
4573       // this-&gt;control() comes from set_results_for_java_call
4574       result_reg-&gt;init_req(_slow_path, control());
4575       result_val-&gt;init_req(_slow_path, slow_result);
4576       result_i_o -&gt;set_req(_slow_path, i_o());
4577       result_mem -&gt;set_req(_slow_path, reset_memory());
4578     }
4579 
4580     // Return the combined state.
4581     set_control(    _gvn.transform(result_reg));
4582     set_i_o(        _gvn.transform(result_i_o));
4583     set_all_memory( _gvn.transform(result_mem));
4584   } // original reexecute is set back here
4585 
4586   set_result(_gvn.transform(result_val));
4587   return true;
4588 }
4589 
4590 //------------------------------basictype2arraycopy----------------------------
4591 address LibraryCallKit::basictype2arraycopy(BasicType t,
4592                                             Node* src_offset,
4593                                             Node* dest_offset,
4594                                             bool disjoint_bases,
4595                                             const char* &amp;name,
4596                                             bool dest_uninitialized) {
4597   const TypeInt* src_offset_inttype  = gvn().find_int_type(src_offset);;
4598   const TypeInt* dest_offset_inttype = gvn().find_int_type(dest_offset);;
4599 
4600   bool aligned = false;
4601   bool disjoint = disjoint_bases;
4602 
4603   // if the offsets are the same, we can treat the memory regions as
4604   // disjoint, because either the memory regions are in different arrays,
4605   // or they are identical (which we can treat as disjoint.)  We can also
4606   // treat a copy with a destination index  less that the source index
4607   // as disjoint since a low-&gt;high copy will work correctly in this case.
4608   if (src_offset_inttype != NULL &amp;&amp; src_offset_inttype-&gt;is_con() &amp;&amp;
4609       dest_offset_inttype != NULL &amp;&amp; dest_offset_inttype-&gt;is_con()) {
4610     // both indices are constants
4611     int s_offs = src_offset_inttype-&gt;get_con();
4612     int d_offs = dest_offset_inttype-&gt;get_con();
4613     int element_size = type2aelembytes(t);
4614     aligned = ((arrayOopDesc::base_offset_in_bytes(t) + s_offs * element_size) % HeapWordSize == 0) &amp;&amp;
4615               ((arrayOopDesc::base_offset_in_bytes(t) + d_offs * element_size) % HeapWordSize == 0);
4616     if (s_offs &gt;= d_offs)  disjoint = true;
4617   } else if (src_offset == dest_offset &amp;&amp; src_offset != NULL) {
4618     // This can occur if the offsets are identical non-constants.
4619     disjoint = true;
4620   }
4621 
4622   return StubRoutines::select_arraycopy_function(t, aligned, disjoint, name, dest_uninitialized);
4623 }
4624 
4625 
4626 //------------------------------inline_arraycopy-----------------------
4627 // public static native void java.lang.System.arraycopy(Object src,  int  srcPos,
4628 //                                                      Object dest, int destPos,
4629 //                                                      int length);
4630 bool LibraryCallKit::inline_arraycopy() {
4631   // Get the arguments.
4632   Node* src         = argument(0);  // type: oop
4633   Node* src_offset  = argument(1);  // type: int
4634   Node* dest        = argument(2);  // type: oop
4635   Node* dest_offset = argument(3);  // type: int
4636   Node* length      = argument(4);  // type: int
4637 
4638   // Compile time checks.  If any of these checks cannot be verified at compile time,
4639   // we do not make a fast path for this call.  Instead, we let the call remain as it
4640   // is.  The checks we choose to mandate at compile time are:
4641   //
4642   // (1) src and dest are arrays.
4643   const Type* src_type  = src-&gt;Value(&amp;_gvn);
4644   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
4645   const TypeAryPtr* top_src  = src_type-&gt;isa_aryptr();
4646   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
4647 
4648   // Do we have the type of src?
4649   bool has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4650   // Do we have the type of dest?
4651   bool has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4652   // Is the type for src from speculation?
4653   bool src_spec = false;
4654   // Is the type for dest from speculation?
4655   bool dest_spec = false;
4656 
4657   if (!has_src || !has_dest) {
4658     // We don't have sufficient type information, let's see if
4659     // speculative types can help. We need to have types for both src
4660     // and dest so that it pays off.
4661 
4662     // Do we already have or could we have type information for src
4663     bool could_have_src = has_src;
4664     // Do we already have or could we have type information for dest
4665     bool could_have_dest = has_dest;
4666 
4667     ciKlass* src_k = NULL;
4668     if (!has_src) {
4669       src_k = src_type-&gt;speculative_type_not_null();
4670       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4671         could_have_src = true;
4672       }
4673     }
4674 
4675     ciKlass* dest_k = NULL;
4676     if (!has_dest) {
4677       dest_k = dest_type-&gt;speculative_type_not_null();
4678       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4679         could_have_dest = true;
4680       }
4681     }
4682 
4683     if (could_have_src &amp;&amp; could_have_dest) {
4684       // This is going to pay off so emit the required guards
4685       if (!has_src) {
4686         src = maybe_cast_profiled_obj(src, src_k);
4687         src_type  = _gvn.type(src);
4688         top_src  = src_type-&gt;isa_aryptr();
4689         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4690         src_spec = true;
4691       }
4692       if (!has_dest) {
4693         dest = maybe_cast_profiled_obj(dest, dest_k);
4694         dest_type  = _gvn.type(dest);
4695         top_dest  = dest_type-&gt;isa_aryptr();
4696         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4697         dest_spec = true;
4698       }
4699     }
4700   }
4701 
4702   if (!has_src || !has_dest) {
4703     // Conservatively insert a memory barrier on all memory slices.
4704     // Do not let writes into the source float below the arraycopy.
4705     insert_mem_bar(Op_MemBarCPUOrder);
4706 
4707     // Call StubRoutines::generic_arraycopy stub.
4708     generate_arraycopy(TypeRawPtr::BOTTOM, T_CONFLICT,
4709                        src, src_offset, dest, dest_offset, length);
4710 
4711     // Do not let reads from the destination float above the arraycopy.
4712     // Since we cannot type the arrays, we don't know which slices
4713     // might be affected.  We could restrict this barrier only to those
4714     // memory slices which pertain to array elements--but don't bother.
4715     if (!InsertMemBarAfterArraycopy)
4716       // (If InsertMemBarAfterArraycopy, there is already one in place.)
4717       insert_mem_bar(Op_MemBarCPUOrder);
4718     return true;
4719   }
4720 
4721   // (2) src and dest arrays must have elements of the same BasicType
4722   // Figure out the size and type of the elements we will be copying.
4723   BasicType src_elem  =  top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4724   BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4725   if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
4726   if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
4727 
4728   if (src_elem != dest_elem || dest_elem == T_VOID) {
4729     // The component types are not the same or are not recognized.  Punt.
4730     // (But, avoid the native method wrapper to JVM_ArrayCopy.)
4731     generate_slow_arraycopy(TypePtr::BOTTOM,
4732                             src, src_offset, dest, dest_offset, length,
4733                             /*dest_uninitialized*/false);
4734     return true;
4735   }
4736 
4737   if (src_elem == T_OBJECT) {
4738     // If both arrays are object arrays then having the exact types
4739     // for both will remove the need for a subtype check at runtime
4740     // before the call and may make it possible to pick a faster copy
4741     // routine (without a subtype check on every element)
4742     // Do we have the exact type of src?
4743     bool could_have_src = src_spec;
4744     // Do we have the exact type of dest?
4745     bool could_have_dest = dest_spec;
4746     ciKlass* src_k = top_src-&gt;klass();
4747     ciKlass* dest_k = top_dest-&gt;klass();
4748     if (!src_spec) {
4749       src_k = src_type-&gt;speculative_type_not_null();
4750       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4751           could_have_src = true;
4752       }
4753     }
4754     if (!dest_spec) {
4755       dest_k = dest_type-&gt;speculative_type_not_null();
4756       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4757         could_have_dest = true;
4758       }
4759     }
4760     if (could_have_src &amp;&amp; could_have_dest) {
4761       // If we can have both exact types, emit the missing guards
4762       if (could_have_src &amp;&amp; !src_spec) {
4763         src = maybe_cast_profiled_obj(src, src_k);
4764       }
4765       if (could_have_dest &amp;&amp; !dest_spec) {
4766         dest = maybe_cast_profiled_obj(dest, dest_k);
4767       }
4768     }
4769   }
4770 
4771   //---------------------------------------------------------------------------
4772   // We will make a fast path for this call to arraycopy.
4773 
4774   // We have the following tests left to perform:
4775   //
4776   // (3) src and dest must not be null.
4777   // (4) src_offset must not be negative.
4778   // (5) dest_offset must not be negative.
4779   // (6) length must not be negative.
4780   // (7) src_offset + length must not exceed length of src.
4781   // (8) dest_offset + length must not exceed length of dest.
4782   // (9) each element of an oop array must be assignable
4783 
4784   RegionNode* slow_region = new (C) RegionNode(1);
4785   record_for_igvn(slow_region);
4786 
4787   // (3) operands must not be null
4788   // We currently perform our null checks with the null_check routine.
4789   // This means that the null exceptions will be reported in the caller
4790   // rather than (correctly) reported inside of the native arraycopy call.
4791   // This should be corrected, given time.  We do our null check with the
4792   // stack pointer restored.
4793   src  = null_check(src,  T_ARRAY);
4794   dest = null_check(dest, T_ARRAY);
4795 
4796   // (4) src_offset must not be negative.
4797   generate_negative_guard(src_offset, slow_region);
4798 
4799   // (5) dest_offset must not be negative.
4800   generate_negative_guard(dest_offset, slow_region);
4801 
4802   // (6) length must not be negative (moved to generate_arraycopy()).
4803   // generate_negative_guard(length, slow_region);
4804 
4805   // (7) src_offset + length must not exceed length of src.
4806   generate_limit_guard(src_offset, length,
4807                        load_array_length(src),
4808                        slow_region);
4809 
4810   // (8) dest_offset + length must not exceed length of dest.
4811   generate_limit_guard(dest_offset, length,
4812                        load_array_length(dest),
4813                        slow_region);
4814 
4815   // (9) each element of an oop array must be assignable
4816   // The generate_arraycopy subroutine checks this.
4817 
4818   // This is where the memory effects are placed:
4819   const TypePtr* adr_type = TypeAryPtr::get_array_body_type(dest_elem);
4820   generate_arraycopy(adr_type, dest_elem,
4821                      src, src_offset, dest, dest_offset, length,
4822                      false, false, slow_region);
4823 
4824   return true;
4825 }
4826 
4827 //-----------------------------generate_arraycopy----------------------
4828 // Generate an optimized call to arraycopy.
4829 // Caller must guard against non-arrays.
4830 // Caller must determine a common array basic-type for both arrays.
4831 // Caller must validate offsets against array bounds.
4832 // The slow_region has already collected guard failure paths
4833 // (such as out of bounds length or non-conformable array types).
4834 // The generated code has this shape, in general:
4835 //
4836 //     if (length == 0)  return   // via zero_path
4837 //     slowval = -1
4838 //     if (types unknown) {
4839 //       slowval = call generic copy loop
4840 //       if (slowval == 0)  return  // via checked_path
4841 //     } else if (indexes in bounds) {
4842 //       if ((is object array) &amp;&amp; !(array type check)) {
4843 //         slowval = call checked copy loop
4844 //         if (slowval == 0)  return  // via checked_path
4845 //       } else {
4846 //         call bulk copy loop
4847 //         return  // via fast_path
4848 //       }
4849 //     }
4850 //     // adjust params for remaining work:
4851 //     if (slowval != -1) {
4852 //       n = -1^slowval; src_offset += n; dest_offset += n; length -= n
4853 //     }
4854 //   slow_region:
4855 //     call slow arraycopy(src, src_offset, dest, dest_offset, length)
4856 //     return  // via slow_call_path
4857 //
4858 // This routine is used from several intrinsics:  System.arraycopy,
4859 // Object.clone (the array subcase), and Arrays.copyOf[Range].
4860 //
4861 void
4862 LibraryCallKit::generate_arraycopy(const TypePtr* adr_type,
4863                                    BasicType basic_elem_type,
4864                                    Node* src,  Node* src_offset,
4865                                    Node* dest, Node* dest_offset,
4866                                    Node* copy_length,
4867                                    bool disjoint_bases,
4868                                    bool length_never_negative,
4869                                    RegionNode* slow_region) {
4870 
4871   if (slow_region == NULL) {
4872     slow_region = new(C) RegionNode(1);
4873     record_for_igvn(slow_region);
4874   }
4875 
4876   Node* original_dest      = dest;
4877   AllocateArrayNode* alloc = NULL;  // used for zeroing, if needed
4878   bool  dest_uninitialized = false;
4879 
4880   // See if this is the initialization of a newly-allocated array.
4881   // If so, we will take responsibility here for initializing it to zero.
4882   // (Note:  Because tightly_coupled_allocation performs checks on the
4883   // out-edges of the dest, we need to avoid making derived pointers
4884   // from it until we have checked its uses.)
4885   if (ReduceBulkZeroing
4886       &amp;&amp; !ZeroTLAB              // pointless if already zeroed
4887       &amp;&amp; basic_elem_type != T_CONFLICT // avoid corner case
4888       &amp;&amp; !src-&gt;eqv_uncast(dest)
4889       &amp;&amp; ((alloc = tightly_coupled_allocation(dest, slow_region))
4890           != NULL)
4891       &amp;&amp; _gvn.find_int_con(alloc-&gt;in(AllocateNode::ALength), 1) &gt; 0
4892       &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn)) {
4893     // "You break it, you buy it."
4894     InitializeNode* init = alloc-&gt;initialization();
4895     assert(init-&gt;is_complete(), "we just did this");
4896     init-&gt;set_complete_with_arraycopy();
4897     assert(dest-&gt;is_CheckCastPP(), "sanity");
4898     assert(dest-&gt;in(0)-&gt;in(0) == init, "dest pinned");
4899     adr_type = TypeRawPtr::BOTTOM;  // all initializations are into raw memory
4900     // From this point on, every exit path is responsible for
4901     // initializing any non-copied parts of the object to zero.
4902     // Also, if this flag is set we make sure that arraycopy interacts properly
4903     // with G1, eliding pre-barriers. See CR 6627983.
4904     dest_uninitialized = true;
4905   } else {
4906     // No zeroing elimination here.
4907     alloc             = NULL;
4908     //original_dest   = dest;
4909     //dest_uninitialized = false;
4910   }
4911 
4912   // Results are placed here:
4913   enum { fast_path        = 1,  // normal void-returning assembly stub
4914          checked_path     = 2,  // special assembly stub with cleanup
4915          slow_call_path   = 3,  // something went wrong; call the VM
4916          zero_path        = 4,  // bypass when length of copy is zero
4917          bcopy_path       = 5,  // copy primitive array by 64-bit blocks
4918          PATH_LIMIT       = 6
4919   };
4920   RegionNode* result_region = new(C) RegionNode(PATH_LIMIT);
4921   PhiNode*    result_i_o    = new(C) PhiNode(result_region, Type::ABIO);
4922   PhiNode*    result_memory = new(C) PhiNode(result_region, Type::MEMORY, adr_type);
4923   record_for_igvn(result_region);
4924   _gvn.set_type_bottom(result_i_o);
4925   _gvn.set_type_bottom(result_memory);
4926   assert(adr_type != TypePtr::BOTTOM, "must be RawMem or a T[] slice");
4927 
4928   // The slow_control path:
4929   Node* slow_control;
4930   Node* slow_i_o = i_o();
4931   Node* slow_mem = memory(adr_type);
4932   debug_only(slow_control = (Node*) badAddress);
4933 
4934   // Checked control path:
4935   Node* checked_control = top();
4936   Node* checked_mem     = NULL;
4937   Node* checked_i_o     = NULL;
4938   Node* checked_value   = NULL;
4939 
4940   if (basic_elem_type == T_CONFLICT) {
4941     assert(!dest_uninitialized, "");
4942     Node* cv = generate_generic_arraycopy(adr_type,
4943                                           src, src_offset, dest, dest_offset,
4944                                           copy_length, dest_uninitialized);
4945     if (cv == NULL)  cv = intcon(-1);  // failure (no stub available)
4946     checked_control = control();
4947     checked_i_o     = i_o();
4948     checked_mem     = memory(adr_type);
4949     checked_value   = cv;
4950     set_control(top());         // no fast path
4951   }
4952 
4953   Node* not_pos = generate_nonpositive_guard(copy_length, length_never_negative);
4954   if (not_pos != NULL) {
4955     PreserveJVMState pjvms(this);
4956     set_control(not_pos);
4957 
4958     // (6) length must not be negative.
4959     if (!length_never_negative) {
4960       generate_negative_guard(copy_length, slow_region);
4961     }
4962 
4963     // copy_length is 0.
4964     if (!stopped() &amp;&amp; dest_uninitialized) {
4965       Node* dest_length = alloc-&gt;in(AllocateNode::ALength);
4966       if (copy_length-&gt;eqv_uncast(dest_length)
4967           || _gvn.find_int_con(dest_length, 1) &lt;= 0) {
4968         // There is no zeroing to do. No need for a secondary raw memory barrier.
4969       } else {
4970         // Clear the whole thing since there are no source elements to copy.
4971         generate_clear_array(adr_type, dest, basic_elem_type,
4972                              intcon(0), NULL,
4973                              alloc-&gt;in(AllocateNode::AllocSize));
4974         // Use a secondary InitializeNode as raw memory barrier.
4975         // Currently it is needed only on this path since other
4976         // paths have stub or runtime calls as raw memory barriers.
4977         InitializeNode* init = insert_mem_bar_volatile(Op_Initialize,
4978                                                        Compile::AliasIdxRaw,
4979                                                        top())-&gt;as_Initialize();
4980         init-&gt;set_complete(&amp;_gvn);  // (there is no corresponding AllocateNode)
4981       }
4982     }
4983 
4984     // Present the results of the fast call.
4985     result_region-&gt;init_req(zero_path, control());
4986     result_i_o   -&gt;init_req(zero_path, i_o());
4987     result_memory-&gt;init_req(zero_path, memory(adr_type));
4988   }
4989 
4990   if (!stopped() &amp;&amp; dest_uninitialized) {
4991     // We have to initialize the *uncopied* part of the array to zero.
4992     // The copy destination is the slice dest[off..off+len].  The other slices
4993     // are dest_head = dest[0..off] and dest_tail = dest[off+len..dest.length].
4994     Node* dest_size   = alloc-&gt;in(AllocateNode::AllocSize);
4995     Node* dest_length = alloc-&gt;in(AllocateNode::ALength);
4996     Node* dest_tail   = _gvn.transform(new(C) AddINode(dest_offset,
4997                                                           copy_length));
4998 
4999     // If there is a head section that needs zeroing, do it now.
5000     if (find_int_con(dest_offset, -1) != 0) {
5001       generate_clear_array(adr_type, dest, basic_elem_type,
5002                            intcon(0), dest_offset,
5003                            NULL);
5004     }
5005 
5006     // Next, perform a dynamic check on the tail length.
5007     // It is often zero, and we can win big if we prove this.
5008     // There are two wins:  Avoid generating the ClearArray
5009     // with its attendant messy index arithmetic, and upgrade
5010     // the copy to a more hardware-friendly word size of 64 bits.
5011     Node* tail_ctl = NULL;
5012     if (!stopped() &amp;&amp; !dest_tail-&gt;eqv_uncast(dest_length)) {
5013       Node* cmp_lt   = _gvn.transform(new(C) CmpINode(dest_tail, dest_length));
5014       Node* bol_lt   = _gvn.transform(new(C) BoolNode(cmp_lt, BoolTest::lt));
5015       tail_ctl = generate_slow_guard(bol_lt, NULL);
5016       assert(tail_ctl != NULL || !stopped(), "must be an outcome");
5017     }
5018 
5019     // At this point, let's assume there is no tail.
5020     if (!stopped() &amp;&amp; alloc != NULL &amp;&amp; basic_elem_type != T_OBJECT) {
5021       // There is no tail.  Try an upgrade to a 64-bit copy.
5022       bool didit = false;
5023       { PreserveJVMState pjvms(this);
5024         didit = generate_block_arraycopy(adr_type, basic_elem_type, alloc,
5025                                          src, src_offset, dest, dest_offset,
5026                                          dest_size, dest_uninitialized);
5027         if (didit) {
5028           // Present the results of the block-copying fast call.
5029           result_region-&gt;init_req(bcopy_path, control());
5030           result_i_o   -&gt;init_req(bcopy_path, i_o());
5031           result_memory-&gt;init_req(bcopy_path, memory(adr_type));
5032         }
5033       }
5034       if (didit)
5035         set_control(top());     // no regular fast path
5036     }
5037 
5038     // Clear the tail, if any.
5039     if (tail_ctl != NULL) {
5040       Node* notail_ctl = stopped() ? NULL : control();
5041       set_control(tail_ctl);
5042       if (notail_ctl == NULL) {
5043         generate_clear_array(adr_type, dest, basic_elem_type,
5044                              dest_tail, NULL,
5045                              dest_size);
5046       } else {
5047         // Make a local merge.
5048         Node* done_ctl = new(C) RegionNode(3);
5049         Node* done_mem = new(C) PhiNode(done_ctl, Type::MEMORY, adr_type);
5050         done_ctl-&gt;init_req(1, notail_ctl);
5051         done_mem-&gt;init_req(1, memory(adr_type));
5052         generate_clear_array(adr_type, dest, basic_elem_type,
5053                              dest_tail, NULL,
5054                              dest_size);
5055         done_ctl-&gt;init_req(2, control());
5056         done_mem-&gt;init_req(2, memory(adr_type));
5057         set_control( _gvn.transform(done_ctl));
5058         set_memory(  _gvn.transform(done_mem), adr_type );
5059       }
5060     }
5061   }
5062 
5063   BasicType copy_type = basic_elem_type;
5064   assert(basic_elem_type != T_ARRAY, "caller must fix this");
5065   if (!stopped() &amp;&amp; copy_type == T_OBJECT) {
5066     // If src and dest have compatible element types, we can copy bits.
5067     // Types S[] and D[] are compatible if D is a supertype of S.
5068     //
5069     // If they are not, we will use checked_oop_disjoint_arraycopy,
5070     // which performs a fast optimistic per-oop check, and backs off
5071     // further to JVM_ArrayCopy on the first per-oop check that fails.
5072     // (Actually, we don't move raw bits only; the GC requires card marks.)
5073 
5074     // Get the Klass* for both src and dest
5075     Node* src_klass  = load_object_klass(src);
5076     Node* dest_klass = load_object_klass(dest);
5077 
5078     // Generate the subtype check.
5079     // This might fold up statically, or then again it might not.
5080     //
5081     // Non-static example:  Copying List&lt;String&gt;.elements to a new String[].
5082     // The backing store for a List&lt;String&gt; is always an Object[],
5083     // but its elements are always type String, if the generic types
5084     // are correct at the source level.
5085     //
5086     // Test S[] against D[], not S against D, because (probably)
5087     // the secondary supertype cache is less busy for S[] than S.
5088     // This usually only matters when D is an interface.
5089     Node* not_subtype_ctrl = gen_subtype_check(src_klass, dest_klass);
5090     // Plug failing path into checked_oop_disjoint_arraycopy
5091     if (not_subtype_ctrl != top()) {
5092       PreserveJVMState pjvms(this);
5093       set_control(not_subtype_ctrl);
5094       // (At this point we can assume disjoint_bases, since types differ.)
5095       int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
5096       Node* p1 = basic_plus_adr(dest_klass, ek_offset);
5097       Node* n1 = LoadKlassNode::make(_gvn, immutable_memory(), p1, TypeRawPtr::BOTTOM);
5098       Node* dest_elem_klass = _gvn.transform(n1);
5099       Node* cv = generate_checkcast_arraycopy(adr_type,
5100                                               dest_elem_klass,
5101                                               src, src_offset, dest, dest_offset,
5102                                               ConvI2X(copy_length), dest_uninitialized);
5103       if (cv == NULL)  cv = intcon(-1);  // failure (no stub available)
5104       checked_control = control();
5105       checked_i_o     = i_o();
5106       checked_mem     = memory(adr_type);
5107       checked_value   = cv;
5108     }
5109     // At this point we know we do not need type checks on oop stores.
5110 
5111     // Let's see if we need card marks:
5112     if (alloc != NULL &amp;&amp; use_ReduceInitialCardMarks()) {
5113       // If we do not need card marks, copy using the jint or jlong stub.
5114       copy_type = LP64_ONLY(UseCompressedOops ? T_INT : T_LONG) NOT_LP64(T_INT);
5115       assert(type2aelembytes(basic_elem_type) == type2aelembytes(copy_type),
5116              "sizes agree");
5117     }
5118   }
5119 
5120   if (!stopped()) {
5121     // Generate the fast path, if possible.
5122     PreserveJVMState pjvms(this);
5123     generate_unchecked_arraycopy(adr_type, copy_type, disjoint_bases,
5124                                  src, src_offset, dest, dest_offset,
5125                                  ConvI2X(copy_length), dest_uninitialized);
5126 
5127     // Present the results of the fast call.
5128     result_region-&gt;init_req(fast_path, control());
5129     result_i_o   -&gt;init_req(fast_path, i_o());
5130     result_memory-&gt;init_req(fast_path, memory(adr_type));
5131   }
5132 
5133   // Here are all the slow paths up to this point, in one bundle:
5134   slow_control = top();
5135   if (slow_region != NULL)
5136     slow_control = _gvn.transform(slow_region);
5137   DEBUG_ONLY(slow_region = (RegionNode*)badAddress);
5138 
5139   set_control(checked_control);
5140   if (!stopped()) {
5141     // Clean up after the checked call.
5142     // The returned value is either 0 or -1^K,
5143     // where K = number of partially transferred array elements.
5144     Node* cmp = _gvn.transform(new(C) CmpINode(checked_value, intcon(0)));
5145     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::eq));
5146     IfNode* iff = create_and_map_if(control(), bol, PROB_MAX, COUNT_UNKNOWN);
5147 
5148     // If it is 0, we are done, so transfer to the end.
5149     Node* checks_done = _gvn.transform(new(C) IfTrueNode(iff));
5150     result_region-&gt;init_req(checked_path, checks_done);
5151     result_i_o   -&gt;init_req(checked_path, checked_i_o);
5152     result_memory-&gt;init_req(checked_path, checked_mem);
5153 
5154     // If it is not zero, merge into the slow call.
5155     set_control( _gvn.transform(new(C) IfFalseNode(iff) ));
5156     RegionNode* slow_reg2 = new(C) RegionNode(3);
5157     PhiNode*    slow_i_o2 = new(C) PhiNode(slow_reg2, Type::ABIO);
5158     PhiNode*    slow_mem2 = new(C) PhiNode(slow_reg2, Type::MEMORY, adr_type);
5159     record_for_igvn(slow_reg2);
5160     slow_reg2  -&gt;init_req(1, slow_control);
5161     slow_i_o2  -&gt;init_req(1, slow_i_o);
5162     slow_mem2  -&gt;init_req(1, slow_mem);
5163     slow_reg2  -&gt;init_req(2, control());
5164     slow_i_o2  -&gt;init_req(2, checked_i_o);
5165     slow_mem2  -&gt;init_req(2, checked_mem);
5166 
5167     slow_control = _gvn.transform(slow_reg2);
5168     slow_i_o     = _gvn.transform(slow_i_o2);
5169     slow_mem     = _gvn.transform(slow_mem2);
5170 
5171     if (alloc != NULL) {
5172       // We'll restart from the very beginning, after zeroing the whole thing.
5173       // This can cause double writes, but that's OK since dest is brand new.
5174       // So we ignore the low 31 bits of the value returned from the stub.
5175     } else {
5176       // We must continue the copy exactly where it failed, or else
5177       // another thread might see the wrong number of writes to dest.
5178       Node* checked_offset = _gvn.transform(new(C) XorINode(checked_value, intcon(-1)));
5179       Node* slow_offset    = new(C) PhiNode(slow_reg2, TypeInt::INT);
5180       slow_offset-&gt;init_req(1, intcon(0));
5181       slow_offset-&gt;init_req(2, checked_offset);
5182       slow_offset  = _gvn.transform(slow_offset);
5183 
5184       // Adjust the arguments by the conditionally incoming offset.
5185       Node* src_off_plus  = _gvn.transform(new(C) AddINode(src_offset,  slow_offset));
5186       Node* dest_off_plus = _gvn.transform(new(C) AddINode(dest_offset, slow_offset));
5187       Node* length_minus  = _gvn.transform(new(C) SubINode(copy_length, slow_offset));
5188 
5189       // Tweak the node variables to adjust the code produced below:
5190       src_offset  = src_off_plus;
5191       dest_offset = dest_off_plus;
5192       copy_length = length_minus;
5193     }
5194   }
5195 
5196   set_control(slow_control);
5197   if (!stopped()) {
5198     // Generate the slow path, if needed.
5199     PreserveJVMState pjvms(this);   // replace_in_map may trash the map
5200 
5201     set_memory(slow_mem, adr_type);
5202     set_i_o(slow_i_o);
5203 
5204     if (dest_uninitialized) {
5205       generate_clear_array(adr_type, dest, basic_elem_type,
5206                            intcon(0), NULL,
5207                            alloc-&gt;in(AllocateNode::AllocSize));
5208     }
5209 
5210     generate_slow_arraycopy(adr_type,
5211                             src, src_offset, dest, dest_offset,
5212                             copy_length, /*dest_uninitialized*/false);
5213 
5214     result_region-&gt;init_req(slow_call_path, control());
5215     result_i_o   -&gt;init_req(slow_call_path, i_o());
5216     result_memory-&gt;init_req(slow_call_path, memory(adr_type));
5217   }
5218 
5219   // Remove unused edges.
5220   for (uint i = 1; i &lt; result_region-&gt;req(); i++) {
5221     if (result_region-&gt;in(i) == NULL)
5222       result_region-&gt;init_req(i, top());
5223   }
5224 
5225   // Finished; return the combined state.
5226   set_control( _gvn.transform(result_region));
5227   set_i_o(     _gvn.transform(result_i_o)    );
5228   set_memory(  _gvn.transform(result_memory), adr_type );
5229 
5230   // The memory edges above are precise in order to model effects around
5231   // array copies accurately to allow value numbering of field loads around
5232   // arraycopy.  Such field loads, both before and after, are common in Java
5233   // collections and similar classes involving header/array data structures.
5234   //
5235   // But with low number of register or when some registers are used or killed
5236   // by arraycopy calls it causes registers spilling on stack. See 6544710.
5237   // The next memory barrier is added to avoid it. If the arraycopy can be
5238   // optimized away (which it can, sometimes) then we can manually remove
5239   // the membar also.
5240   //
5241   // Do not let reads from the cloned object float above the arraycopy.
5242   if (alloc != NULL) {
5243     // Do not let stores that initialize this object be reordered with
5244     // a subsequent store that would make this object accessible by
5245     // other threads.
5246     // Record what AllocateNode this StoreStore protects so that
5247     // escape analysis can go from the MemBarStoreStoreNode to the
5248     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
5249     // based on the escape status of the AllocateNode.
5250     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
5251   } else if (InsertMemBarAfterArraycopy)
5252     insert_mem_bar(Op_MemBarCPUOrder);
5253 }
5254 
5255 
5256 // Helper function which determines if an arraycopy immediately follows
5257 // an allocation, with no intervening tests or other escapes for the object.
5258 AllocateArrayNode*
5259 LibraryCallKit::tightly_coupled_allocation(Node* ptr,
5260                                            RegionNode* slow_region) {
5261   if (stopped())             return NULL;  // no fast path
5262   if (C-&gt;AliasLevel() == 0)  return NULL;  // no MergeMems around
5263 
5264   AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr, &amp;_gvn);
5265   if (alloc == NULL)  return NULL;
5266 
5267   Node* rawmem = memory(Compile::AliasIdxRaw);
5268   // Is the allocation's memory state untouched?
5269   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0)-&gt;is_Initialize())) {
5270     // Bail out if there have been raw-memory effects since the allocation.
5271     // (Example:  There might have been a call or safepoint.)
5272     return NULL;
5273   }
5274   rawmem = rawmem-&gt;in(0)-&gt;as_Initialize()-&gt;memory(Compile::AliasIdxRaw);
5275   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0) == alloc)) {
5276     return NULL;
5277   }
5278 
5279   // There must be no unexpected observers of this allocation.
5280   for (DUIterator_Fast imax, i = ptr-&gt;fast_outs(imax); i &lt; imax; i++) {
5281     Node* obs = ptr-&gt;fast_out(i);
5282     if (obs != this-&gt;map()) {
5283       return NULL;
5284     }
5285   }
5286 
5287   // This arraycopy must unconditionally follow the allocation of the ptr.
5288   Node* alloc_ctl = ptr-&gt;in(0);
5289   assert(just_allocated_object(alloc_ctl) == ptr, "most recent allo");
5290 
5291   Node* ctl = control();
5292   while (ctl != alloc_ctl) {
5293     // There may be guards which feed into the slow_region.
5294     // Any other control flow means that we might not get a chance
5295     // to finish initializing the allocated object.
5296     if ((ctl-&gt;is_IfFalse() || ctl-&gt;is_IfTrue()) &amp;&amp; ctl-&gt;in(0)-&gt;is_If()) {
5297       IfNode* iff = ctl-&gt;in(0)-&gt;as_If();
5298       Node* not_ctl = iff-&gt;proj_out(1 - ctl-&gt;as_Proj()-&gt;_con);
5299       assert(not_ctl != NULL &amp;&amp; not_ctl != ctl, "found alternate");
5300       if (slow_region != NULL &amp;&amp; slow_region-&gt;find_edge(not_ctl) &gt;= 1) {
5301         ctl = iff-&gt;in(0);       // This test feeds the known slow_region.
5302         continue;
5303       }
5304       // One more try:  Various low-level checks bottom out in
5305       // uncommon traps.  If the debug-info of the trap omits
5306       // any reference to the allocation, as we've already
5307       // observed, then there can be no objection to the trap.
5308       bool found_trap = false;
5309       for (DUIterator_Fast jmax, j = not_ctl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
5310         Node* obs = not_ctl-&gt;fast_out(j);
5311         if (obs-&gt;in(0) == not_ctl &amp;&amp; obs-&gt;is_Call() &amp;&amp;
5312             (obs-&gt;as_Call()-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point())) {
5313           found_trap = true; break;
5314         }
5315       }
5316       if (found_trap) {
5317         ctl = iff-&gt;in(0);       // This test feeds a harmless uncommon trap.
5318         continue;
5319       }
5320     }
5321     return NULL;
5322   }
5323 
5324   // If we get this far, we have an allocation which immediately
5325   // precedes the arraycopy, and we can take over zeroing the new object.
5326   // The arraycopy will finish the initialization, and provide
5327   // a new control state to which we will anchor the destination pointer.
5328 
5329   return alloc;
5330 }
5331 
5332 // Helper for initialization of arrays, creating a ClearArray.
5333 // It writes zero bits in [start..end), within the body of an array object.
5334 // The memory effects are all chained onto the 'adr_type' alias category.
5335 //
5336 // Since the object is otherwise uninitialized, we are free
5337 // to put a little "slop" around the edges of the cleared area,
5338 // as long as it does not go back into the array's header,
5339 // or beyond the array end within the heap.
5340 //
5341 // The lower edge can be rounded down to the nearest jint and the
5342 // upper edge can be rounded up to the nearest MinObjAlignmentInBytes.
5343 //
5344 // Arguments:
5345 //   adr_type           memory slice where writes are generated
5346 //   dest               oop of the destination array
5347 //   basic_elem_type    element type of the destination
5348 //   slice_idx          array index of first element to store
5349 //   slice_len          number of elements to store (or NULL)
5350 //   dest_size          total size in bytes of the array object
5351 //
5352 // Exactly one of slice_len or dest_size must be non-NULL.
5353 // If dest_size is non-NULL, zeroing extends to the end of the object.
5354 // If slice_len is non-NULL, the slice_idx value must be a constant.
5355 void
5356 LibraryCallKit::generate_clear_array(const TypePtr* adr_type,
5357                                      Node* dest,
5358                                      BasicType basic_elem_type,
5359                                      Node* slice_idx,
5360                                      Node* slice_len,
5361                                      Node* dest_size) {
5362   // one or the other but not both of slice_len and dest_size:
5363   assert((slice_len != NULL? 1: 0) + (dest_size != NULL? 1: 0) == 1, "");
5364   if (slice_len == NULL)  slice_len = top();
5365   if (dest_size == NULL)  dest_size = top();
5366 
5367   // operate on this memory slice:
5368   Node* mem = memory(adr_type); // memory slice to operate on
5369 
5370   // scaling and rounding of indexes:
5371   int scale = exact_log2(type2aelembytes(basic_elem_type));
5372   int abase = arrayOopDesc::base_offset_in_bytes(basic_elem_type);
5373   int clear_low = (-1 &lt;&lt; scale) &amp; (BytesPerInt  - 1);
5374   int bump_bit  = (-1 &lt;&lt; scale) &amp; BytesPerInt;
5375 
5376   // determine constant starts and ends
5377   const intptr_t BIG_NEG = -128;
5378   assert(BIG_NEG + 2*abase &lt; 0, "neg enough");
5379   intptr_t slice_idx_con = (intptr_t) find_int_con(slice_idx, BIG_NEG);
5380   intptr_t slice_len_con = (intptr_t) find_int_con(slice_len, BIG_NEG);
5381   if (slice_len_con == 0) {
5382     return;                     // nothing to do here
5383   }
5384   intptr_t start_con = (abase + (slice_idx_con &lt;&lt; scale)) &amp; ~clear_low;
5385   intptr_t end_con   = find_intptr_t_con(dest_size, -1);
5386   if (slice_idx_con &gt;= 0 &amp;&amp; slice_len_con &gt;= 0) {
5387     assert(end_con &lt; 0, "not two cons");
5388     end_con = round_to(abase + ((slice_idx_con + slice_len_con) &lt;&lt; scale),
5389                        BytesPerLong);
5390   }
5391 
5392   if (start_con &gt;= 0 &amp;&amp; end_con &gt;= 0) {
5393     // Constant start and end.  Simple.
5394     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5395                                        start_con, end_con, &amp;_gvn);
5396   } else if (start_con &gt;= 0 &amp;&amp; dest_size != top()) {
5397     // Constant start, pre-rounded end after the tail of the array.
5398     Node* end = dest_size;
5399     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5400                                        start_con, end, &amp;_gvn);
5401   } else if (start_con &gt;= 0 &amp;&amp; slice_len != top()) {
5402     // Constant start, non-constant end.  End needs rounding up.
5403     // End offset = round_up(abase + ((slice_idx_con + slice_len) &lt;&lt; scale), 8)
5404     intptr_t end_base  = abase + (slice_idx_con &lt;&lt; scale);
5405     int      end_round = (-1 &lt;&lt; scale) &amp; (BytesPerLong  - 1);
5406     Node*    end       = ConvI2X(slice_len);
5407     if (scale != 0)
5408       end = _gvn.transform(new(C) LShiftXNode(end, intcon(scale) ));
5409     end_base += end_round;
5410     end = _gvn.transform(new(C) AddXNode(end, MakeConX(end_base)));
5411     end = _gvn.transform(new(C) AndXNode(end, MakeConX(~end_round)));
5412     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5413                                        start_con, end, &amp;_gvn);
5414   } else if (start_con &lt; 0 &amp;&amp; dest_size != top()) {
5415     // Non-constant start, pre-rounded end after the tail of the array.
5416     // This is almost certainly a "round-to-end" operation.
5417     Node* start = slice_idx;
5418     start = ConvI2X(start);
5419     if (scale != 0)
5420       start = _gvn.transform(new(C) LShiftXNode( start, intcon(scale) ));
5421     start = _gvn.transform(new(C) AddXNode(start, MakeConX(abase)));
5422     if ((bump_bit | clear_low) != 0) {
5423       int to_clear = (bump_bit | clear_low);
5424       // Align up mod 8, then store a jint zero unconditionally
5425       // just before the mod-8 boundary.
5426       if (((abase + bump_bit) &amp; ~to_clear) - bump_bit
5427           &lt; arrayOopDesc::length_offset_in_bytes() + BytesPerInt) {
5428         bump_bit = 0;
5429         assert((abase &amp; to_clear) == 0, "array base must be long-aligned");
5430       } else {
5431         // Bump 'start' up to (or past) the next jint boundary:
5432         start = _gvn.transform(new(C) AddXNode(start, MakeConX(bump_bit)));
5433         assert((abase &amp; clear_low) == 0, "array base must be int-aligned");
5434       }
5435       // Round bumped 'start' down to jlong boundary in body of array.
5436       start = _gvn.transform(new(C) AndXNode(start, MakeConX(~to_clear)));
5437       if (bump_bit != 0) {
5438         // Store a zero to the immediately preceding jint:
5439         Node* x1 = _gvn.transform(new(C) AddXNode(start, MakeConX(-bump_bit)));
5440         Node* p1 = basic_plus_adr(dest, x1);
5441         mem = StoreNode::make(_gvn, control(), mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);
5442         mem = _gvn.transform(mem);
5443       }
5444     }
5445     Node* end = dest_size; // pre-rounded
5446     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5447                                        start, end, &amp;_gvn);
5448   } else {
5449     // Non-constant start, unrounded non-constant end.
5450     // (Nobody zeroes a random midsection of an array using this routine.)
5451     ShouldNotReachHere();       // fix caller
5452   }
5453 
5454   // Done.
5455   set_memory(mem, adr_type);
5456 }
5457 
5458 
5459 bool
5460 LibraryCallKit::generate_block_arraycopy(const TypePtr* adr_type,
5461                                          BasicType basic_elem_type,
5462                                          AllocateNode* alloc,
5463                                          Node* src,  Node* src_offset,
5464                                          Node* dest, Node* dest_offset,
5465                                          Node* dest_size, bool dest_uninitialized) {
5466   // See if there is an advantage from block transfer.
5467   int scale = exact_log2(type2aelembytes(basic_elem_type));
5468   if (scale &gt;= LogBytesPerLong)
5469     return false;               // it is already a block transfer
5470 
5471   // Look at the alignment of the starting offsets.
5472   int abase = arrayOopDesc::base_offset_in_bytes(basic_elem_type);
5473 
5474   intptr_t src_off_con  = (intptr_t) find_int_con(src_offset, -1);
5475   intptr_t dest_off_con = (intptr_t) find_int_con(dest_offset, -1);
5476   if (src_off_con &lt; 0 || dest_off_con &lt; 0)
5477     // At present, we can only understand constants.
5478     return false;
5479 
5480   intptr_t src_off  = abase + (src_off_con  &lt;&lt; scale);
5481   intptr_t dest_off = abase + (dest_off_con &lt;&lt; scale);
5482 
5483   if (((src_off | dest_off) &amp; (BytesPerLong-1)) != 0) {
5484     // Non-aligned; too bad.
5485     // One more chance:  Pick off an initial 32-bit word.
5486     // This is a common case, since abase can be odd mod 8.
5487     if (((src_off | dest_off) &amp; (BytesPerLong-1)) == BytesPerInt &amp;&amp;
5488         ((src_off ^ dest_off) &amp; (BytesPerLong-1)) == 0) {
5489       Node* sptr = basic_plus_adr(src,  src_off);
5490       Node* dptr = basic_plus_adr(dest, dest_off);
5491       Node* sval = make_load(control(), sptr, TypeInt::INT, T_INT, adr_type, MemNode::unordered);
5492       store_to_memory(control(), dptr, sval, T_INT, adr_type, MemNode::unordered);
5493       src_off += BytesPerInt;
5494       dest_off += BytesPerInt;
5495     } else {
5496       return false;
5497     }
5498   }
5499   assert(src_off % BytesPerLong == 0, "");
5500   assert(dest_off % BytesPerLong == 0, "");
5501 
5502   // Do this copy by giant steps.
5503   Node* sptr  = basic_plus_adr(src,  src_off);
5504   Node* dptr  = basic_plus_adr(dest, dest_off);
5505   Node* countx = dest_size;
5506   countx = _gvn.transform(new (C) SubXNode(countx, MakeConX(dest_off)));
5507   countx = _gvn.transform(new (C) URShiftXNode(countx, intcon(LogBytesPerLong)));
5508 
5509   bool disjoint_bases = true;   // since alloc != NULL
5510   generate_unchecked_arraycopy(adr_type, T_LONG, disjoint_bases,
5511                                sptr, NULL, dptr, NULL, countx, dest_uninitialized);
5512 
5513   return true;
5514 }
5515 
5516 
5517 // Helper function; generates code for the slow case.
5518 // We make a call to a runtime method which emulates the native method,
5519 // but without the native wrapper overhead.
5520 void
5521 LibraryCallKit::generate_slow_arraycopy(const TypePtr* adr_type,
5522                                         Node* src,  Node* src_offset,
5523                                         Node* dest, Node* dest_offset,
5524                                         Node* copy_length, bool dest_uninitialized) {
5525   assert(!dest_uninitialized, "Invariant");
5526   Node* call = make_runtime_call(RC_NO_LEAF | RC_UNCOMMON,
5527                                  OptoRuntime::slow_arraycopy_Type(),
5528                                  OptoRuntime::slow_arraycopy_Java(),
5529                                  "slow_arraycopy", adr_type,
5530                                  src, src_offset, dest, dest_offset,
5531                                  copy_length);
5532 
5533   // Handle exceptions thrown by this fellow:
5534   make_slow_call_ex(call, env()-&gt;Throwable_klass(), false);
5535 }
5536 
5537 // Helper function; generates code for cases requiring runtime checks.
5538 Node*
5539 LibraryCallKit::generate_checkcast_arraycopy(const TypePtr* adr_type,
5540                                              Node* dest_elem_klass,
5541                                              Node* src,  Node* src_offset,
5542                                              Node* dest, Node* dest_offset,
5543                                              Node* copy_length, bool dest_uninitialized) {
5544   if (stopped())  return NULL;
5545 
5546   address copyfunc_addr = StubRoutines::checkcast_arraycopy(dest_uninitialized);
5547   if (copyfunc_addr == NULL) { // Stub was not generated, go slow path.
5548     return NULL;
5549   }
5550 
5551   // Pick out the parameters required to perform a store-check
5552   // for the target array.  This is an optimistic check.  It will
5553   // look in each non-null element's class, at the desired klass's
5554   // super_check_offset, for the desired klass.
5555   int sco_offset = in_bytes(Klass::super_check_offset_offset());
5556   Node* p3 = basic_plus_adr(dest_elem_klass, sco_offset);
5557   Node* n3 = new(C) LoadINode(NULL, memory(p3), p3, _gvn.type(p3)-&gt;is_ptr(), TypeInt::INT, MemNode::unordered);
5558   Node* check_offset = ConvI2X(_gvn.transform(n3));
5559   Node* check_value  = dest_elem_klass;
5560 
5561   Node* src_start  = array_element_address(src,  src_offset,  T_OBJECT);
5562   Node* dest_start = array_element_address(dest, dest_offset, T_OBJECT);
5563 
5564   // (We know the arrays are never conjoint, because their types differ.)
5565   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5566                                  OptoRuntime::checkcast_arraycopy_Type(),
5567                                  copyfunc_addr, "checkcast_arraycopy", adr_type,
5568                                  // five arguments, of which two are
5569                                  // intptr_t (jlong in LP64)
5570                                  src_start, dest_start,
5571                                  copy_length XTOP,
5572                                  check_offset XTOP,
5573                                  check_value);
5574 
5575   return _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5576 }
5577 
5578 
5579 // Helper function; generates code for cases requiring runtime checks.
5580 Node*
5581 LibraryCallKit::generate_generic_arraycopy(const TypePtr* adr_type,
5582                                            Node* src,  Node* src_offset,
5583                                            Node* dest, Node* dest_offset,
5584                                            Node* copy_length, bool dest_uninitialized) {
5585   assert(!dest_uninitialized, "Invariant");
5586   if (stopped())  return NULL;
5587   address copyfunc_addr = StubRoutines::generic_arraycopy();
5588   if (copyfunc_addr == NULL) { // Stub was not generated, go slow path.
5589     return NULL;
5590   }
5591 
5592   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5593                     OptoRuntime::generic_arraycopy_Type(),
5594                     copyfunc_addr, "generic_arraycopy", adr_type,
5595                     src, src_offset, dest, dest_offset, copy_length);
5596 
5597   return _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5598 }
5599 
5600 // Helper function; generates the fast out-of-line call to an arraycopy stub.
5601 void
5602 LibraryCallKit::generate_unchecked_arraycopy(const TypePtr* adr_type,
5603                                              BasicType basic_elem_type,
5604                                              bool disjoint_bases,
5605                                              Node* src,  Node* src_offset,
5606                                              Node* dest, Node* dest_offset,
5607                                              Node* copy_length, bool dest_uninitialized) {
5608   if (stopped())  return;               // nothing to do
5609 
5610   Node* src_start  = src;
5611   Node* dest_start = dest;
5612   if (src_offset != NULL || dest_offset != NULL) {
5613     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5614     src_start  = array_element_address(src,  src_offset,  basic_elem_type);
5615     dest_start = array_element_address(dest, dest_offset, basic_elem_type);
5616   }
5617 
5618   // Figure out which arraycopy runtime method to call.
5619   const char* copyfunc_name = "arraycopy";
5620   address     copyfunc_addr =
5621       basictype2arraycopy(basic_elem_type, src_offset, dest_offset,
5622                           disjoint_bases, copyfunc_name, dest_uninitialized);
5623 
5624   // Call it.  Note that the count_ix value is not scaled to a byte-size.
5625   make_runtime_call(RC_LEAF|RC_NO_FP,
5626                     OptoRuntime::fast_arraycopy_Type(),
5627                     copyfunc_addr, copyfunc_name, adr_type,
5628                     src_start, dest_start, copy_length XTOP);
5629 }
5630 
5631 //-------------inline_encodeISOArray-----------------------------------
5632 // encode char[] to byte[] in ISO_8859_1
5633 bool LibraryCallKit::inline_encodeISOArray() {
5634   assert(callee()-&gt;signature()-&gt;size() == 5, "encodeISOArray has 5 parameters");
5635   // no receiver since it is static method
5636   Node *src         = argument(0);
5637   Node *src_offset  = argument(1);
5638   Node *dst         = argument(2);
5639   Node *dst_offset  = argument(3);
5640   Node *length      = argument(4);
5641 
5642   const Type* src_type = src-&gt;Value(&amp;_gvn);
5643   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
5644   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5645   const TypeAryPtr* top_dest = dst_type-&gt;isa_aryptr();
5646   if (top_src  == NULL || top_src-&gt;klass()  == NULL ||
5647       top_dest == NULL || top_dest-&gt;klass() == NULL) {
5648     // failed array check
5649     return false;
5650   }
5651 
5652   // Figure out the size and type of the elements we will be copying.
5653   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5654   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5655   if (src_elem != T_CHAR || dst_elem != T_BYTE) {
5656     return false;
5657   }
5658   Node* src_start = array_element_address(src, src_offset, src_elem);
5659   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
5660   // 'src_start' points to src array + scaled offset
5661   // 'dst_start' points to dst array + scaled offset
5662 
5663   const TypeAryPtr* mtype = TypeAryPtr::BYTES;
5664   Node* enc = new (C) EncodeISOArrayNode(control(), memory(mtype), src_start, dst_start, length);
5665   enc = _gvn.transform(enc);
5666   Node* res_mem = _gvn.transform(new (C) SCMemProjNode(enc));
5667   set_memory(res_mem, mtype);
5668   set_result(enc);
5669   return true;
5670 }
5671 
5672 /**
5673  * Calculate CRC32 for byte.
5674  * int java.util.zip.CRC32.update(int crc, int b)
5675  */
5676 bool LibraryCallKit::inline_updateCRC32() {
5677   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5678   assert(callee()-&gt;signature()-&gt;size() == 2, "update has 2 parameters");
5679   // no receiver since it is static method
5680   Node* crc  = argument(0); // type: int
5681   Node* b    = argument(1); // type: int
5682 
5683   /*
5684    *    int c = ~ crc;
5685    *    b = timesXtoThe32[(b ^ c) &amp; 0xFF];
5686    *    b = b ^ (c &gt;&gt;&gt; 8);
5687    *    crc = ~b;
5688    */
5689 
5690   Node* M1 = intcon(-1);
5691   crc = _gvn.transform(new (C) XorINode(crc, M1));
5692   Node* result = _gvn.transform(new (C) XorINode(crc, b));
5693   result = _gvn.transform(new (C) AndINode(result, intcon(0xFF)));
5694 
5695   Node* base = makecon(TypeRawPtr::make(StubRoutines::crc_table_addr()));
5696   Node* offset = _gvn.transform(new (C) LShiftINode(result, intcon(0x2)));
5697   Node* adr = basic_plus_adr(top(), base, ConvI2X(offset));
5698   result = make_load(control(), adr, TypeInt::INT, T_INT, MemNode::unordered);
5699 
5700   crc = _gvn.transform(new (C) URShiftINode(crc, intcon(8)));
5701   result = _gvn.transform(new (C) XorINode(crc, result));
5702   result = _gvn.transform(new (C) XorINode(result, M1));
5703   set_result(result);
5704   return true;
5705 }
5706 
5707 /**
5708  * Calculate CRC32 for byte[] array.
5709  * int java.util.zip.CRC32.updateBytes(int crc, byte[] buf, int off, int len)
5710  */
5711 bool LibraryCallKit::inline_updateBytesCRC32() {
5712   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5713   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5714   // no receiver since it is static method
5715   Node* crc     = argument(0); // type: int
5716   Node* src     = argument(1); // type: oop
5717   Node* offset  = argument(2); // type: int
5718   Node* length  = argument(3); // type: int
5719 
5720   const Type* src_type = src-&gt;Value(&amp;_gvn);
5721   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5722   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5723     // failed array check
5724     return false;
5725   }
5726 
5727   // Figure out the size and type of the elements we will be copying.
5728   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5729   if (src_elem != T_BYTE) {
5730     return false;
5731   }
5732 
5733   // 'src_start' points to src array + scaled offset
5734   Node* src_start = array_element_address(src, offset, src_elem);
5735 
5736   // We assume that range check is done by caller.
5737   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5738 
5739   // Call the stub.
5740   address stubAddr = StubRoutines::updateBytesCRC32();
5741   const char *stubName = "updateBytesCRC32";
5742 
5743   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5744                                  stubAddr, stubName, TypePtr::BOTTOM,
5745                                  crc, src_start, length);
5746   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5747   set_result(result);
5748   return true;
5749 }
5750 
5751 /**
5752  * Calculate CRC32 for ByteBuffer.
5753  * int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)
5754  */
5755 bool LibraryCallKit::inline_updateByteBufferCRC32() {
5756   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5757   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5758   // no receiver since it is static method
5759   Node* crc     = argument(0); // type: int
5760   Node* src     = argument(1); // type: long
5761   Node* offset  = argument(3); // type: int
5762   Node* length  = argument(4); // type: int
5763 
5764   src = ConvL2X(src);  // adjust Java long to machine word
5765   Node* base = _gvn.transform(new (C) CastX2PNode(src));
5766   offset = ConvI2X(offset);
5767 
5768   // 'src_start' points to src array + scaled offset
5769   Node* src_start = basic_plus_adr(top(), base, offset);
5770 
5771   // Call the stub.
5772   address stubAddr = StubRoutines::updateBytesCRC32();
5773   const char *stubName = "updateBytesCRC32";
5774 
5775   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5776                                  stubAddr, stubName, TypePtr::BOTTOM,
5777                                  crc, src_start, length);
5778   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5779   set_result(result);
5780   return true;
5781 }
5782 
5783 //----------------------------inline_reference_get----------------------------
5784 // public T java.lang.ref.Reference.get();
5785 bool LibraryCallKit::inline_reference_get() {
5786   const int referent_offset = java_lang_ref_Reference::referent_offset;
5787   guarantee(referent_offset &gt; 0, "should have already been set");
5788 
5789   // Get the argument:
5790   Node* reference_obj = null_check_receiver();
5791   if (stopped()) return true;
5792 
5793   Node* adr = basic_plus_adr(reference_obj, reference_obj, referent_offset);
5794 
5795   ciInstanceKlass* klass = env()-&gt;Object_klass();
5796   const TypeOopPtr* object_type = TypeOopPtr::make_from_klass(klass);
5797 
5798   Node* no_ctrl = NULL;
5799   Node* result = make_load(no_ctrl, adr, object_type, T_OBJECT, MemNode::unordered);
5800 
5801   // Use the pre-barrier to record the value in the referent field
5802   pre_barrier(false /* do_load */,
5803               control(),
5804               NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
5805               result /* pre_val */,
5806               T_OBJECT);
5807 
5808   // Add memory barrier to prevent commoning reads from this field
5809   // across safepoint since GC can change its value.
5810   insert_mem_bar(Op_MemBarCPUOrder);
5811 
5812   set_result(result);
5813   return true;
5814 }
5815 
5816 
5817 Node * LibraryCallKit::load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString,
5818                                               bool is_exact=true, bool is_static=false) {
5819 
5820   const TypeInstPtr* tinst = _gvn.type(fromObj)-&gt;isa_instptr();
5821   assert(tinst != NULL, "obj is null");
5822   assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
5823   assert(!is_exact || tinst-&gt;klass_is_exact(), "klass not exact");
5824 
5825   ciField* field = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;get_field_by_name(ciSymbol::make(fieldName),
5826                                                                           ciSymbol::make(fieldTypeString),
5827                                                                           is_static);
5828   if (field == NULL) return (Node *) NULL;
5829   assert (field != NULL, "undefined field");
5830 
5831   // Next code  copied from Parse::do_get_xxx():
5832 
5833   // Compute address and memory type.
5834   int offset  = field-&gt;offset_in_bytes();
5835   bool is_vol = field-&gt;is_volatile();
5836   ciType* field_klass = field-&gt;type();
5837   assert(field_klass-&gt;is_loaded(), "should be loaded");
5838   const TypePtr* adr_type = C-&gt;alias_type(field)-&gt;adr_type();
5839   Node *adr = basic_plus_adr(fromObj, fromObj, offset);
5840   BasicType bt = field-&gt;layout_type();
5841 
5842   // Build the resultant type of the load
5843   const Type *type = TypeOopPtr::make_from_klass(field_klass-&gt;as_klass());
5844 
5845   // Build the load.
5846   Node* loadedField = make_load(NULL, adr, type, bt, adr_type, MemNode::unordered, is_vol);
5847   return loadedField;
5848 }
5849 
5850 
5851 //------------------------------inline_aescrypt_Block-----------------------
5852 bool LibraryCallKit::inline_aescrypt_Block(vmIntrinsics::ID id) {
5853   address stubAddr;
5854   const char *stubName;
5855   assert(UseAES, "need AES instruction support");
5856 
5857   switch(id) {
5858   case vmIntrinsics::_aescrypt_encryptBlock:
5859     stubAddr = StubRoutines::aescrypt_encryptBlock();
5860     stubName = "aescrypt_encryptBlock";
5861     break;
5862   case vmIntrinsics::_aescrypt_decryptBlock:
5863     stubAddr = StubRoutines::aescrypt_decryptBlock();
5864     stubName = "aescrypt_decryptBlock";
5865     break;
5866   }
5867   if (stubAddr == NULL) return false;
5868 
5869   Node* aescrypt_object = argument(0);
5870   Node* src             = argument(1);
5871   Node* src_offset      = argument(2);
5872   Node* dest            = argument(3);
5873   Node* dest_offset     = argument(4);
5874 
5875   // (1) src and dest are arrays.
5876   const Type* src_type = src-&gt;Value(&amp;_gvn);
5877   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5878   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5879   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5880   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5881 
5882   // for the quick and dirty code we will skip all the checks.
5883   // we are just trying to get the call to be generated.
5884   Node* src_start  = src;
5885   Node* dest_start = dest;
5886   if (src_offset != NULL || dest_offset != NULL) {
5887     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5888     src_start  = array_element_address(src,  src_offset,  T_BYTE);
5889     dest_start = array_element_address(dest, dest_offset, T_BYTE);
5890   }
5891 
5892   // now need to get the start of its expanded key array
5893   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
5894   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
5895   if (k_start == NULL) return false;
5896 
5897   if (Matcher::pass_original_key_for_aes()) {
5898     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
5899     // compatibility issues between Java key expansion and SPARC crypto instructions
5900     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
5901     if (original_k_start == NULL) return false;
5902 
5903     // Call the stub.
5904     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
5905                       stubAddr, stubName, TypePtr::BOTTOM,
5906                       src_start, dest_start, k_start, original_k_start);
5907   } else {
5908     // Call the stub.
5909     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
5910                       stubAddr, stubName, TypePtr::BOTTOM,
5911                       src_start, dest_start, k_start);
5912   }
5913 
5914   return true;
5915 }
5916 
5917 //------------------------------inline_cipherBlockChaining_AESCrypt-----------------------
5918 bool LibraryCallKit::inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id) {
5919   address stubAddr;
5920   const char *stubName;
5921 
5922   assert(UseAES, "need AES instruction support");
5923 
5924   switch(id) {
5925   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
5926     stubAddr = StubRoutines::cipherBlockChaining_encryptAESCrypt();
5927     stubName = "cipherBlockChaining_encryptAESCrypt";
5928     break;
5929   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
5930     stubAddr = StubRoutines::cipherBlockChaining_decryptAESCrypt();
5931     stubName = "cipherBlockChaining_decryptAESCrypt";
5932     break;
5933   }
5934   if (stubAddr == NULL) return false;
5935 
5936   Node* cipherBlockChaining_object = argument(0);
5937   Node* src                        = argument(1);
5938   Node* src_offset                 = argument(2);
5939   Node* len                        = argument(3);
5940   Node* dest                       = argument(4);
5941   Node* dest_offset                = argument(5);
5942 
5943   // (1) src and dest are arrays.
5944   const Type* src_type = src-&gt;Value(&amp;_gvn);
5945   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5946   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5947   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5948   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL
5949           &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5950 
5951   // checks are the responsibility of the caller
5952   Node* src_start  = src;
5953   Node* dest_start = dest;
5954   if (src_offset != NULL || dest_offset != NULL) {
5955     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5956     src_start  = array_element_address(src,  src_offset,  T_BYTE);
5957     dest_start = array_element_address(dest, dest_offset, T_BYTE);
5958   }
5959 
5960   // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
5961   // (because of the predicated logic executed earlier).
5962   // so we cast it here safely.
5963   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
5964 
5965   Node* embeddedCipherObj = load_field_from_object(cipherBlockChaining_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
5966   if (embeddedCipherObj == NULL) return false;
5967 
5968   // cast it to what we know it will be at runtime
5969   const TypeInstPtr* tinst = _gvn.type(cipherBlockChaining_object)-&gt;isa_instptr();
5970   assert(tinst != NULL, "CBC obj is null");
5971   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBC obj is not loaded");
5972   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
5973   if (!klass_AESCrypt-&gt;is_loaded()) return false;
5974 
5975   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
5976   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
5977   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
5978   Node* aescrypt_object = new(C) CheckCastPPNode(control(), embeddedCipherObj, xtype);
5979   aescrypt_object = _gvn.transform(aescrypt_object);
5980 
5981   // we need to get the start of the aescrypt_object's expanded key array
5982   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
5983   if (k_start == NULL) return false;
5984 
5985   // similarly, get the start address of the r vector
5986   Node* objRvec = load_field_from_object(cipherBlockChaining_object, "r", "[B", /*is_exact*/ false);
5987   if (objRvec == NULL) return false;
5988   Node* r_start = array_element_address(objRvec, intcon(0), T_BYTE);
5989 
5990   Node* cbcCrypt;
5991   if (Matcher::pass_original_key_for_aes()) {
5992     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
5993     // compatibility issues between Java key expansion and SPARC crypto instructions
5994     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
5995     if (original_k_start == NULL) return false;
5996 
5997     // Call the stub, passing src_start, dest_start, k_start, r_start, src_len and original_k_start
5998     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
5999                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6000                                  stubAddr, stubName, TypePtr::BOTTOM,
6001                                  src_start, dest_start, k_start, r_start, len, original_k_start);
6002   } else {
6003     // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
6004     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6005                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6006                                  stubAddr, stubName, TypePtr::BOTTOM,
6007                                  src_start, dest_start, k_start, r_start, len);
6008   }
6009 
6010   // return cipher length (int)
6011   Node* retvalue = _gvn.transform(new (C) ProjNode(cbcCrypt, TypeFunc::Parms));
6012   set_result(retvalue);
6013   return true;
6014 }
6015 
6016 //------------------------------get_key_start_from_aescrypt_object-----------------------
6017 Node * LibraryCallKit::get_key_start_from_aescrypt_object(Node *aescrypt_object) {
6018   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "K", "[I", /*is_exact*/ false);
6019   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6020   if (objAESCryptKey == NULL) return (Node *) NULL;
6021 
6022   // now have the array, need to get the start address of the K array
6023   Node* k_start = array_element_address(objAESCryptKey, intcon(0), T_INT);
6024   return k_start;
6025 }
6026 
6027 //------------------------------get_original_key_start_from_aescrypt_object-----------------------
6028 Node * LibraryCallKit::get_original_key_start_from_aescrypt_object(Node *aescrypt_object) {
6029   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "lastKey", "[B", /*is_exact*/ false);
6030   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6031   if (objAESCryptKey == NULL) return (Node *) NULL;
6032 
6033   // now have the array, need to get the start address of the lastKey array
6034   Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);
6035   return original_k_start;
6036 }
6037 
6038 //----------------------------inline_cipherBlockChaining_AESCrypt_predicate----------------------------
6039 // Return node representing slow path of predicate check.
6040 // the pseudo code we want to emulate with this predicate is:
6041 // for encryption:
6042 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
6043 // for decryption:
6044 //    if ((embeddedCipherObj instanceof AESCrypt) &amp;&amp; (cipher!=plain)) do_intrinsic, else do_javapath
6045 //    note cipher==plain is more conservative than the original java code but that's OK
6046 //
6047 Node* LibraryCallKit::inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting) {
6048   // First, check receiver for NULL since it is virtual method.
6049   Node* objCBC = argument(0);
6050   objCBC = null_check(objCBC);
6051 
6052   if (stopped()) return NULL; // Always NULL
6053 
6054   // Load embeddedCipher field of CipherBlockChaining object.
6055   Node* embeddedCipherObj = load_field_from_object(objCBC, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6056 
6057   // get AESCrypt klass for instanceOf check
6058   // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
6059   // will have same classloader as CipherBlockChaining object
6060   const TypeInstPtr* tinst = _gvn.type(objCBC)-&gt;isa_instptr();
6061   assert(tinst != NULL, "CBCobj is null");
6062   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBCobj is not loaded");
6063 
6064   // we want to do an instanceof comparison against the AESCrypt class
6065   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6066   if (!klass_AESCrypt-&gt;is_loaded()) {
6067     // if AESCrypt is not even loaded, we never take the intrinsic fast path
6068     Node* ctrl = control();
6069     set_control(top()); // no regular fast path
6070     return ctrl;
6071   }
6072   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6073 
6074   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
6075   Node* cmp_instof  = _gvn.transform(new (C) CmpINode(instof, intcon(1)));
6076   Node* bool_instof  = _gvn.transform(new (C) BoolNode(cmp_instof, BoolTest::ne));
6077 
6078   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6079 
6080   // for encryption, we are done
6081   if (!decrypting)
6082     return instof_false;  // even if it is NULL
6083 
6084   // for decryption, we need to add a further check to avoid
6085   // taking the intrinsic path when cipher and plain are the same
6086   // see the original java code for why.
6087   RegionNode* region = new(C) RegionNode(3);
6088   region-&gt;init_req(1, instof_false);
6089   Node* src = argument(1);
6090   Node* dest = argument(4);
6091   Node* cmp_src_dest = _gvn.transform(new (C) CmpPNode(src, dest));
6092   Node* bool_src_dest = _gvn.transform(new (C) BoolNode(cmp_src_dest, BoolTest::eq));
6093   Node* src_dest_conjoint = generate_guard(bool_src_dest, NULL, PROB_MIN);
6094   region-&gt;init_req(2, src_dest_conjoint);
6095 
6096   record_for_igvn(region);
6097   return _gvn.transform(region);
6098 }
</pre></body></html>

<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/opto/library_call.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1999, 2013, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "classfile/vmSymbols.hpp"
  28 #include "compiler/compileBroker.hpp"
  29 #include "compiler/compileLog.hpp"
  30 #include "oops/objArrayKlass.hpp"
  31 #include "opto/addnode.hpp"
  32 #include "opto/callGenerator.hpp"
  33 #include "opto/castnode.hpp"
  34 #include "opto/cfgnode.hpp"
  35 #include "opto/convertnode.hpp"
  36 #include "opto/countbitsnode.hpp"
  37 #include "opto/intrinsicnode.hpp"
  38 #include "opto/idealKit.hpp"
  39 #include "opto/mathexactnode.hpp"
  40 #include "opto/movenode.hpp"
  41 #include "opto/mulnode.hpp"
  42 #include "opto/narrowptrnode.hpp"
  43 #include "opto/parse.hpp"
  44 #include "opto/runtime.hpp"
  45 #include "opto/subnode.hpp"
  46 #include "prims/nativeLookup.hpp"
  47 #include "runtime/sharedRuntime.hpp"
  48 #include "trace/traceMacros.hpp"
  49 
  50 class LibraryIntrinsic : public InlineCallGenerator {
  51   // Extend the set of intrinsics known to the runtime:
  52  public:
  53  private:
  54   bool             _is_virtual;
  55   bool             _is_predicted;
  56   bool             _does_virtual_dispatch;
  57   vmIntrinsics::ID _intrinsic_id;
  58 
  59  public:
  60   LibraryIntrinsic(ciMethod* m, bool is_virtual, bool is_predicted, bool does_virtual_dispatch, vmIntrinsics::ID id)
  61     : InlineCallGenerator(m),
  62       _is_virtual(is_virtual),
  63       _is_predicted(is_predicted),
  64       _does_virtual_dispatch(does_virtual_dispatch),
  65       _intrinsic_id(id)
  66   {
  67   }
  68   virtual bool is_intrinsic() const { return true; }
  69   virtual bool is_virtual()   const { return _is_virtual; }
  70   virtual bool is_predicted()   const { return _is_predicted; }
  71   virtual bool does_virtual_dispatch()   const { return _does_virtual_dispatch; }
  72   virtual JVMState* generate(JVMState* jvms, Parse* parent_parser);
  73   virtual Node* generate_predicate(JVMState* jvms);
  74   vmIntrinsics::ID intrinsic_id() const { return _intrinsic_id; }
  75 };
  76 
  77 
  78 // Local helper class for LibraryIntrinsic:
  79 class LibraryCallKit : public GraphKit {
  80  private:
  81   LibraryIntrinsic* _intrinsic;     // the library intrinsic being called
  82   Node*             _result;        // the result node, if any
  83   int               _reexecute_sp;  // the stack pointer when bytecode needs to be reexecuted
  84 
  85   const TypeOopPtr* sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr = false);
  86 
  87  public:
  88   LibraryCallKit(JVMState* jvms, LibraryIntrinsic* intrinsic)
  89     : GraphKit(jvms),
  90       _intrinsic(intrinsic),
  91       _result(NULL)
  92   {
  93     // Check if this is a root compile.  In that case we don't have a caller.
  94     if (!jvms-&gt;has_method()) {
  95       _reexecute_sp = sp();
  96     } else {
  97       // Find out how many arguments the interpreter needs when deoptimizing
  98       // and save the stack pointer value so it can used by uncommon_trap.
  99       // We find the argument count by looking at the declared signature.
 100       bool ignored_will_link;
 101       ciSignature* declared_signature = NULL;
 102       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 103       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 104       _reexecute_sp = sp() + nargs;  // "push" arguments back on stack
 105     }
 106   }
 107 
 108   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 109 
 110   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 111   int               bci()       const    { return jvms()-&gt;bci(); }
 112   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 113   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 114   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 115 
 116   bool try_to_inline();
 117   Node* try_to_predicate();
 118 
 119   void push_result() {
 120     // Push the result onto the stack.
 121     if (!stopped() &amp;&amp; result() != NULL) {
 122       BasicType bt = result()-&gt;bottom_type()-&gt;basic_type();
 123       push_node(bt, result());
 124     }
 125   }
 126 
 127  private:
 128   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 129     fatal(err_msg_res("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid)));
 130   }
 131 
 132   void  set_result(Node* n) { assert(_result == NULL, "only set once"); _result = n; }
 133   void  set_result(RegionNode* region, PhiNode* value);
 134   Node*     result() { return _result; }
 135 
 136   virtual int reexecute_sp() { return _reexecute_sp; }
 137 
 138   // Helper functions to inline natives
 139   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 140   Node* generate_slow_guard(Node* test, RegionNode* region);
 141   Node* generate_fair_guard(Node* test, RegionNode* region);
 142   Node* generate_negative_guard(Node* index, RegionNode* region,
 143                                 // resulting CastII of index:
 144                                 Node* *pos_index = NULL);
 145   Node* generate_nonpositive_guard(Node* index, bool never_negative,
 146                                    // resulting CastII of index:
 147                                    Node* *pos_index = NULL);
 148   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 149                              Node* array_length,
 150                              RegionNode* region);
 151   Node* generate_current_thread(Node* &amp;tls_output);
 152   address basictype2arraycopy(BasicType t, Node *src_offset, Node *dest_offset,
 153                               bool disjoint_bases, const char* &amp;name, bool dest_uninitialized);
 154   Node* load_mirror_from_klass(Node* klass);
 155   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 156                                       RegionNode* region, int null_path,
 157                                       int offset);
 158   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 159                                RegionNode* region, int null_path) {
 160     int offset = java_lang_Class::klass_offset_in_bytes();
 161     return load_klass_from_mirror_common(mirror, never_see_null,
 162                                          region, null_path,
 163                                          offset);
 164   }
 165   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 166                                      RegionNode* region, int null_path) {
 167     int offset = java_lang_Class::array_klass_offset_in_bytes();
 168     return load_klass_from_mirror_common(mirror, never_see_null,
 169                                          region, null_path,
 170                                          offset);
 171   }
 172   Node* generate_access_flags_guard(Node* kls,
 173                                     int modifier_mask, int modifier_bits,
 174                                     RegionNode* region);
 175   Node* generate_interface_guard(Node* kls, RegionNode* region);
 176   Node* generate_array_guard(Node* kls, RegionNode* region) {
 177     return generate_array_guard_common(kls, region, false, false);
 178   }
 179   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 180     return generate_array_guard_common(kls, region, false, true);
 181   }
 182   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 183     return generate_array_guard_common(kls, region, true, false);
 184   }
 185   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 186     return generate_array_guard_common(kls, region, true, true);
 187   }
 188   Node* generate_array_guard_common(Node* kls, RegionNode* region,
 189                                     bool obj_array, bool not_array);
 190   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 191   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 192                                      bool is_virtual = false, bool is_static = false);
 193   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 194     return generate_method_call(method_id, false, true);
 195   }
 196   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 197     return generate_method_call(method_id, true, false);
 198   }
 199   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static);
 200 
 201   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2);
 202   Node* make_string_method_node(int opcode, Node* str1, Node* str2);
 203   bool inline_string_compareTo();
 204   bool inline_string_indexOf();
 205   Node* string_indexOf(Node* string_object, ciTypeArray* target_array, jint offset, jint cache_i, jint md2_i);
 206   bool inline_string_equals();
 207   Node* round_double_node(Node* n);
 208   bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);
 209   bool inline_math_native(vmIntrinsics::ID id);
 210   bool inline_trig(vmIntrinsics::ID id);
 211   bool inline_math(vmIntrinsics::ID id);
 212   template &lt;typename OverflowOp&gt;
 213   bool inline_math_overflow(Node* arg1, Node* arg2);
 214   void inline_math_mathExact(Node* math, Node* test);
 215   bool inline_math_addExactI(bool is_increment);
 216   bool inline_math_addExactL(bool is_increment);
 217   bool inline_math_multiplyExactI();
 218   bool inline_math_multiplyExactL();
 219   bool inline_math_negateExactI();
 220   bool inline_math_negateExactL();
 221   bool inline_math_subtractExactI(bool is_decrement);
 222   bool inline_math_subtractExactL(bool is_decrement);
 223   bool inline_exp();
 224   bool inline_pow();
 225   void finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName);
 226   bool inline_min_max(vmIntrinsics::ID id);
 227   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 228   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 229   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset);
 230   Node* make_unsafe_address(Node* base, Node* offset);
 231   // Helper for inline_unsafe_access.
 232   // Generates the guards that check whether the result of
 233   // Unsafe.getObject should be recorded in an SATB log buffer.
 234   void insert_pre_barrier(Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar);
 235   bool inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile);
 236   bool inline_unsafe_prefetch(bool is_native_ptr, bool is_store, bool is_static);
 237   static bool klass_needs_init_guard(Node* kls);
 238   bool inline_unsafe_allocate();
 239   bool inline_unsafe_copyMemory();
 240   bool inline_native_currentThread();
 241 #ifdef TRACE_HAVE_INTRINSICS
 242   bool inline_native_classID();
 243   bool inline_native_threadID();
 244 #endif
 245   bool inline_native_time_funcs(address method, const char* funcName);
 246   bool inline_native_isInterrupted();
 247   bool inline_native_Class_query(vmIntrinsics::ID id);
 248   bool inline_native_subtype_check();
 249 
 250   bool inline_native_newArray();
 251   bool inline_native_getLength();
 252   bool inline_array_copyOf(bool is_copyOfRange);
 253   bool inline_array_equals();
 254   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark);
 255   bool inline_native_clone(bool is_virtual);
 256   bool inline_native_Reflection_getCallerClass();
 257   // Helper function for inlining native object hash method
 258   bool inline_native_hashcode(bool is_virtual, bool is_static);
 259   bool inline_native_getClass();
 260 
 261   // Helper functions for inlining arraycopy
 262   bool inline_arraycopy();
 263   void generate_arraycopy(const TypePtr* adr_type,
 264                           BasicType basic_elem_type,
 265                           Node* src,  Node* src_offset,
 266                           Node* dest, Node* dest_offset,
 267                           Node* copy_length,
 268                           bool disjoint_bases = false,
 269                           bool length_never_negative = false,
 270                           RegionNode* slow_region = NULL);
 271   AllocateArrayNode* tightly_coupled_allocation(Node* ptr,
 272                                                 RegionNode* slow_region);
 273   void generate_clear_array(const TypePtr* adr_type,
 274                             Node* dest,
 275                             BasicType basic_elem_type,
 276                             Node* slice_off,
 277                             Node* slice_len,
 278                             Node* slice_end);
 279   bool generate_block_arraycopy(const TypePtr* adr_type,
 280                                 BasicType basic_elem_type,
 281                                 AllocateNode* alloc,
 282                                 Node* src,  Node* src_offset,
 283                                 Node* dest, Node* dest_offset,
 284                                 Node* dest_size, bool dest_uninitialized);
 285   void generate_slow_arraycopy(const TypePtr* adr_type,
 286                                Node* src,  Node* src_offset,
 287                                Node* dest, Node* dest_offset,
 288                                Node* copy_length, bool dest_uninitialized);
 289   Node* generate_checkcast_arraycopy(const TypePtr* adr_type,
 290                                      Node* dest_elem_klass,
 291                                      Node* src,  Node* src_offset,
 292                                      Node* dest, Node* dest_offset,
 293                                      Node* copy_length, bool dest_uninitialized);
 294   Node* generate_generic_arraycopy(const TypePtr* adr_type,
 295                                    Node* src,  Node* src_offset,
 296                                    Node* dest, Node* dest_offset,
 297                                    Node* copy_length, bool dest_uninitialized);
 298   void generate_unchecked_arraycopy(const TypePtr* adr_type,
 299                                     BasicType basic_elem_type,
 300                                     bool disjoint_bases,
 301                                     Node* src,  Node* src_offset,
 302                                     Node* dest, Node* dest_offset,
 303                                     Node* copy_length, bool dest_uninitialized);
 304   typedef enum { LS_xadd, LS_xchg, LS_cmpxchg } LoadStoreKind;
 305   bool inline_unsafe_load_store(BasicType type,  LoadStoreKind kind);
 306   bool inline_unsafe_ordered_store(BasicType type);
 307   bool inline_unsafe_fence(vmIntrinsics::ID id);
 308   bool inline_fp_conversions(vmIntrinsics::ID id);
 309   bool inline_number_methods(vmIntrinsics::ID id);
 310   bool inline_reference_get();
 311   bool inline_aescrypt_Block(vmIntrinsics::ID id);
 312   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
 313   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
 314   Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);
 315   Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);
 316   bool inline_encodeISOArray();
 317   bool inline_updateCRC32();
 318   bool inline_updateBytesCRC32();
 319   bool inline_updateByteBufferCRC32();
 320 };
 321 
 322 
 323 //---------------------------make_vm_intrinsic----------------------------
 324 CallGenerator* Compile::make_vm_intrinsic(ciMethod* m, bool is_virtual) {
 325   vmIntrinsics::ID id = m-&gt;intrinsic_id();
 326   assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
 327 
 328   if (DisableIntrinsic[0] != '\0'
 329       &amp;&amp; strstr(DisableIntrinsic, vmIntrinsics::name_at(id)) != NULL) {
 330     // disabled by a user request on the command line:
 331     // example: -XX:DisableIntrinsic=_hashCode,_getClass
 332     return NULL;
 333   }
 334 
 335   if (!m-&gt;is_loaded()) {
 336     // do not attempt to inline unloaded methods
 337     return NULL;
 338   }
 339 
 340   // Only a few intrinsics implement a virtual dispatch.
 341   // They are expensive calls which are also frequently overridden.
 342   if (is_virtual) {
 343     switch (id) {
 344     case vmIntrinsics::_hashCode:
 345     case vmIntrinsics::_clone:
 346       // OK, Object.hashCode and Object.clone intrinsics come in both flavors
 347       break;
 348     default:
 349       return NULL;
 350     }
 351   }
 352 
 353   // -XX:-InlineNatives disables nearly all intrinsics:
 354   if (!InlineNatives) {
 355     switch (id) {
 356     case vmIntrinsics::_indexOf:
 357     case vmIntrinsics::_compareTo:
 358     case vmIntrinsics::_equals:
 359     case vmIntrinsics::_equalsC:
 360     case vmIntrinsics::_getAndAddInt:
 361     case vmIntrinsics::_getAndAddLong:
 362     case vmIntrinsics::_getAndSetInt:
 363     case vmIntrinsics::_getAndSetLong:
 364     case vmIntrinsics::_getAndSetObject:
 365     case vmIntrinsics::_loadFence:
 366     case vmIntrinsics::_storeFence:
 367     case vmIntrinsics::_fullFence:
 368       break;  // InlineNatives does not control String.compareTo
 369     case vmIntrinsics::_Reference_get:
 370       break;  // InlineNatives does not control Reference.get
 371     default:
 372       return NULL;
 373     }
 374   }
 375 
 376   bool is_predicted = false;
 377   bool does_virtual_dispatch = false;
 378 
 379   switch (id) {
 380   case vmIntrinsics::_compareTo:
 381     if (!SpecialStringCompareTo)  return NULL;
 382     if (!Matcher::match_rule_supported(Op_StrComp))  return NULL;
 383     break;
 384   case vmIntrinsics::_indexOf:
 385     if (!SpecialStringIndexOf)  return NULL;
 386     break;
 387   case vmIntrinsics::_equals:
 388     if (!SpecialStringEquals)  return NULL;
 389     if (!Matcher::match_rule_supported(Op_StrEquals))  return NULL;
 390     break;
 391   case vmIntrinsics::_equalsC:
 392     if (!SpecialArraysEquals)  return NULL;
 393     if (!Matcher::match_rule_supported(Op_AryEq))  return NULL;
 394     break;
 395   case vmIntrinsics::_arraycopy:
 396     if (!InlineArrayCopy)  return NULL;
 397     break;
 398   case vmIntrinsics::_copyMemory:
 399     if (StubRoutines::unsafe_arraycopy() == NULL)  return NULL;
 400     if (!InlineArrayCopy)  return NULL;
 401     break;
 402   case vmIntrinsics::_hashCode:
 403     if (!InlineObjectHash)  return NULL;
 404     does_virtual_dispatch = true;
 405     break;
 406   case vmIntrinsics::_clone:
 407     does_virtual_dispatch = true;
 408   case vmIntrinsics::_copyOf:
 409   case vmIntrinsics::_copyOfRange:
 410     if (!InlineObjectCopy)  return NULL;
 411     // These also use the arraycopy intrinsic mechanism:
 412     if (!InlineArrayCopy)  return NULL;
 413     break;
 414   case vmIntrinsics::_encodeISOArray:
 415     if (!SpecialEncodeISOArray)  return NULL;
 416     if (!Matcher::match_rule_supported(Op_EncodeISOArray))  return NULL;
 417     break;
 418   case vmIntrinsics::_checkIndex:
 419     // We do not intrinsify this.  The optimizer does fine with it.
 420     return NULL;
 421 
 422   case vmIntrinsics::_getCallerClass:
 423     if (!InlineReflectionGetCallerClass)  return NULL;
 424     if (SystemDictionary::reflect_CallerSensitive_klass() == NULL)  return NULL;
 425     break;
 426 
 427   case vmIntrinsics::_bitCount_i:
 428     if (!Matcher::match_rule_supported(Op_PopCountI)) return NULL;
 429     break;
 430 
 431   case vmIntrinsics::_bitCount_l:
 432     if (!Matcher::match_rule_supported(Op_PopCountL)) return NULL;
 433     break;
 434 
 435   case vmIntrinsics::_numberOfLeadingZeros_i:
 436     if (!Matcher::match_rule_supported(Op_CountLeadingZerosI)) return NULL;
 437     break;
 438 
 439   case vmIntrinsics::_numberOfLeadingZeros_l:
 440     if (!Matcher::match_rule_supported(Op_CountLeadingZerosL)) return NULL;
 441     break;
 442 
 443   case vmIntrinsics::_numberOfTrailingZeros_i:
 444     if (!Matcher::match_rule_supported(Op_CountTrailingZerosI)) return NULL;
 445     break;
 446 
 447   case vmIntrinsics::_numberOfTrailingZeros_l:
 448     if (!Matcher::match_rule_supported(Op_CountTrailingZerosL)) return NULL;
 449     break;
 450 
 451   case vmIntrinsics::_reverseBytes_c:
 452     if (!Matcher::match_rule_supported(Op_ReverseBytesUS)) return NULL;
 453     break;
 454   case vmIntrinsics::_reverseBytes_s:
 455     if (!Matcher::match_rule_supported(Op_ReverseBytesS))  return NULL;
 456     break;
 457   case vmIntrinsics::_reverseBytes_i:
 458     if (!Matcher::match_rule_supported(Op_ReverseBytesI))  return NULL;
 459     break;
 460   case vmIntrinsics::_reverseBytes_l:
 461     if (!Matcher::match_rule_supported(Op_ReverseBytesL))  return NULL;
 462     break;
 463 
 464   case vmIntrinsics::_Reference_get:
 465     // Use the intrinsic version of Reference.get() so that the value in
 466     // the referent field can be registered by the G1 pre-barrier code.
 467     // Also add memory barrier to prevent commoning reads from this field
 468     // across safepoint since GC can change it value.
 469     break;
 470 
 471   case vmIntrinsics::_compareAndSwapObject:
 472 #ifdef _LP64
 473     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_CompareAndSwapP)) return NULL;
 474 #endif
 475     break;
 476 
 477   case vmIntrinsics::_compareAndSwapLong:
 478     if (!Matcher::match_rule_supported(Op_CompareAndSwapL)) return NULL;
 479     break;
 480 
 481   case vmIntrinsics::_getAndAddInt:
 482     if (!Matcher::match_rule_supported(Op_GetAndAddI)) return NULL;
 483     break;
 484 
 485   case vmIntrinsics::_getAndAddLong:
 486     if (!Matcher::match_rule_supported(Op_GetAndAddL)) return NULL;
 487     break;
 488 
 489   case vmIntrinsics::_getAndSetInt:
 490     if (!Matcher::match_rule_supported(Op_GetAndSetI)) return NULL;
 491     break;
 492 
 493   case vmIntrinsics::_getAndSetLong:
 494     if (!Matcher::match_rule_supported(Op_GetAndSetL)) return NULL;
 495     break;
 496 
 497   case vmIntrinsics::_getAndSetObject:
 498 #ifdef _LP64
 499     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 500     if (UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetN)) return NULL;
 501     break;
 502 #else
 503     if (!Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 504     break;
 505 #endif
 506 
 507   case vmIntrinsics::_aescrypt_encryptBlock:
 508   case vmIntrinsics::_aescrypt_decryptBlock:
 509     if (!UseAESIntrinsics) return NULL;
 510     break;
 511 
 512   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 513   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 514     if (!UseAESIntrinsics) return NULL;
 515     // these two require the predicated logic
 516     is_predicted = true;
 517     break;
 518 
 519   case vmIntrinsics::_updateCRC32:
 520   case vmIntrinsics::_updateBytesCRC32:
 521   case vmIntrinsics::_updateByteBufferCRC32:
 522     if (!UseCRC32Intrinsics) return NULL;
 523     break;
 524 
 525   case vmIntrinsics::_incrementExactI:
 526   case vmIntrinsics::_addExactI:
 527     if (!Matcher::match_rule_supported(Op_OverflowAddI) || !UseMathExactIntrinsics) return NULL;
 528     break;
 529   case vmIntrinsics::_incrementExactL:
 530   case vmIntrinsics::_addExactL:
 531     if (!Matcher::match_rule_supported(Op_OverflowAddL) || !UseMathExactIntrinsics) return NULL;
 532     break;
 533   case vmIntrinsics::_decrementExactI:
 534   case vmIntrinsics::_subtractExactI:
 535     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 536     break;
 537   case vmIntrinsics::_decrementExactL:
 538   case vmIntrinsics::_subtractExactL:
 539     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 540     break;
 541   case vmIntrinsics::_negateExactI:
 542     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 543     break;
 544   case vmIntrinsics::_negateExactL:
 545     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 546     break;
 547   case vmIntrinsics::_multiplyExactI:
 548     if (!Matcher::match_rule_supported(Op_OverflowMulI) || !UseMathExactIntrinsics) return NULL;
 549     break;
 550   case vmIntrinsics::_multiplyExactL:
 551     if (!Matcher::match_rule_supported(Op_OverflowMulL) || !UseMathExactIntrinsics) return NULL;
 552     break;
 553 
 554  default:
 555     assert(id &lt;= vmIntrinsics::LAST_COMPILER_INLINE, "caller responsibility");
 556     assert(id != vmIntrinsics::_Object_init &amp;&amp; id != vmIntrinsics::_invoke, "enum out of order?");
 557     break;
 558   }
 559 
 560   // -XX:-InlineClassNatives disables natives from the Class class.
 561   // The flag applies to all reflective calls, notably Array.newArray
 562   // (visible to Java programmers as Array.newInstance).
 563   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Class() ||
 564       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_reflect_Array()) {
 565     if (!InlineClassNatives)  return NULL;
 566   }
 567 
 568   // -XX:-InlineThreadNatives disables natives from the Thread class.
 569   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Thread()) {
 570     if (!InlineThreadNatives)  return NULL;
 571   }
 572 
 573   // -XX:-InlineMathNatives disables natives from the Math,Float and Double classes.
 574   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Math() ||
 575       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Float() ||
 576       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Double()) {
 577     if (!InlineMathNatives)  return NULL;
 578   }
 579 
 580   // -XX:-InlineUnsafeOps disables natives from the Unsafe class.
 581   if (m-&gt;holder()-&gt;name() == ciSymbol::sun_misc_Unsafe()) {
 582     if (!InlineUnsafeOps)  return NULL;
 583   }
 584 
 585   return new LibraryIntrinsic(m, is_virtual, is_predicted, does_virtual_dispatch, (vmIntrinsics::ID) id);
 586 }
 587 
 588 //----------------------register_library_intrinsics-----------------------
 589 // Initialize this file's data structures, for each Compile instance.
 590 void Compile::register_library_intrinsics() {
 591   // Nothing to do here.
 592 }
 593 
 594 JVMState* LibraryIntrinsic::generate(JVMState* jvms, Parse* parent_parser) {
 595   LibraryCallKit kit(jvms, this);
 596   Compile* C = kit.C;
 597   int nodes = C-&gt;unique();
 598 #ifndef PRODUCT
 599   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 600     char buf[1000];
 601     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 602     tty-&gt;print_cr("Intrinsic %s", str);
 603   }
 604 #endif
 605   ciMethod* callee = kit.callee();
 606   const int bci    = kit.bci();
 607 
 608   // Try to inline the intrinsic.
 609   if (kit.try_to_inline()) {
 610     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 611       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 612     }
 613     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 614     if (C-&gt;log()) {
 615       C-&gt;log()-&gt;elem("intrinsic id='%s'%s nodes='%d'",
 616                      vmIntrinsics::name_at(intrinsic_id()),
 617                      (is_virtual() ? " virtual='1'" : ""),
 618                      C-&gt;unique() - nodes);
 619     }
 620     // Push the result from the inlined method onto the stack.
 621     kit.push_result();
 622     C-&gt;print_inlining_update(this);
 623     return kit.transfer_exceptions_into_jvms();
 624   }
 625 
 626   // The intrinsic bailed out
 627   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 628     if (jvms-&gt;has_method()) {
 629       // Not a root compile.
 630       const char* msg = is_virtual() ? "failed to inline (intrinsic, virtual)" : "failed to inline (intrinsic)";
 631       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, msg);
 632     } else {
 633       // Root compile
 634       tty-&gt;print("Did not generate intrinsic %s%s at bci:%d in",
 635                vmIntrinsics::name_at(intrinsic_id()),
 636                (is_virtual() ? " (virtual)" : ""), bci);
 637     }
 638   }
 639   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 640   C-&gt;print_inlining_update(this);
 641   return NULL;
 642 }
 643 
 644 Node* LibraryIntrinsic::generate_predicate(JVMState* jvms) {
 645   LibraryCallKit kit(jvms, this);
 646   Compile* C = kit.C;
 647   int nodes = C-&gt;unique();
 648 #ifndef PRODUCT
 649   assert(is_predicted(), "sanity");
 650   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 651     char buf[1000];
 652     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 653     tty-&gt;print_cr("Predicate for intrinsic %s", str);
 654   }
 655 #endif
 656   ciMethod* callee = kit.callee();
 657   const int bci    = kit.bci();
 658 
 659   Node* slow_ctl = kit.try_to_predicate();
 660   if (!kit.failing()) {
 661     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 662       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 663     }
 664     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 665     if (C-&gt;log()) {
 666       C-&gt;log()-&gt;elem("predicate_intrinsic id='%s'%s nodes='%d'",
 667                      vmIntrinsics::name_at(intrinsic_id()),
 668                      (is_virtual() ? " virtual='1'" : ""),
 669                      C-&gt;unique() - nodes);
 670     }
 671     return slow_ctl; // Could be NULL if the check folds.
 672   }
 673 
 674   // The intrinsic bailed out
 675   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 676     if (jvms-&gt;has_method()) {
 677       // Not a root compile.
 678       const char* msg = "failed to generate predicate for intrinsic";
 679       C-&gt;print_inlining(kit.callee(), jvms-&gt;depth() - 1, bci, msg);
 680     } else {
 681       // Root compile
 682       C-&gt;print_inlining_stream()-&gt;print("Did not generate predicate for intrinsic %s%s at bci:%d in",
 683                                         vmIntrinsics::name_at(intrinsic_id()),
 684                                         (is_virtual() ? " (virtual)" : ""), bci);
 685     }
 686   }
 687   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 688   return NULL;
 689 }
 690 
 691 bool LibraryCallKit::try_to_inline() {
 692   // Handle symbolic names for otherwise undistinguished boolean switches:
 693   const bool is_store       = true;
 694   const bool is_native_ptr  = true;
 695   const bool is_static      = true;
 696   const bool is_volatile    = true;
 697 
 698   if (!jvms()-&gt;has_method()) {
 699     // Root JVMState has a null method.
 700     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 701     // Insert the memory aliasing node
 702     set_all_memory(reset_memory());
 703   }
 704   assert(merged_memory(), "");
 705 
 706 
 707   switch (intrinsic_id()) {
 708   case vmIntrinsics::_hashCode:                 return inline_native_hashcode(intrinsic()-&gt;is_virtual(), !is_static);
 709   case vmIntrinsics::_identityHashCode:         return inline_native_hashcode(/*!virtual*/ false,         is_static);
 710   case vmIntrinsics::_getClass:                 return inline_native_getClass();
 711 
 712   case vmIntrinsics::_dsin:
 713   case vmIntrinsics::_dcos:
 714   case vmIntrinsics::_dtan:
 715   case vmIntrinsics::_dabs:
 716   case vmIntrinsics::_datan2:
 717   case vmIntrinsics::_dsqrt:
 718   case vmIntrinsics::_dexp:
 719   case vmIntrinsics::_dlog:
 720   case vmIntrinsics::_dlog10:
 721   case vmIntrinsics::_dpow:                     return inline_math_native(intrinsic_id());
 722 
 723   case vmIntrinsics::_min:
 724   case vmIntrinsics::_max:                      return inline_min_max(intrinsic_id());
 725 
 726   case vmIntrinsics::_addExactI:                return inline_math_addExactI(false /* add */);
 727   case vmIntrinsics::_addExactL:                return inline_math_addExactL(false /* add */);
 728   case vmIntrinsics::_decrementExactI:          return inline_math_subtractExactI(true /* decrement */);
 729   case vmIntrinsics::_decrementExactL:          return inline_math_subtractExactL(true /* decrement */);
 730   case vmIntrinsics::_incrementExactI:          return inline_math_addExactI(true /* increment */);
 731   case vmIntrinsics::_incrementExactL:          return inline_math_addExactL(true /* increment */);
 732   case vmIntrinsics::_multiplyExactI:           return inline_math_multiplyExactI();
 733   case vmIntrinsics::_multiplyExactL:           return inline_math_multiplyExactL();
 734   case vmIntrinsics::_negateExactI:             return inline_math_negateExactI();
 735   case vmIntrinsics::_negateExactL:             return inline_math_negateExactL();
 736   case vmIntrinsics::_subtractExactI:           return inline_math_subtractExactI(false /* subtract */);
 737   case vmIntrinsics::_subtractExactL:           return inline_math_subtractExactL(false /* subtract */);
 738 
 739   case vmIntrinsics::_arraycopy:                return inline_arraycopy();
 740 
 741   case vmIntrinsics::_compareTo:                return inline_string_compareTo();
 742   case vmIntrinsics::_indexOf:                  return inline_string_indexOf();
 743   case vmIntrinsics::_equals:                   return inline_string_equals();
 744 
 745   case vmIntrinsics::_getObject:                return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,  !is_volatile);
 746   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN, !is_volatile);
 747   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 748   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 749   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 750   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile);
 751   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile);
 752   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 753   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 754 
 755   case vmIntrinsics::_putObject:                return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,  !is_volatile);
 756   case vmIntrinsics::_putBoolean:               return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN, !is_volatile);
 757   case vmIntrinsics::_putByte:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 758   case vmIntrinsics::_putShort:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 759   case vmIntrinsics::_putChar:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 760   case vmIntrinsics::_putInt:                   return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile);
 761   case vmIntrinsics::_putLong:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile);
 762   case vmIntrinsics::_putFloat:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 763   case vmIntrinsics::_putDouble:                return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 764 
 765   case vmIntrinsics::_getByte_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 766   case vmIntrinsics::_getShort_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 767   case vmIntrinsics::_getChar_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 768   case vmIntrinsics::_getInt_raw:               return inline_unsafe_access( is_native_ptr, !is_store, T_INT,     !is_volatile);
 769   case vmIntrinsics::_getLong_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_LONG,    !is_volatile);
 770   case vmIntrinsics::_getFloat_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 771   case vmIntrinsics::_getDouble_raw:            return inline_unsafe_access( is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 772   case vmIntrinsics::_getAddress_raw:           return inline_unsafe_access( is_native_ptr, !is_store, T_ADDRESS, !is_volatile);
 773 
 774   case vmIntrinsics::_putByte_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 775   case vmIntrinsics::_putShort_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 776   case vmIntrinsics::_putChar_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 777   case vmIntrinsics::_putInt_raw:               return inline_unsafe_access( is_native_ptr,  is_store, T_INT,     !is_volatile);
 778   case vmIntrinsics::_putLong_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_LONG,    !is_volatile);
 779   case vmIntrinsics::_putFloat_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 780   case vmIntrinsics::_putDouble_raw:            return inline_unsafe_access( is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 781   case vmIntrinsics::_putAddress_raw:           return inline_unsafe_access( is_native_ptr,  is_store, T_ADDRESS, !is_volatile);
 782 
 783   case vmIntrinsics::_getObjectVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   is_volatile);
 784   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  is_volatile);
 785   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     is_volatile);
 786   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    is_volatile);
 787   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     is_volatile);
 788   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      is_volatile);
 789   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     is_volatile);
 790   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    is_volatile);
 791   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   is_volatile);
 792 
 793   case vmIntrinsics::_putObjectVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   is_volatile);
 794   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  is_volatile);
 795   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     is_volatile);
 796   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    is_volatile);
 797   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     is_volatile);
 798   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      is_volatile);
 799   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     is_volatile);
 800   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    is_volatile);
 801   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   is_volatile);
 802 
 803   case vmIntrinsics::_prefetchRead:             return inline_unsafe_prefetch(!is_native_ptr, !is_store, !is_static);
 804   case vmIntrinsics::_prefetchWrite:            return inline_unsafe_prefetch(!is_native_ptr,  is_store, !is_static);
 805   case vmIntrinsics::_prefetchReadStatic:       return inline_unsafe_prefetch(!is_native_ptr, !is_store,  is_static);
 806   case vmIntrinsics::_prefetchWriteStatic:      return inline_unsafe_prefetch(!is_native_ptr,  is_store,  is_static);
 807 
 808   case vmIntrinsics::_compareAndSwapObject:     return inline_unsafe_load_store(T_OBJECT, LS_cmpxchg);
 809   case vmIntrinsics::_compareAndSwapInt:        return inline_unsafe_load_store(T_INT,    LS_cmpxchg);
 810   case vmIntrinsics::_compareAndSwapLong:       return inline_unsafe_load_store(T_LONG,   LS_cmpxchg);
 811 
 812   case vmIntrinsics::_putOrderedObject:         return inline_unsafe_ordered_store(T_OBJECT);
 813   case vmIntrinsics::_putOrderedInt:            return inline_unsafe_ordered_store(T_INT);
 814   case vmIntrinsics::_putOrderedLong:           return inline_unsafe_ordered_store(T_LONG);
 815 
 816   case vmIntrinsics::_getAndAddInt:             return inline_unsafe_load_store(T_INT,    LS_xadd);
 817   case vmIntrinsics::_getAndAddLong:            return inline_unsafe_load_store(T_LONG,   LS_xadd);
 818   case vmIntrinsics::_getAndSetInt:             return inline_unsafe_load_store(T_INT,    LS_xchg);
 819   case vmIntrinsics::_getAndSetLong:            return inline_unsafe_load_store(T_LONG,   LS_xchg);
 820   case vmIntrinsics::_getAndSetObject:          return inline_unsafe_load_store(T_OBJECT, LS_xchg);
 821 
 822   case vmIntrinsics::_loadFence:
 823   case vmIntrinsics::_storeFence:
 824   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 825 
 826   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 827   case vmIntrinsics::_isInterrupted:            return inline_native_isInterrupted();
 828 
 829 #ifdef TRACE_HAVE_INTRINSICS
 830   case vmIntrinsics::_classID:                  return inline_native_classID();
 831   case vmIntrinsics::_threadID:                 return inline_native_threadID();
 832   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, TRACE_TIME_METHOD), "counterTime");
 833 #endif
 834   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), "currentTimeMillis");
 835   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), "nanoTime");
 836   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 837   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 838   case vmIntrinsics::_newArray:                 return inline_native_newArray();
 839   case vmIntrinsics::_getLength:                return inline_native_getLength();
 840   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 841   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 842   case vmIntrinsics::_equalsC:                  return inline_array_equals();
 843   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 844 
 845   case vmIntrinsics::_isAssignableFrom:         return inline_native_subtype_check();
 846 
 847   case vmIntrinsics::_isInstance:
 848   case vmIntrinsics::_getModifiers:
 849   case vmIntrinsics::_isInterface:
 850   case vmIntrinsics::_isArray:
 851   case vmIntrinsics::_isPrimitive:
 852   case vmIntrinsics::_getSuperclass:
 853   case vmIntrinsics::_getComponentType:
 854   case vmIntrinsics::_getClassAccessFlags:      return inline_native_Class_query(intrinsic_id());
 855 
 856   case vmIntrinsics::_floatToRawIntBits:
 857   case vmIntrinsics::_floatToIntBits:
 858   case vmIntrinsics::_intBitsToFloat:
 859   case vmIntrinsics::_doubleToRawLongBits:
 860   case vmIntrinsics::_doubleToLongBits:
 861   case vmIntrinsics::_longBitsToDouble:         return inline_fp_conversions(intrinsic_id());
 862 
 863   case vmIntrinsics::_numberOfLeadingZeros_i:
 864   case vmIntrinsics::_numberOfLeadingZeros_l:
 865   case vmIntrinsics::_numberOfTrailingZeros_i:
 866   case vmIntrinsics::_numberOfTrailingZeros_l:
 867   case vmIntrinsics::_bitCount_i:
 868   case vmIntrinsics::_bitCount_l:
 869   case vmIntrinsics::_reverseBytes_i:
 870   case vmIntrinsics::_reverseBytes_l:
 871   case vmIntrinsics::_reverseBytes_s:
 872   case vmIntrinsics::_reverseBytes_c:           return inline_number_methods(intrinsic_id());
 873 
 874   case vmIntrinsics::_getCallerClass:           return inline_native_Reflection_getCallerClass();
 875 
 876   case vmIntrinsics::_Reference_get:            return inline_reference_get();
 877 
 878   case vmIntrinsics::_aescrypt_encryptBlock:
 879   case vmIntrinsics::_aescrypt_decryptBlock:    return inline_aescrypt_Block(intrinsic_id());
 880 
 881   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 882   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 883     return inline_cipherBlockChaining_AESCrypt(intrinsic_id());
 884 
 885   case vmIntrinsics::_encodeISOArray:
 886     return inline_encodeISOArray();
 887 
 888   case vmIntrinsics::_updateCRC32:
 889     return inline_updateCRC32();
 890   case vmIntrinsics::_updateBytesCRC32:
 891     return inline_updateBytesCRC32();
 892   case vmIntrinsics::_updateByteBufferCRC32:
 893     return inline_updateByteBufferCRC32();
 894 
 895   default:
 896     // If you get here, it may be that someone has added a new intrinsic
 897     // to the list in vmSymbols.hpp without implementing it here.
 898 #ifndef PRODUCT
 899     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 900       tty-&gt;print_cr("*** Warning: Unimplemented intrinsic %s(%d)",
 901                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 902     }
 903 #endif
 904     return false;
 905   }
 906 }
 907 
 908 Node* LibraryCallKit::try_to_predicate() {
 909   if (!jvms()-&gt;has_method()) {
 910     // Root JVMState has a null method.
 911     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 912     // Insert the memory aliasing node
 913     set_all_memory(reset_memory());
 914   }
 915   assert(merged_memory(), "");
 916 
 917   switch (intrinsic_id()) {
 918   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 919     return inline_cipherBlockChaining_AESCrypt_predicate(false);
 920   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 921     return inline_cipherBlockChaining_AESCrypt_predicate(true);
 922 
 923   default:
 924     // If you get here, it may be that someone has added a new intrinsic
 925     // to the list in vmSymbols.hpp without implementing it here.
 926 #ifndef PRODUCT
 927     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 928       tty-&gt;print_cr("*** Warning: Unimplemented predicate for intrinsic %s(%d)",
 929                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 930     }
 931 #endif
 932     Node* slow_ctl = control();
 933     set_control(top()); // No fast path instrinsic
 934     return slow_ctl;
 935   }
 936 }
 937 
 938 //------------------------------set_result-------------------------------
 939 // Helper function for finishing intrinsics.
 940 void LibraryCallKit::set_result(RegionNode* region, PhiNode* value) {
 941   record_for_igvn(region);
 942   set_control(_gvn.transform(region));
 943   set_result( _gvn.transform(value));
 944   assert(value-&gt;type()-&gt;basic_type() == result()-&gt;bottom_type()-&gt;basic_type(), "sanity");
 945 }
 946 
 947 //------------------------------generate_guard---------------------------
 948 // Helper function for generating guarded fast-slow graph structures.
 949 // The given 'test', if true, guards a slow path.  If the test fails
 950 // then a fast path can be taken.  (We generally hope it fails.)
 951 // In all cases, GraphKit::control() is updated to the fast path.
 952 // The returned value represents the control for the slow path.
 953 // The return value is never 'top'; it is either a valid control
 954 // or NULL if it is obvious that the slow path can never be taken.
 955 // Also, if region and the slow control are not NULL, the slow edge
 956 // is appended to the region.
 957 Node* LibraryCallKit::generate_guard(Node* test, RegionNode* region, float true_prob) {
 958   if (stopped()) {
 959     // Already short circuited.
 960     return NULL;
 961   }
 962 
 963   // Build an if node and its projections.
 964   // If test is true we take the slow path, which we assume is uncommon.
 965   if (_gvn.type(test) == TypeInt::ZERO) {
 966     // The slow branch is never taken.  No need to build this guard.
 967     return NULL;
 968   }
 969 
 970   IfNode* iff = create_and_map_if(control(), test, true_prob, COUNT_UNKNOWN);
 971 
 972   Node* if_slow = _gvn.transform(new (C) IfTrueNode(iff));
 973   if (if_slow == top()) {
 974     // The slow branch is never taken.  No need to build this guard.
 975     return NULL;
 976   }
 977 
 978   if (region != NULL)
 979     region-&gt;add_req(if_slow);
 980 
 981   Node* if_fast = _gvn.transform(new (C) IfFalseNode(iff));
 982   set_control(if_fast);
 983 
 984   return if_slow;
 985 }
 986 
 987 inline Node* LibraryCallKit::generate_slow_guard(Node* test, RegionNode* region) {
 988   return generate_guard(test, region, PROB_UNLIKELY_MAG(3));
 989 }
 990 inline Node* LibraryCallKit::generate_fair_guard(Node* test, RegionNode* region) {
 991   return generate_guard(test, region, PROB_FAIR);
 992 }
 993 
 994 inline Node* LibraryCallKit::generate_negative_guard(Node* index, RegionNode* region,
 995                                                      Node* *pos_index) {
 996   if (stopped())
 997     return NULL;                // already stopped
 998   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS)) // [0,maxint]
 999     return NULL;                // index is already adequately typed
1000   Node* cmp_lt = _gvn.transform(new (C) CmpINode(index, intcon(0)));
1001   Node* bol_lt = _gvn.transform(new (C) BoolNode(cmp_lt, BoolTest::lt));
1002   Node* is_neg = generate_guard(bol_lt, region, PROB_MIN);
1003   if (is_neg != NULL &amp;&amp; pos_index != NULL) {
1004     // Emulate effect of Parse::adjust_map_after_if.
1005     Node* ccast = new (C) CastIINode(index, TypeInt::POS);
1006     ccast-&gt;set_req(0, control());
1007     (*pos_index) = _gvn.transform(ccast);
1008   }
1009   return is_neg;
1010 }
1011 
1012 inline Node* LibraryCallKit::generate_nonpositive_guard(Node* index, bool never_negative,
1013                                                         Node* *pos_index) {
1014   if (stopped())
1015     return NULL;                // already stopped
1016   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS1)) // [1,maxint]
1017     return NULL;                // index is already adequately typed
1018   Node* cmp_le = _gvn.transform(new (C) CmpINode(index, intcon(0)));
1019   BoolTest::mask le_or_eq = (never_negative ? BoolTest::eq : BoolTest::le);
1020   Node* bol_le = _gvn.transform(new (C) BoolNode(cmp_le, le_or_eq));
1021   Node* is_notp = generate_guard(bol_le, NULL, PROB_MIN);
1022   if (is_notp != NULL &amp;&amp; pos_index != NULL) {
1023     // Emulate effect of Parse::adjust_map_after_if.
1024     Node* ccast = new (C) CastIINode(index, TypeInt::POS1);
1025     ccast-&gt;set_req(0, control());
1026     (*pos_index) = _gvn.transform(ccast);
1027   }
1028   return is_notp;
1029 }
1030 
1031 // Make sure that 'position' is a valid limit index, in [0..length].
1032 // There are two equivalent plans for checking this:
1033 //   A. (offset + copyLength)  unsigned&lt;=  arrayLength
1034 //   B. offset  &lt;=  (arrayLength - copyLength)
1035 // We require that all of the values above, except for the sum and
1036 // difference, are already known to be non-negative.
1037 // Plan A is robust in the face of overflow, if offset and copyLength
1038 // are both hugely positive.
1039 //
1040 // Plan B is less direct and intuitive, but it does not overflow at
1041 // all, since the difference of two non-negatives is always
1042 // representable.  Whenever Java methods must perform the equivalent
1043 // check they generally use Plan B instead of Plan A.
1044 // For the moment we use Plan A.
1045 inline Node* LibraryCallKit::generate_limit_guard(Node* offset,
1046                                                   Node* subseq_length,
1047                                                   Node* array_length,
1048                                                   RegionNode* region) {
1049   if (stopped())
1050     return NULL;                // already stopped
1051   bool zero_offset = _gvn.type(offset) == TypeInt::ZERO;
1052   if (zero_offset &amp;&amp; subseq_length-&gt;eqv_uncast(array_length))
1053     return NULL;                // common case of whole-array copy
1054   Node* last = subseq_length;
1055   if (!zero_offset)             // last += offset
1056     last = _gvn.transform(new (C) AddINode(last, offset));
1057   Node* cmp_lt = _gvn.transform(new (C) CmpUNode(array_length, last));
1058   Node* bol_lt = _gvn.transform(new (C) BoolNode(cmp_lt, BoolTest::lt));
1059   Node* is_over = generate_guard(bol_lt, region, PROB_MIN);
1060   return is_over;
1061 }
1062 
1063 
1064 //--------------------------generate_current_thread--------------------
1065 Node* LibraryCallKit::generate_current_thread(Node* &amp;tls_output) {
1066   ciKlass*    thread_klass = env()-&gt;Thread_klass();
1067   const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
1068   Node* thread = _gvn.transform(new (C) ThreadLocalNode());
1069   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::threadObj_offset()));
1070   Node* threadObj = make_load(NULL, p, thread_type, T_OBJECT, MemNode::unordered);
1071   tls_output = thread;
1072   return threadObj;
1073 }
1074 
1075 
1076 //------------------------------make_string_method_node------------------------
1077 // Helper method for String intrinsic functions. This version is called
1078 // with str1 and str2 pointing to String object nodes.
1079 //
1080 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1, Node* str2) {
1081   Node* no_ctrl = NULL;
1082 
1083   // Get start addr of string
1084   Node* str1_value   = load_String_value(no_ctrl, str1);
1085   Node* str1_offset  = load_String_offset(no_ctrl, str1);
1086   Node* str1_start   = array_element_address(str1_value, str1_offset, T_CHAR);
1087 
1088   // Get length of string 1
1089   Node* str1_len  = load_String_length(no_ctrl, str1);
1090 
1091   Node* str2_value   = load_String_value(no_ctrl, str2);
1092   Node* str2_offset  = load_String_offset(no_ctrl, str2);
1093   Node* str2_start   = array_element_address(str2_value, str2_offset, T_CHAR);
1094 
1095   Node* str2_len = NULL;
1096   Node* result = NULL;
1097 
1098   switch (opcode) {
1099   case Op_StrIndexOf:
1100     // Get length of string 2
1101     str2_len = load_String_length(no_ctrl, str2);
1102 
1103     result = new (C) StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1104                                  str1_start, str1_len, str2_start, str2_len);
1105     break;
1106   case Op_StrComp:
1107     // Get length of string 2
1108     str2_len = load_String_length(no_ctrl, str2);
1109 
1110     result = new (C) StrCompNode(control(), memory(TypeAryPtr::CHARS),
1111                                  str1_start, str1_len, str2_start, str2_len);
1112     break;
1113   case Op_StrEquals:
1114     result = new (C) StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1115                                str1_start, str2_start, str1_len);
1116     break;
1117   default:
1118     ShouldNotReachHere();
1119     return NULL;
1120   }
1121 
1122   // All these intrinsics have checks.
1123   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1124 
1125   return _gvn.transform(result);
1126 }
1127 
1128 // Helper method for String intrinsic functions. This version is called
1129 // with str1 and str2 pointing to char[] nodes, with cnt1 and cnt2 pointing
1130 // to Int nodes containing the lenghts of str1 and str2.
1131 //
1132 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2) {
1133   Node* result = NULL;
1134   switch (opcode) {
1135   case Op_StrIndexOf:
1136     result = new (C) StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1137                                  str1_start, cnt1, str2_start, cnt2);
1138     break;
1139   case Op_StrComp:
1140     result = new (C) StrCompNode(control(), memory(TypeAryPtr::CHARS),
1141                                  str1_start, cnt1, str2_start, cnt2);
1142     break;
1143   case Op_StrEquals:
1144     result = new (C) StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1145                                  str1_start, str2_start, cnt1);
1146     break;
1147   default:
1148     ShouldNotReachHere();
1149     return NULL;
1150   }
1151 
1152   // All these intrinsics have checks.
1153   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1154 
1155   return _gvn.transform(result);
1156 }
1157 
1158 //------------------------------inline_string_compareTo------------------------
1159 // public int java.lang.String.compareTo(String anotherString);
1160 bool LibraryCallKit::inline_string_compareTo() {
1161   Node* receiver = null_check(argument(0));
1162   Node* arg      = null_check(argument(1));
1163   if (stopped()) {
1164     return true;
1165   }
1166   set_result(make_string_method_node(Op_StrComp, receiver, arg));
1167   return true;
1168 }
1169 
1170 //------------------------------inline_string_equals------------------------
1171 bool LibraryCallKit::inline_string_equals() {
1172   Node* receiver = null_check_receiver();
1173   // NOTE: Do not null check argument for String.equals() because spec
1174   // allows to specify NULL as argument.
1175   Node* argument = this-&gt;argument(1);
1176   if (stopped()) {
1177     return true;
1178   }
1179 
1180   // paths (plus control) merge
1181   RegionNode* region = new (C) RegionNode(5);
1182   Node* phi = new (C) PhiNode(region, TypeInt::BOOL);
1183 
1184   // does source == target string?
1185   Node* cmp = _gvn.transform(new (C) CmpPNode(receiver, argument));
1186   Node* bol = _gvn.transform(new (C) BoolNode(cmp, BoolTest::eq));
1187 
1188   Node* if_eq = generate_slow_guard(bol, NULL);
1189   if (if_eq != NULL) {
1190     // receiver == argument
1191     phi-&gt;init_req(2, intcon(1));
1192     region-&gt;init_req(2, if_eq);
1193   }
1194 
1195   // get String klass for instanceOf
1196   ciInstanceKlass* klass = env()-&gt;String_klass();
1197 
1198   if (!stopped()) {
1199     Node* inst = gen_instanceof(argument, makecon(TypeKlassPtr::make(klass)));
1200     Node* cmp  = _gvn.transform(new (C) CmpINode(inst, intcon(1)));
1201     Node* bol  = _gvn.transform(new (C) BoolNode(cmp, BoolTest::ne));
1202 
1203     Node* inst_false = generate_guard(bol, NULL, PROB_MIN);
1204     //instanceOf == true, fallthrough
1205 
1206     if (inst_false != NULL) {
1207       phi-&gt;init_req(3, intcon(0));
1208       region-&gt;init_req(3, inst_false);
1209     }
1210   }
1211 
1212   if (!stopped()) {
1213     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(klass);
1214 
1215     // Properly cast the argument to String
1216     argument = _gvn.transform(new (C) CheckCastPPNode(control(), argument, string_type));
1217     // This path is taken only when argument's type is String:NotNull.
1218     argument = cast_not_null(argument, false);
1219 
1220     Node* no_ctrl = NULL;
1221 
1222     // Get start addr of receiver
1223     Node* receiver_val    = load_String_value(no_ctrl, receiver);
1224     Node* receiver_offset = load_String_offset(no_ctrl, receiver);
1225     Node* receiver_start = array_element_address(receiver_val, receiver_offset, T_CHAR);
1226 
1227     // Get length of receiver
1228     Node* receiver_cnt  = load_String_length(no_ctrl, receiver);
1229 
1230     // Get start addr of argument
1231     Node* argument_val    = load_String_value(no_ctrl, argument);
1232     Node* argument_offset = load_String_offset(no_ctrl, argument);
1233     Node* argument_start = array_element_address(argument_val, argument_offset, T_CHAR);
1234 
1235     // Get length of argument
1236     Node* argument_cnt  = load_String_length(no_ctrl, argument);
1237 
1238     // Check for receiver count != argument count
1239     Node* cmp = _gvn.transform(new(C) CmpINode(receiver_cnt, argument_cnt));
1240     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::ne));
1241     Node* if_ne = generate_slow_guard(bol, NULL);
1242     if (if_ne != NULL) {
1243       phi-&gt;init_req(4, intcon(0));
1244       region-&gt;init_req(4, if_ne);
1245     }
1246 
1247     // Check for count == 0 is done by assembler code for StrEquals.
1248 
1249     if (!stopped()) {
1250       Node* equals = make_string_method_node(Op_StrEquals, receiver_start, receiver_cnt, argument_start, argument_cnt);
1251       phi-&gt;init_req(1, equals);
1252       region-&gt;init_req(1, control());
1253     }
1254   }
1255 
1256   // post merge
1257   set_control(_gvn.transform(region));
1258   record_for_igvn(region);
1259 
1260   set_result(_gvn.transform(phi));
1261   return true;
1262 }
1263 
1264 //------------------------------inline_array_equals----------------------------
1265 bool LibraryCallKit::inline_array_equals() {
1266   Node* arg1 = argument(0);
1267   Node* arg2 = argument(1);
1268   set_result(_gvn.transform(new (C) AryEqNode(control(), memory(TypeAryPtr::CHARS), arg1, arg2)));
1269   return true;
1270 }
1271 
1272 // Java version of String.indexOf(constant string)
1273 // class StringDecl {
1274 //   StringDecl(char[] ca) {
1275 //     offset = 0;
1276 //     count = ca.length;
1277 //     value = ca;
1278 //   }
1279 //   int offset;
1280 //   int count;
1281 //   char[] value;
1282 // }
1283 //
1284 // static int string_indexOf_J(StringDecl string_object, char[] target_object,
1285 //                             int targetOffset, int cache_i, int md2) {
1286 //   int cache = cache_i;
1287 //   int sourceOffset = string_object.offset;
1288 //   int sourceCount = string_object.count;
1289 //   int targetCount = target_object.length;
1290 //
1291 //   int targetCountLess1 = targetCount - 1;
1292 //   int sourceEnd = sourceOffset + sourceCount - targetCountLess1;
1293 //
1294 //   char[] source = string_object.value;
1295 //   char[] target = target_object;
1296 //   int lastChar = target[targetCountLess1];
1297 //
1298 //  outer_loop:
1299 //   for (int i = sourceOffset; i &lt; sourceEnd; ) {
1300 //     int src = source[i + targetCountLess1];
1301 //     if (src == lastChar) {
1302 //       // With random strings and a 4-character alphabet,
1303 //       // reverse matching at this point sets up 0.8% fewer
1304 //       // frames, but (paradoxically) makes 0.3% more probes.
1305 //       // Since those probes are nearer the lastChar probe,
1306 //       // there is may be a net D$ win with reverse matching.
1307 //       // But, reversing loop inhibits unroll of inner loop
1308 //       // for unknown reason.  So, does running outer loop from
1309 //       // (sourceOffset - targetCountLess1) to (sourceOffset + sourceCount)
1310 //       for (int j = 0; j &lt; targetCountLess1; j++) {
1311 //         if (target[targetOffset + j] != source[i+j]) {
1312 //           if ((cache &amp; (1 &lt;&lt; source[i+j])) == 0) {
1313 //             if (md2 &lt; j+1) {
1314 //               i += j+1;
1315 //               continue outer_loop;
1316 //             }
1317 //           }
1318 //           i += md2;
1319 //           continue outer_loop;
1320 //         }
1321 //       }
1322 //       return i - sourceOffset;
1323 //     }
1324 //     if ((cache &amp; (1 &lt;&lt; src)) == 0) {
1325 //       i += targetCountLess1;
1326 //     } // using "i += targetCount;" and an "else i++;" causes a jump to jump.
1327 //     i++;
1328 //   }
1329 //   return -1;
1330 // }
1331 
1332 //------------------------------string_indexOf------------------------
1333 Node* LibraryCallKit::string_indexOf(Node* string_object, ciTypeArray* target_array, jint targetOffset_i,
1334                                      jint cache_i, jint md2_i) {
1335 
1336   Node* no_ctrl  = NULL;
1337   float likely   = PROB_LIKELY(0.9);
1338   float unlikely = PROB_UNLIKELY(0.9);
1339 
1340   const int nargs = 0; // no arguments to push back for uncommon trap in predicate
1341 
1342   Node* source        = load_String_value(no_ctrl, string_object);
1343   Node* sourceOffset  = load_String_offset(no_ctrl, string_object);
1344   Node* sourceCount   = load_String_length(no_ctrl, string_object);
1345 
1346   Node* target = _gvn.transform( makecon(TypeOopPtr::make_from_constant(target_array, true)));
1347   jint target_length = target_array-&gt;length();
1348   const TypeAry* target_array_type = TypeAry::make(TypeInt::CHAR, TypeInt::make(0, target_length, Type::WidenMin));
1349   const TypeAryPtr* target_type = TypeAryPtr::make(TypePtr::BotPTR, target_array_type, target_array-&gt;klass(), true, Type::OffsetBot);
1350 
1351   // String.value field is known to be @Stable.
1352   if (UseImplicitStableValues) {
1353     target = cast_array_to_stable(target, target_type);
1354   }
1355 
1356   IdealKit kit(this, false, true);
1357 #define __ kit.
1358   Node* zero             = __ ConI(0);
1359   Node* one              = __ ConI(1);
1360   Node* cache            = __ ConI(cache_i);
1361   Node* md2              = __ ConI(md2_i);
1362   Node* lastChar         = __ ConI(target_array-&gt;char_at(target_length - 1));
1363   Node* targetCount      = __ ConI(target_length);
1364   Node* targetCountLess1 = __ ConI(target_length - 1);
1365   Node* targetOffset     = __ ConI(targetOffset_i);
1366   Node* sourceEnd        = __ SubI(__ AddI(sourceOffset, sourceCount), targetCountLess1);
1367 
1368   IdealVariable rtn(kit), i(kit), j(kit); __ declarations_done();
1369   Node* outer_loop = __ make_label(2 /* goto */);
1370   Node* return_    = __ make_label(1);
1371 
1372   __ set(rtn,__ ConI(-1));
1373   __ loop(this, nargs, i, sourceOffset, BoolTest::lt, sourceEnd); {
1374        Node* i2  = __ AddI(__ value(i), targetCountLess1);
1375        // pin to prohibit loading of "next iteration" value which may SEGV (rare)
1376        Node* src = load_array_element(__ ctrl(), source, i2, TypeAryPtr::CHARS);
1377        __ if_then(src, BoolTest::eq, lastChar, unlikely); {
1378          __ loop(this, nargs, j, zero, BoolTest::lt, targetCountLess1); {
1379               Node* tpj = __ AddI(targetOffset, __ value(j));
1380               Node* targ = load_array_element(no_ctrl, target, tpj, target_type);
1381               Node* ipj  = __ AddI(__ value(i), __ value(j));
1382               Node* src2 = load_array_element(no_ctrl, source, ipj, TypeAryPtr::CHARS);
1383               __ if_then(targ, BoolTest::ne, src2); {
1384                 __ if_then(__ AndI(cache, __ LShiftI(one, src2)), BoolTest::eq, zero); {
1385                   __ if_then(md2, BoolTest::lt, __ AddI(__ value(j), one)); {
1386                     __ increment(i, __ AddI(__ value(j), one));
1387                     __ goto_(outer_loop);
1388                   } __ end_if(); __ dead(j);
1389                 }__ end_if(); __ dead(j);
1390                 __ increment(i, md2);
1391                 __ goto_(outer_loop);
1392               }__ end_if();
1393               __ increment(j, one);
1394          }__ end_loop(); __ dead(j);
1395          __ set(rtn, __ SubI(__ value(i), sourceOffset)); __ dead(i);
1396          __ goto_(return_);
1397        }__ end_if();
1398        __ if_then(__ AndI(cache, __ LShiftI(one, src)), BoolTest::eq, zero, likely); {
1399          __ increment(i, targetCountLess1);
1400        }__ end_if();
1401        __ increment(i, one);
1402        __ bind(outer_loop);
1403   }__ end_loop(); __ dead(i);
1404   __ bind(return_);
1405 
1406   // Final sync IdealKit and GraphKit.
1407   final_sync(kit);
1408   Node* result = __ value(rtn);
1409 #undef __
1410   C-&gt;set_has_loops(true);
1411   return result;
1412 }
1413 
1414 //------------------------------inline_string_indexOf------------------------
1415 bool LibraryCallKit::inline_string_indexOf() {
1416   Node* receiver = argument(0);
1417   Node* arg      = argument(1);
1418 
1419   Node* result;
1420   // Disable the use of pcmpestri until it can be guaranteed that
1421   // the load doesn't cross into the uncommited space.
1422   if (Matcher::has_match_rule(Op_StrIndexOf) &amp;&amp;
1423       UseSSE42Intrinsics) {
1424     // Generate SSE4.2 version of indexOf
1425     // We currently only have match rules that use SSE4.2
1426 
1427     receiver = null_check(receiver);
1428     arg      = null_check(arg);
1429     if (stopped()) {
1430       return true;
1431     }
1432 
1433     ciInstanceKlass* str_klass = env()-&gt;String_klass();
1434     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(str_klass);
1435 
1436     // Make the merge point
1437     RegionNode* result_rgn = new (C) RegionNode(4);
1438     Node*       result_phi = new (C) PhiNode(result_rgn, TypeInt::INT);
1439     Node* no_ctrl  = NULL;
1440 
1441     // Get start addr of source string
1442     Node* source = load_String_value(no_ctrl, receiver);
1443     Node* source_offset = load_String_offset(no_ctrl, receiver);
1444     Node* source_start = array_element_address(source, source_offset, T_CHAR);
1445 
1446     // Get length of source string
1447     Node* source_cnt  = load_String_length(no_ctrl, receiver);
1448 
1449     // Get start addr of substring
1450     Node* substr = load_String_value(no_ctrl, arg);
1451     Node* substr_offset = load_String_offset(no_ctrl, arg);
1452     Node* substr_start = array_element_address(substr, substr_offset, T_CHAR);
1453 
1454     // Get length of source string
1455     Node* substr_cnt  = load_String_length(no_ctrl, arg);
1456 
1457     // Check for substr count &gt; string count
1458     Node* cmp = _gvn.transform(new(C) CmpINode(substr_cnt, source_cnt));
1459     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::gt));
1460     Node* if_gt = generate_slow_guard(bol, NULL);
1461     if (if_gt != NULL) {
1462       result_phi-&gt;init_req(2, intcon(-1));
1463       result_rgn-&gt;init_req(2, if_gt);
1464     }
1465 
1466     if (!stopped()) {
1467       // Check for substr count == 0
1468       cmp = _gvn.transform(new(C) CmpINode(substr_cnt, intcon(0)));
1469       bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::eq));
1470       Node* if_zero = generate_slow_guard(bol, NULL);
1471       if (if_zero != NULL) {
1472         result_phi-&gt;init_req(3, intcon(0));
1473         result_rgn-&gt;init_req(3, if_zero);
1474       }
1475     }
1476 
1477     if (!stopped()) {
1478       result = make_string_method_node(Op_StrIndexOf, source_start, source_cnt, substr_start, substr_cnt);
1479       result_phi-&gt;init_req(1, result);
1480       result_rgn-&gt;init_req(1, control());
1481     }
1482     set_control(_gvn.transform(result_rgn));
1483     record_for_igvn(result_rgn);
1484     result = _gvn.transform(result_phi);
1485 
1486   } else { // Use LibraryCallKit::string_indexOf
1487     // don't intrinsify if argument isn't a constant string.
1488     if (!arg-&gt;is_Con()) {
1489      return false;
1490     }
1491     const TypeOopPtr* str_type = _gvn.type(arg)-&gt;isa_oopptr();
1492     if (str_type == NULL) {
1493       return false;
1494     }
1495     ciInstanceKlass* klass = env()-&gt;String_klass();
1496     ciObject* str_const = str_type-&gt;const_oop();
1497     if (str_const == NULL || str_const-&gt;klass() != klass) {
1498       return false;
1499     }
1500     ciInstance* str = str_const-&gt;as_instance();
1501     assert(str != NULL, "must be instance");
1502 
1503     ciObject* v = str-&gt;field_value_by_offset(java_lang_String::value_offset_in_bytes()).as_object();
1504     ciTypeArray* pat = v-&gt;as_type_array(); // pattern (argument) character array
1505 
1506     int o;
1507     int c;
1508     if (java_lang_String::has_offset_field()) {
1509       o = str-&gt;field_value_by_offset(java_lang_String::offset_offset_in_bytes()).as_int();
1510       c = str-&gt;field_value_by_offset(java_lang_String::count_offset_in_bytes()).as_int();
1511     } else {
1512       o = 0;
1513       c = pat-&gt;length();
1514     }
1515 
1516     // constant strings have no offset and count == length which
1517     // simplifies the resulting code somewhat so lets optimize for that.
1518     if (o != 0 || c != pat-&gt;length()) {
1519      return false;
1520     }
1521 
1522     receiver = null_check(receiver, T_OBJECT);
1523     // NOTE: No null check on the argument is needed since it's a constant String oop.
1524     if (stopped()) {
1525       return true;
1526     }
1527 
1528     // The null string as a pattern always returns 0 (match at beginning of string)
1529     if (c == 0) {
1530       set_result(intcon(0));
1531       return true;
1532     }
1533 
1534     // Generate default indexOf
1535     jchar lastChar = pat-&gt;char_at(o + (c - 1));
1536     int cache = 0;
1537     int i;
1538     for (i = 0; i &lt; c - 1; i++) {
1539       assert(i &lt; pat-&gt;length(), "out of range");
1540       cache |= (1 &lt;&lt; (pat-&gt;char_at(o + i) &amp; (sizeof(cache) * BitsPerByte - 1)));
1541     }
1542 
1543     int md2 = c;
1544     for (i = 0; i &lt; c - 1; i++) {
1545       assert(i &lt; pat-&gt;length(), "out of range");
1546       if (pat-&gt;char_at(o + i) == lastChar) {
1547         md2 = (c - 1) - i;
1548       }
1549     }
1550 
1551     result = string_indexOf(receiver, pat, o, cache, md2);
1552   }
1553   set_result(result);
1554   return true;
1555 }
1556 
1557 //--------------------------round_double_node--------------------------------
1558 // Round a double node if necessary.
1559 Node* LibraryCallKit::round_double_node(Node* n) {
1560   if (Matcher::strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt;= 1)
1561     n = _gvn.transform(new (C) RoundDoubleNode(0, n));
1562   return n;
1563 }
1564 
1565 //------------------------------inline_math-----------------------------------
1566 // public static double Math.abs(double)
1567 // public static double Math.sqrt(double)
1568 // public static double Math.log(double)
1569 // public static double Math.log10(double)
1570 bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
1571   Node* arg = round_double_node(argument(0));
1572   Node* n;
1573   switch (id) {
1574   case vmIntrinsics::_dabs:   n = new (C) AbsDNode(                arg);  break;
1575   case vmIntrinsics::_dsqrt:  n = new (C) SqrtDNode(C, control(),  arg);  break;
1576   case vmIntrinsics::_dlog:   n = new (C) LogDNode(C, control(),   arg);  break;
1577   case vmIntrinsics::_dlog10: n = new (C) Log10DNode(C, control(), arg);  break;
1578   default:  fatal_unexpected_iid(id);  break;
1579   }
1580   set_result(_gvn.transform(n));
1581   return true;
1582 }
1583 
1584 //------------------------------inline_trig----------------------------------
1585 // Inline sin/cos/tan instructions, if possible.  If rounding is required, do
1586 // argument reduction which will turn into a fast/slow diamond.
1587 bool LibraryCallKit::inline_trig(vmIntrinsics::ID id) {
1588   Node* arg = round_double_node(argument(0));
1589   Node* n = NULL;
1590 
1591   switch (id) {
1592   case vmIntrinsics::_dsin:  n = new (C) SinDNode(C, control(), arg);  break;
1593   case vmIntrinsics::_dcos:  n = new (C) CosDNode(C, control(), arg);  break;
1594   case vmIntrinsics::_dtan:  n = new (C) TanDNode(C, control(), arg);  break;
1595   default:  fatal_unexpected_iid(id);  break;
1596   }
1597   n = _gvn.transform(n);
1598 
1599   // Rounding required?  Check for argument reduction!
1600   if (Matcher::strict_fp_requires_explicit_rounding) {
1601     static const double     pi_4 =  0.7853981633974483;
1602     static const double neg_pi_4 = -0.7853981633974483;
1603     // pi/2 in 80-bit extended precision
1604     // static const unsigned char pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0x3f,0x00,0x00,0x00,0x00,0x00,0x00};
1605     // -pi/2 in 80-bit extended precision
1606     // static const unsigned char neg_pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0xbf,0x00,0x00,0x00,0x00,0x00,0x00};
1607     // Cutoff value for using this argument reduction technique
1608     //static const double    pi_2_minus_epsilon =  1.564660403643354;
1609     //static const double neg_pi_2_plus_epsilon = -1.564660403643354;
1610 
1611     // Pseudocode for sin:
1612     // if (x &lt;= Math.PI / 4.0) {
1613     //   if (x &gt;= -Math.PI / 4.0) return  fsin(x);
1614     //   if (x &gt;= -Math.PI / 2.0) return -fcos(x + Math.PI / 2.0);
1615     // } else {
1616     //   if (x &lt;=  Math.PI / 2.0) return  fcos(x - Math.PI / 2.0);
1617     // }
1618     // return StrictMath.sin(x);
1619 
1620     // Pseudocode for cos:
1621     // if (x &lt;= Math.PI / 4.0) {
1622     //   if (x &gt;= -Math.PI / 4.0) return  fcos(x);
1623     //   if (x &gt;= -Math.PI / 2.0) return  fsin(x + Math.PI / 2.0);
1624     // } else {
1625     //   if (x &lt;=  Math.PI / 2.0) return -fsin(x - Math.PI / 2.0);
1626     // }
1627     // return StrictMath.cos(x);
1628 
1629     // Actually, sticking in an 80-bit Intel value into C2 will be tough; it
1630     // requires a special machine instruction to load it.  Instead we'll try
1631     // the 'easy' case.  If we really need the extra range +/- PI/2 we'll
1632     // probably do the math inside the SIN encoding.
1633 
1634     // Make the merge point
1635     RegionNode* r = new (C) RegionNode(3);
1636     Node* phi = new (C) PhiNode(r, Type::DOUBLE);
1637 
1638     // Flatten arg so we need only 1 test
1639     Node *abs = _gvn.transform(new (C) AbsDNode(arg));
1640     // Node for PI/4 constant
1641     Node *pi4 = makecon(TypeD::make(pi_4));
1642     // Check PI/4 : abs(arg)
1643     Node *cmp = _gvn.transform(new (C) CmpDNode(pi4,abs));
1644     // Check: If PI/4 &lt; abs(arg) then go slow
1645     Node *bol = _gvn.transform(new (C) BoolNode( cmp, BoolTest::lt ));
1646     // Branch either way
1647     IfNode *iff = create_and_xform_if(control(),bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1648     set_control(opt_iff(r,iff));
1649 
1650     // Set fast path result
1651     phi-&gt;init_req(2, n);
1652 
1653     // Slow path - non-blocking leaf call
1654     Node* call = NULL;
1655     switch (id) {
1656     case vmIntrinsics::_dsin:
1657       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1658                                CAST_FROM_FN_PTR(address, SharedRuntime::dsin),
1659                                "Sin", NULL, arg, top());
1660       break;
1661     case vmIntrinsics::_dcos:
1662       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1663                                CAST_FROM_FN_PTR(address, SharedRuntime::dcos),
1664                                "Cos", NULL, arg, top());
1665       break;
1666     case vmIntrinsics::_dtan:
1667       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1668                                CAST_FROM_FN_PTR(address, SharedRuntime::dtan),
1669                                "Tan", NULL, arg, top());
1670       break;
1671     }
1672     assert(control()-&gt;in(0) == call, "");
1673     Node* slow_result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
1674     r-&gt;init_req(1, control());
1675     phi-&gt;init_req(1, slow_result);
1676 
1677     // Post-merge
1678     set_control(_gvn.transform(r));
1679     record_for_igvn(r);
1680     n = _gvn.transform(phi);
1681 
1682     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1683   }
1684   set_result(n);
1685   return true;
1686 }
1687 
1688 void LibraryCallKit::finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName) {
1689   //-------------------
1690   //result=(result.isNaN())? funcAddr():result;
1691   // Check: If isNaN() by checking result!=result? then either trap
1692   // or go to runtime
1693   Node* cmpisnan = _gvn.transform(new (C) CmpDNode(result, result));
1694   // Build the boolean node
1695   Node* bolisnum = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::eq));
1696 
1697   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1698     { BuildCutout unless(this, bolisnum, PROB_STATIC_FREQUENT);
1699       // The pow or exp intrinsic returned a NaN, which requires a call
1700       // to the runtime.  Recompile with the runtime call.
1701       uncommon_trap(Deoptimization::Reason_intrinsic,
1702                     Deoptimization::Action_make_not_entrant);
1703     }
1704     set_result(result);
1705   } else {
1706     // If this inlining ever returned NaN in the past, we compile a call
1707     // to the runtime to properly handle corner cases
1708 
1709     IfNode* iff = create_and_xform_if(control(), bolisnum, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1710     Node* if_slow = _gvn.transform(new (C) IfFalseNode(iff));
1711     Node* if_fast = _gvn.transform(new (C) IfTrueNode(iff));
1712 
1713     if (!if_slow-&gt;is_top()) {
1714       RegionNode* result_region = new (C) RegionNode(3);
1715       PhiNode*    result_val = new (C) PhiNode(result_region, Type::DOUBLE);
1716 
1717       result_region-&gt;init_req(1, if_fast);
1718       result_val-&gt;init_req(1, result);
1719 
1720       set_control(if_slow);
1721 
1722       const TypePtr* no_memory_effects = NULL;
1723       Node* rt = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1724                                    no_memory_effects,
1725                                    x, top(), y, y ? top() : NULL);
1726       Node* value = _gvn.transform(new (C) ProjNode(rt, TypeFunc::Parms+0));
1727 #ifdef ASSERT
1728       Node* value_top = _gvn.transform(new (C) ProjNode(rt, TypeFunc::Parms+1));
1729       assert(value_top == top(), "second value must be top");
1730 #endif
1731 
1732       result_region-&gt;init_req(2, control());
1733       result_val-&gt;init_req(2, value);
1734       set_result(result_region, result_val);
1735     } else {
1736       set_result(result);
1737     }
1738   }
1739 }
1740 
1741 //------------------------------inline_exp-------------------------------------
1742 // Inline exp instructions, if possible.  The Intel hardware only misses
1743 // really odd corner cases (+/- Infinity).  Just uncommon-trap them.
1744 bool LibraryCallKit::inline_exp() {
1745   Node* arg = round_double_node(argument(0));
1746   Node* n   = _gvn.transform(new (C) ExpDNode(C, control(), arg));
1747 
1748   finish_pow_exp(n, arg, NULL, OptoRuntime::Math_D_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dexp), "EXP");
1749 
1750   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1751   return true;
1752 }
1753 
1754 //------------------------------inline_pow-------------------------------------
1755 // Inline power instructions, if possible.
1756 bool LibraryCallKit::inline_pow() {
1757   // Pseudocode for pow
1758   // if (x &lt;= 0.0) {
1759   //   long longy = (long)y;
1760   //   if ((double)longy == y) { // if y is long
1761   //     if (y + 1 == y) longy = 0; // huge number: even
1762   //     result = ((1&amp;longy) == 0)?-DPow(abs(x), y):DPow(abs(x), y);
1763   //   } else {
1764   //     result = NaN;
1765   //   }
1766   // } else {
1767   //   result = DPow(x,y);
1768   // }
1769   // if (result != result)?  {
1770   //   result = uncommon_trap() or runtime_call();
1771   // }
1772   // return result;
1773 
1774   Node* x = round_double_node(argument(0));
1775   Node* y = round_double_node(argument(2));
1776 
1777   Node* result = NULL;
1778 
1779   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1780     // Short form: skip the fancy tests and just check for NaN result.
1781     result = _gvn.transform(new (C) PowDNode(C, control(), x, y));
1782   } else {
1783     // If this inlining ever returned NaN in the past, include all
1784     // checks + call to the runtime.
1785 
1786     // Set the merge point for If node with condition of (x &lt;= 0.0)
1787     // There are four possible paths to region node and phi node
1788     RegionNode *r = new (C) RegionNode(4);
1789     Node *phi = new (C) PhiNode(r, Type::DOUBLE);
1790 
1791     // Build the first if node: if (x &lt;= 0.0)
1792     // Node for 0 constant
1793     Node *zeronode = makecon(TypeD::ZERO);
1794     // Check x:0
1795     Node *cmp = _gvn.transform(new (C) CmpDNode(x, zeronode));
1796     // Check: If (x&lt;=0) then go complex path
1797     Node *bol1 = _gvn.transform(new (C) BoolNode( cmp, BoolTest::le ));
1798     // Branch either way
1799     IfNode *if1 = create_and_xform_if(control(),bol1, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1800     // Fast path taken; set region slot 3
1801     Node *fast_taken = _gvn.transform(new (C) IfFalseNode(if1));
1802     r-&gt;init_req(3,fast_taken); // Capture fast-control
1803 
1804     // Fast path not-taken, i.e. slow path
1805     Node *complex_path = _gvn.transform(new (C) IfTrueNode(if1));
1806 
1807     // Set fast path result
1808     Node *fast_result = _gvn.transform(new (C) PowDNode(C, control(), x, y));
1809     phi-&gt;init_req(3, fast_result);
1810 
1811     // Complex path
1812     // Build the second if node (if y is long)
1813     // Node for (long)y
1814     Node *longy = _gvn.transform(new (C) ConvD2LNode(y));
1815     // Node for (double)((long) y)
1816     Node *doublelongy= _gvn.transform(new (C) ConvL2DNode(longy));
1817     // Check (double)((long) y) : y
1818     Node *cmplongy= _gvn.transform(new (C) CmpDNode(doublelongy, y));
1819     // Check if (y isn't long) then go to slow path
1820 
1821     Node *bol2 = _gvn.transform(new (C) BoolNode( cmplongy, BoolTest::ne ));
1822     // Branch either way
1823     IfNode *if2 = create_and_xform_if(complex_path,bol2, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1824     Node* ylong_path = _gvn.transform(new (C) IfFalseNode(if2));
1825 
1826     Node *slow_path = _gvn.transform(new (C) IfTrueNode(if2));
1827 
1828     // Calculate DPow(abs(x), y)*(1 &amp; (long)y)
1829     // Node for constant 1
1830     Node *conone = longcon(1);
1831     // 1&amp; (long)y
1832     Node *signnode= _gvn.transform(new (C) AndLNode(conone, longy));
1833 
1834     // A huge number is always even. Detect a huge number by checking
1835     // if y + 1 == y and set integer to be tested for parity to 0.
1836     // Required for corner case:
1837     // (long)9.223372036854776E18 = max_jlong
1838     // (double)(long)9.223372036854776E18 = 9.223372036854776E18
1839     // max_jlong is odd but 9.223372036854776E18 is even
1840     Node* yplus1 = _gvn.transform(new (C) AddDNode(y, makecon(TypeD::make(1))));
1841     Node *cmpyplus1= _gvn.transform(new (C) CmpDNode(yplus1, y));
1842     Node *bolyplus1 = _gvn.transform(new (C) BoolNode( cmpyplus1, BoolTest::eq ));
1843     Node* correctedsign = NULL;
1844     if (ConditionalMoveLimit != 0) {
1845       correctedsign = _gvn.transform( CMoveNode::make(C, NULL, bolyplus1, signnode, longcon(0), TypeLong::LONG));
1846     } else {
1847       IfNode *ifyplus1 = create_and_xform_if(ylong_path,bolyplus1, PROB_FAIR, COUNT_UNKNOWN);
1848       RegionNode *r = new (C) RegionNode(3);
1849       Node *phi = new (C) PhiNode(r, TypeLong::LONG);
1850       r-&gt;init_req(1, _gvn.transform(new (C) IfFalseNode(ifyplus1)));
1851       r-&gt;init_req(2, _gvn.transform(new (C) IfTrueNode(ifyplus1)));
1852       phi-&gt;init_req(1, signnode);
1853       phi-&gt;init_req(2, longcon(0));
1854       correctedsign = _gvn.transform(phi);
1855       ylong_path = _gvn.transform(r);
1856       record_for_igvn(r);
1857     }
1858 
1859     // zero node
1860     Node *conzero = longcon(0);
1861     // Check (1&amp;(long)y)==0?
1862     Node *cmpeq1 = _gvn.transform(new (C) CmpLNode(correctedsign, conzero));
1863     // Check if (1&amp;(long)y)!=0?, if so the result is negative
1864     Node *bol3 = _gvn.transform(new (C) BoolNode( cmpeq1, BoolTest::ne ));
1865     // abs(x)
1866     Node *absx=_gvn.transform(new (C) AbsDNode(x));
1867     // abs(x)^y
1868     Node *absxpowy = _gvn.transform(new (C) PowDNode(C, control(), absx, y));
1869     // -abs(x)^y
1870     Node *negabsxpowy = _gvn.transform(new (C) NegDNode (absxpowy));
1871     // (1&amp;(long)y)==1?-DPow(abs(x), y):DPow(abs(x), y)
1872     Node *signresult = NULL;
1873     if (ConditionalMoveLimit != 0) {
1874       signresult = _gvn.transform( CMoveNode::make(C, NULL, bol3, absxpowy, negabsxpowy, Type::DOUBLE));
1875     } else {
1876       IfNode *ifyeven = create_and_xform_if(ylong_path,bol3, PROB_FAIR, COUNT_UNKNOWN);
1877       RegionNode *r = new (C) RegionNode(3);
1878       Node *phi = new (C) PhiNode(r, Type::DOUBLE);
1879       r-&gt;init_req(1, _gvn.transform(new (C) IfFalseNode(ifyeven)));
1880       r-&gt;init_req(2, _gvn.transform(new (C) IfTrueNode(ifyeven)));
1881       phi-&gt;init_req(1, absxpowy);
1882       phi-&gt;init_req(2, negabsxpowy);
1883       signresult = _gvn.transform(phi);
1884       ylong_path = _gvn.transform(r);
1885       record_for_igvn(r);
1886     }
1887     // Set complex path fast result
1888     r-&gt;init_req(2, ylong_path);
1889     phi-&gt;init_req(2, signresult);
1890 
1891     static const jlong nan_bits = CONST64(0x7ff8000000000000);
1892     Node *slow_result = makecon(TypeD::make(*(double*)&amp;nan_bits)); // return NaN
1893     r-&gt;init_req(1,slow_path);
1894     phi-&gt;init_req(1,slow_result);
1895 
1896     // Post merge
1897     set_control(_gvn.transform(r));
1898     record_for_igvn(r);
1899     result = _gvn.transform(phi);
1900   }
1901 
1902   finish_pow_exp(result, x, y, OptoRuntime::Math_DD_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dpow), "POW");
1903 
1904   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1905   return true;
1906 }
1907 
1908 //------------------------------runtime_math-----------------------------
1909 bool LibraryCallKit::runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName) {
1910   assert(call_type == OptoRuntime::Math_DD_D_Type() || call_type == OptoRuntime::Math_D_D_Type(),
1911          "must be (DD)D or (D)D type");
1912 
1913   // Inputs
1914   Node* a = round_double_node(argument(0));
1915   Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : NULL;
1916 
1917   const TypePtr* no_memory_effects = NULL;
1918   Node* trig = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1919                                  no_memory_effects,
1920                                  a, top(), b, b ? top() : NULL);
1921   Node* value = _gvn.transform(new (C) ProjNode(trig, TypeFunc::Parms+0));
1922 #ifdef ASSERT
1923   Node* value_top = _gvn.transform(new (C) ProjNode(trig, TypeFunc::Parms+1));
1924   assert(value_top == top(), "second value must be top");
1925 #endif
1926 
1927   set_result(value);
1928   return true;
1929 }
1930 
1931 //------------------------------inline_math_native-----------------------------
1932 bool LibraryCallKit::inline_math_native(vmIntrinsics::ID id) {
1933 #define FN_PTR(f) CAST_FROM_FN_PTR(address, f)
1934   switch (id) {
1935     // These intrinsics are not properly supported on all hardware
1936   case vmIntrinsics::_dcos:   return Matcher::has_match_rule(Op_CosD)   ? inline_trig(id) :
1937     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dcos),   "COS");
1938   case vmIntrinsics::_dsin:   return Matcher::has_match_rule(Op_SinD)   ? inline_trig(id) :
1939     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dsin),   "SIN");
1940   case vmIntrinsics::_dtan:   return Matcher::has_match_rule(Op_TanD)   ? inline_trig(id) :
1941     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dtan),   "TAN");
1942 
1943   case vmIntrinsics::_dlog:   return Matcher::has_match_rule(Op_LogD)   ? inline_math(id) :
1944     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog),   "LOG");
1945   case vmIntrinsics::_dlog10: return Matcher::has_match_rule(Op_Log10D) ? inline_math(id) :
1946     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog10), "LOG10");
1947 
1948     // These intrinsics are supported on all hardware
1949   case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_math(id) : false;
1950   case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_math(id) : false;
1951 
1952   case vmIntrinsics::_dexp:   return Matcher::has_match_rule(Op_ExpD)   ? inline_exp()    :
1953     runtime_math(OptoRuntime::Math_D_D_Type(),  FN_PTR(SharedRuntime::dexp),  "EXP");
1954   case vmIntrinsics::_dpow:   return Matcher::has_match_rule(Op_PowD)   ? inline_pow()    :
1955     runtime_math(OptoRuntime::Math_DD_D_Type(), FN_PTR(SharedRuntime::dpow),  "POW");
1956 #undef FN_PTR
1957 
1958    // These intrinsics are not yet correctly implemented
1959   case vmIntrinsics::_datan2:
1960     return false;
1961 
1962   default:
1963     fatal_unexpected_iid(id);
1964     return false;
1965   }
1966 }
1967 
1968 static bool is_simple_name(Node* n) {
1969   return (n-&gt;req() == 1         // constant
1970           || (n-&gt;is_Type() &amp;&amp; n-&gt;as_Type()-&gt;type()-&gt;singleton())
1971           || n-&gt;is_Proj()       // parameter or return value
1972           || n-&gt;is_Phi()        // local of some sort
1973           );
1974 }
1975 
1976 //----------------------------inline_min_max-----------------------------------
1977 bool LibraryCallKit::inline_min_max(vmIntrinsics::ID id) {
1978   set_result(generate_min_max(id, argument(0), argument(1)));
1979   return true;
1980 }
1981 
1982 void LibraryCallKit::inline_math_mathExact(Node* math, Node *test) {
1983   Node* bol = _gvn.transform( new (C) BoolNode(test, BoolTest::overflow) );
1984   IfNode* check = create_and_map_if(control(), bol, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
1985   Node* fast_path = _gvn.transform( new (C) IfFalseNode(check));
1986   Node* slow_path = _gvn.transform( new (C) IfTrueNode(check) );
1987 
1988   {
1989     PreserveJVMState pjvms(this);
1990     PreserveReexecuteState preexecs(this);
1991     jvms()-&gt;set_should_reexecute(true);
1992 
1993     set_control(slow_path);
1994     set_i_o(i_o());
1995 
1996     uncommon_trap(Deoptimization::Reason_intrinsic,
1997                   Deoptimization::Action_none);
1998   }
1999 
2000   set_control(fast_path);
2001   set_result(math);
2002 }
2003 
2004 template &lt;typename OverflowOp&gt;
2005 bool LibraryCallKit::inline_math_overflow(Node* arg1, Node* arg2) {
2006   typedef typename OverflowOp::MathOp MathOp;
2007 
2008   MathOp* mathOp = new(C) MathOp(arg1, arg2);
2009   Node* operation = _gvn.transform( mathOp );
2010   Node* ofcheck = _gvn.transform( new(C) OverflowOp(arg1, arg2) );
2011   inline_math_mathExact(operation, ofcheck);
2012   return true;
2013 }
2014 
2015 bool LibraryCallKit::inline_math_addExactI(bool is_increment) {
2016   return inline_math_overflow&lt;OverflowAddINode&gt;(argument(0), is_increment ? intcon(1) : argument(1));
2017 }
2018 
2019 bool LibraryCallKit::inline_math_addExactL(bool is_increment) {
2020   return inline_math_overflow&lt;OverflowAddLNode&gt;(argument(0), is_increment ? longcon(1) : argument(2));
2021 }
2022 
2023 bool LibraryCallKit::inline_math_subtractExactI(bool is_decrement) {
2024   return inline_math_overflow&lt;OverflowSubINode&gt;(argument(0), is_decrement ? intcon(1) : argument(1));
2025 }
2026 
2027 bool LibraryCallKit::inline_math_subtractExactL(bool is_decrement) {
2028   return inline_math_overflow&lt;OverflowSubLNode&gt;(argument(0), is_decrement ? longcon(1) : argument(2));
2029 }
2030 
2031 bool LibraryCallKit::inline_math_negateExactI() {
2032   return inline_math_overflow&lt;OverflowSubINode&gt;(intcon(0), argument(0));
2033 }
2034 
2035 bool LibraryCallKit::inline_math_negateExactL() {
2036   return inline_math_overflow&lt;OverflowSubLNode&gt;(longcon(0), argument(0));
2037 }
2038 
2039 bool LibraryCallKit::inline_math_multiplyExactI() {
2040   return inline_math_overflow&lt;OverflowMulINode&gt;(argument(0), argument(1));
2041 }
2042 
2043 bool LibraryCallKit::inline_math_multiplyExactL() {
2044   return inline_math_overflow&lt;OverflowMulLNode&gt;(argument(0), argument(2));
2045 }
2046 
2047 Node*
2048 LibraryCallKit::generate_min_max(vmIntrinsics::ID id, Node* x0, Node* y0) {
2049   // These are the candidate return value:
2050   Node* xvalue = x0;
2051   Node* yvalue = y0;
2052 
2053   if (xvalue == yvalue) {
2054     return xvalue;
2055   }
2056 
2057   bool want_max = (id == vmIntrinsics::_max);
2058 
2059   const TypeInt* txvalue = _gvn.type(xvalue)-&gt;isa_int();
2060   const TypeInt* tyvalue = _gvn.type(yvalue)-&gt;isa_int();
2061   if (txvalue == NULL || tyvalue == NULL)  return top();
2062   // This is not really necessary, but it is consistent with a
2063   // hypothetical MaxINode::Value method:
2064   int widen = MAX2(txvalue-&gt;_widen, tyvalue-&gt;_widen);
2065 
2066   // %%% This folding logic should (ideally) be in a different place.
2067   // Some should be inside IfNode, and there to be a more reliable
2068   // transformation of ?: style patterns into cmoves.  We also want
2069   // more powerful optimizations around cmove and min/max.
2070 
2071   // Try to find a dominating comparison of these guys.
2072   // It can simplify the index computation for Arrays.copyOf
2073   // and similar uses of System.arraycopy.
2074   // First, compute the normalized version of CmpI(x, y).
2075   int   cmp_op = Op_CmpI;
2076   Node* xkey = xvalue;
2077   Node* ykey = yvalue;
2078   Node* ideal_cmpxy = _gvn.transform(new(C) CmpINode(xkey, ykey));
2079   if (ideal_cmpxy-&gt;is_Cmp()) {
2080     // E.g., if we have CmpI(length - offset, count),
2081     // it might idealize to CmpI(length, count + offset)
2082     cmp_op = ideal_cmpxy-&gt;Opcode();
2083     xkey = ideal_cmpxy-&gt;in(1);
2084     ykey = ideal_cmpxy-&gt;in(2);
2085   }
2086 
2087   // Start by locating any relevant comparisons.
2088   Node* start_from = (xkey-&gt;outcnt() &lt; ykey-&gt;outcnt()) ? xkey : ykey;
2089   Node* cmpxy = NULL;
2090   Node* cmpyx = NULL;
2091   for (DUIterator_Fast kmax, k = start_from-&gt;fast_outs(kmax); k &lt; kmax; k++) {
2092     Node* cmp = start_from-&gt;fast_out(k);
2093     if (cmp-&gt;outcnt() &gt; 0 &amp;&amp;            // must have prior uses
2094         cmp-&gt;in(0) == NULL &amp;&amp;           // must be context-independent
2095         cmp-&gt;Opcode() == cmp_op) {      // right kind of compare
2096       if (cmp-&gt;in(1) == xkey &amp;&amp; cmp-&gt;in(2) == ykey)  cmpxy = cmp;
2097       if (cmp-&gt;in(1) == ykey &amp;&amp; cmp-&gt;in(2) == xkey)  cmpyx = cmp;
2098     }
2099   }
2100 
2101   const int NCMPS = 2;
2102   Node* cmps[NCMPS] = { cmpxy, cmpyx };
2103   int cmpn;
2104   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2105     if (cmps[cmpn] != NULL)  break;     // find a result
2106   }
2107   if (cmpn &lt; NCMPS) {
2108     // Look for a dominating test that tells us the min and max.
2109     int depth = 0;                // Limit search depth for speed
2110     Node* dom = control();
2111     for (; dom != NULL; dom = IfNode::up_one_dom(dom, true)) {
2112       if (++depth &gt;= 100)  break;
2113       Node* ifproj = dom;
2114       if (!ifproj-&gt;is_Proj())  continue;
2115       Node* iff = ifproj-&gt;in(0);
2116       if (!iff-&gt;is_If())  continue;
2117       Node* bol = iff-&gt;in(1);
2118       if (!bol-&gt;is_Bool())  continue;
2119       Node* cmp = bol-&gt;in(1);
2120       if (cmp == NULL)  continue;
2121       for (cmpn = 0; cmpn &lt; NCMPS; cmpn++)
2122         if (cmps[cmpn] == cmp)  break;
2123       if (cmpn == NCMPS)  continue;
2124       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2125       if (ifproj-&gt;is_IfFalse())  btest = BoolTest(btest).negate();
2126       if (cmp-&gt;in(1) == ykey)    btest = BoolTest(btest).commute();
2127       // At this point, we know that 'x btest y' is true.
2128       switch (btest) {
2129       case BoolTest::eq:
2130         // They are proven equal, so we can collapse the min/max.
2131         // Either value is the answer.  Choose the simpler.
2132         if (is_simple_name(yvalue) &amp;&amp; !is_simple_name(xvalue))
2133           return yvalue;
2134         return xvalue;
2135       case BoolTest::lt:          // x &lt; y
2136       case BoolTest::le:          // x &lt;= y
2137         return (want_max ? yvalue : xvalue);
2138       case BoolTest::gt:          // x &gt; y
2139       case BoolTest::ge:          // x &gt;= y
2140         return (want_max ? xvalue : yvalue);
2141       }
2142     }
2143   }
2144 
2145   // We failed to find a dominating test.
2146   // Let's pick a test that might GVN with prior tests.
2147   Node*          best_bol   = NULL;
2148   BoolTest::mask best_btest = BoolTest::illegal;
2149   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2150     Node* cmp = cmps[cmpn];
2151     if (cmp == NULL)  continue;
2152     for (DUIterator_Fast jmax, j = cmp-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2153       Node* bol = cmp-&gt;fast_out(j);
2154       if (!bol-&gt;is_Bool())  continue;
2155       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2156       if (btest == BoolTest::eq || btest == BoolTest::ne)  continue;
2157       if (cmp-&gt;in(1) == ykey)   btest = BoolTest(btest).commute();
2158       if (bol-&gt;outcnt() &gt; (best_bol == NULL ? 0 : best_bol-&gt;outcnt())) {
2159         best_bol   = bol-&gt;as_Bool();
2160         best_btest = btest;
2161       }
2162     }
2163   }
2164 
2165   Node* answer_if_true  = NULL;
2166   Node* answer_if_false = NULL;
2167   switch (best_btest) {
2168   default:
2169     if (cmpxy == NULL)
2170       cmpxy = ideal_cmpxy;
2171     best_bol = _gvn.transform(new(C) BoolNode(cmpxy, BoolTest::lt));
2172     // and fall through:
2173   case BoolTest::lt:          // x &lt; y
2174   case BoolTest::le:          // x &lt;= y
2175     answer_if_true  = (want_max ? yvalue : xvalue);
2176     answer_if_false = (want_max ? xvalue : yvalue);
2177     break;
2178   case BoolTest::gt:          // x &gt; y
2179   case BoolTest::ge:          // x &gt;= y
2180     answer_if_true  = (want_max ? xvalue : yvalue);
2181     answer_if_false = (want_max ? yvalue : xvalue);
2182     break;
2183   }
2184 
2185   jint hi, lo;
2186   if (want_max) {
2187     // We can sharpen the minimum.
2188     hi = MAX2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2189     lo = MAX2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2190   } else {
2191     // We can sharpen the maximum.
2192     hi = MIN2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2193     lo = MIN2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2194   }
2195 
2196   // Use a flow-free graph structure, to avoid creating excess control edges
2197   // which could hinder other optimizations.
2198   // Since Math.min/max is often used with arraycopy, we want
2199   // tightly_coupled_allocation to be able to see beyond min/max expressions.
2200   Node* cmov = CMoveNode::make(C, NULL, best_bol,
2201                                answer_if_false, answer_if_true,
2202                                TypeInt::make(lo, hi, widen));
2203 
2204   return _gvn.transform(cmov);
2205 
2206   /*
2207   // This is not as desirable as it may seem, since Min and Max
2208   // nodes do not have a full set of optimizations.
2209   // And they would interfere, anyway, with 'if' optimizations
2210   // and with CMoveI canonical forms.
2211   switch (id) {
2212   case vmIntrinsics::_min:
2213     result_val = _gvn.transform(new (C, 3) MinINode(x,y)); break;
2214   case vmIntrinsics::_max:
2215     result_val = _gvn.transform(new (C, 3) MaxINode(x,y)); break;
2216   default:
2217     ShouldNotReachHere();
2218   }
2219   */
2220 }
2221 
2222 inline int
2223 LibraryCallKit::classify_unsafe_addr(Node* &amp;base, Node* &amp;offset) {
2224   const TypePtr* base_type = TypePtr::NULL_PTR;
2225   if (base != NULL)  base_type = _gvn.type(base)-&gt;isa_ptr();
2226   if (base_type == NULL) {
2227     // Unknown type.
2228     return Type::AnyPtr;
2229   } else if (base_type == TypePtr::NULL_PTR) {
2230     // Since this is a NULL+long form, we have to switch to a rawptr.
2231     base   = _gvn.transform(new (C) CastX2PNode(offset));
2232     offset = MakeConX(0);
2233     return Type::RawPtr;
2234   } else if (base_type-&gt;base() == Type::RawPtr) {
2235     return Type::RawPtr;
2236   } else if (base_type-&gt;isa_oopptr()) {
2237     // Base is never null =&gt; always a heap address.
2238     if (base_type-&gt;ptr() == TypePtr::NotNull) {
2239       return Type::OopPtr;
2240     }
2241     // Offset is small =&gt; always a heap address.
2242     const TypeX* offset_type = _gvn.type(offset)-&gt;isa_intptr_t();
2243     if (offset_type != NULL &amp;&amp;
2244         base_type-&gt;offset() == 0 &amp;&amp;     // (should always be?)
2245         offset_type-&gt;_lo &gt;= 0 &amp;&amp;
2246         !MacroAssembler::needs_explicit_null_check(offset_type-&gt;_hi)) {
2247       return Type::OopPtr;
2248     }
2249     // Otherwise, it might either be oop+off or NULL+addr.
2250     return Type::AnyPtr;
2251   } else {
2252     // No information:
2253     return Type::AnyPtr;
2254   }
2255 }
2256 
2257 inline Node* LibraryCallKit::make_unsafe_address(Node* base, Node* offset) {
2258   int kind = classify_unsafe_addr(base, offset);
2259   if (kind == Type::RawPtr) {
2260     return basic_plus_adr(top(), base, offset);
2261   } else {
2262     return basic_plus_adr(base, offset);
2263   }
2264 }
2265 
2266 //--------------------------inline_number_methods-----------------------------
2267 // inline int     Integer.numberOfLeadingZeros(int)
2268 // inline int        Long.numberOfLeadingZeros(long)
2269 //
2270 // inline int     Integer.numberOfTrailingZeros(int)
2271 // inline int        Long.numberOfTrailingZeros(long)
2272 //
2273 // inline int     Integer.bitCount(int)
2274 // inline int        Long.bitCount(long)
2275 //
2276 // inline char  Character.reverseBytes(char)
2277 // inline short     Short.reverseBytes(short)
2278 // inline int     Integer.reverseBytes(int)
2279 // inline long       Long.reverseBytes(long)
2280 bool LibraryCallKit::inline_number_methods(vmIntrinsics::ID id) {
2281   Node* arg = argument(0);
2282   Node* n;
2283   switch (id) {
2284   case vmIntrinsics::_numberOfLeadingZeros_i:   n = new (C) CountLeadingZerosINode( arg);  break;
2285   case vmIntrinsics::_numberOfLeadingZeros_l:   n = new (C) CountLeadingZerosLNode( arg);  break;
2286   case vmIntrinsics::_numberOfTrailingZeros_i:  n = new (C) CountTrailingZerosINode(arg);  break;
2287   case vmIntrinsics::_numberOfTrailingZeros_l:  n = new (C) CountTrailingZerosLNode(arg);  break;
2288   case vmIntrinsics::_bitCount_i:               n = new (C) PopCountINode(          arg);  break;
2289   case vmIntrinsics::_bitCount_l:               n = new (C) PopCountLNode(          arg);  break;
2290   case vmIntrinsics::_reverseBytes_c:           n = new (C) ReverseBytesUSNode(0,   arg);  break;
2291   case vmIntrinsics::_reverseBytes_s:           n = new (C) ReverseBytesSNode( 0,   arg);  break;
2292   case vmIntrinsics::_reverseBytes_i:           n = new (C) ReverseBytesINode( 0,   arg);  break;
2293   case vmIntrinsics::_reverseBytes_l:           n = new (C) ReverseBytesLNode( 0,   arg);  break;
2294   default:  fatal_unexpected_iid(id);  break;
2295   }
2296   set_result(_gvn.transform(n));
2297   return true;
2298 }
2299 
2300 //----------------------------inline_unsafe_access----------------------------
2301 
2302 const static BasicType T_ADDRESS_HOLDER = T_LONG;
2303 
2304 // Helper that guards and inserts a pre-barrier.
2305 void LibraryCallKit::insert_pre_barrier(Node* base_oop, Node* offset,
2306                                         Node* pre_val, bool need_mem_bar) {
2307   // We could be accessing the referent field of a reference object. If so, when G1
2308   // is enabled, we need to log the value in the referent field in an SATB buffer.
2309   // This routine performs some compile time filters and generates suitable
2310   // runtime filters that guard the pre-barrier code.
2311   // Also add memory barrier for non volatile load from the referent field
2312   // to prevent commoning of loads across safepoint.
2313   if (!UseG1GC &amp;&amp; !need_mem_bar)
2314     return;
2315 
2316   // Some compile time checks.
2317 
2318   // If offset is a constant, is it java_lang_ref_Reference::_reference_offset?
2319   const TypeX* otype = offset-&gt;find_intptr_t_type();
2320   if (otype != NULL &amp;&amp; otype-&gt;is_con() &amp;&amp;
2321       otype-&gt;get_con() != java_lang_ref_Reference::referent_offset) {
2322     // Constant offset but not the reference_offset so just return
2323     return;
2324   }
2325 
2326   // We only need to generate the runtime guards for instances.
2327   const TypeOopPtr* btype = base_oop-&gt;bottom_type()-&gt;isa_oopptr();
2328   if (btype != NULL) {
2329     if (btype-&gt;isa_aryptr()) {
2330       // Array type so nothing to do
2331       return;
2332     }
2333 
2334     const TypeInstPtr* itype = btype-&gt;isa_instptr();
2335     if (itype != NULL) {
2336       // Can the klass of base_oop be statically determined to be
2337       // _not_ a sub-class of Reference and _not_ Object?
2338       ciKlass* klass = itype-&gt;klass();
2339       if ( klass-&gt;is_loaded() &amp;&amp;
2340           !klass-&gt;is_subtype_of(env()-&gt;Reference_klass()) &amp;&amp;
2341           !env()-&gt;Object_klass()-&gt;is_subtype_of(klass)) {
2342         return;
2343       }
2344     }
2345   }
2346 
2347   // The compile time filters did not reject base_oop/offset so
2348   // we need to generate the following runtime filters
2349   //
2350   // if (offset == java_lang_ref_Reference::_reference_offset) {
2351   //   if (instance_of(base, java.lang.ref.Reference)) {
2352   //     pre_barrier(_, pre_val, ...);
2353   //   }
2354   // }
2355 
2356   float likely   = PROB_LIKELY(  0.999);
2357   float unlikely = PROB_UNLIKELY(0.999);
2358 
2359   IdealKit ideal(this);
2360 #define __ ideal.
2361 
2362   Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset);
2363 
2364   __ if_then(offset, BoolTest::eq, referent_off, unlikely); {
2365       // Update graphKit memory and control from IdealKit.
2366       sync_kit(ideal);
2367 
2368       Node* ref_klass_con = makecon(TypeKlassPtr::make(env()-&gt;Reference_klass()));
2369       Node* is_instof = gen_instanceof(base_oop, ref_klass_con);
2370 
2371       // Update IdealKit memory and control from graphKit.
2372       __ sync_kit(this);
2373 
2374       Node* one = __ ConI(1);
2375       // is_instof == 0 if base_oop == NULL
2376       __ if_then(is_instof, BoolTest::eq, one, unlikely); {
2377 
2378         // Update graphKit from IdeakKit.
2379         sync_kit(ideal);
2380 
2381         // Use the pre-barrier to record the value in the referent field
2382         pre_barrier(false /* do_load */,
2383                     __ ctrl(),
2384                     NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
2385                     pre_val /* pre_val */,
2386                     T_OBJECT);
2387         if (need_mem_bar) {
2388           // Add memory barrier to prevent commoning reads from this field
2389           // across safepoint since GC can change its value.
2390           insert_mem_bar(Op_MemBarCPUOrder);
2391         }
2392         // Update IdealKit from graphKit.
2393         __ sync_kit(this);
2394 
2395       } __ end_if(); // _ref_type != ref_none
2396   } __ end_if(); // offset == referent_offset
2397 
2398   // Final sync IdealKit and GraphKit.
2399   final_sync(ideal);
2400 #undef __
2401 }
2402 
2403 
2404 // Interpret Unsafe.fieldOffset cookies correctly:
2405 extern jlong Unsafe_field_offset_to_byte_offset(jlong field_offset);
2406 
2407 const TypeOopPtr* LibraryCallKit::sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr) {
2408   // Attempt to infer a sharper value type from the offset and base type.
2409   ciKlass* sharpened_klass = NULL;
2410 
2411   // See if it is an instance field, with an object type.
2412   if (alias_type-&gt;field() != NULL) {
2413     assert(!is_native_ptr, "native pointer op cannot use a java address");
2414     if (alias_type-&gt;field()-&gt;type()-&gt;is_klass()) {
2415       sharpened_klass = alias_type-&gt;field()-&gt;type()-&gt;as_klass();
2416     }
2417   }
2418 
2419   // See if it is a narrow oop array.
2420   if (adr_type-&gt;isa_aryptr()) {
2421     if (adr_type-&gt;offset() &gt;= objArrayOopDesc::base_offset_in_bytes()) {
2422       const TypeOopPtr *elem_type = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_oopptr();
2423       if (elem_type != NULL) {
2424         sharpened_klass = elem_type-&gt;klass();
2425       }
2426     }
2427   }
2428 
2429   // The sharpened class might be unloaded if there is no class loader
2430   // contraint in place.
2431   if (sharpened_klass != NULL &amp;&amp; sharpened_klass-&gt;is_loaded()) {
2432     const TypeOopPtr* tjp = TypeOopPtr::make_from_klass(sharpened_klass);
2433 
2434 #ifndef PRODUCT
2435     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
2436       tty-&gt;print("  from base type: ");  adr_type-&gt;dump();
2437       tty-&gt;print("  sharpened value: ");  tjp-&gt;dump();
2438     }
2439 #endif
2440     // Sharpen the value type.
2441     return tjp;
2442   }
2443   return NULL;
2444 }
2445 
2446 bool LibraryCallKit::inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile) {
2447   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2448 
2449 #ifndef PRODUCT
2450   {
2451     ResourceMark rm;
2452     // Check the signatures.
2453     ciSignature* sig = callee()-&gt;signature();
2454 #ifdef ASSERT
2455     if (!is_store) {
2456       // Object getObject(Object base, int/long offset), etc.
2457       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2458       if (rtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::getAddress_name())
2459           rtype = T_ADDRESS;  // it is really a C void*
2460       assert(rtype == type, "getter must return the expected value");
2461       if (!is_native_ptr) {
2462         assert(sig-&gt;count() == 2, "oop getter has 2 arguments");
2463         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "getter base is object");
2464         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "getter offset is correct");
2465       } else {
2466         assert(sig-&gt;count() == 1, "native getter has 1 argument");
2467         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "getter base is long");
2468       }
2469     } else {
2470       // void putObject(Object base, int/long offset, Object x), etc.
2471       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, "putter must not return a value");
2472       if (!is_native_ptr) {
2473         assert(sig-&gt;count() == 3, "oop putter has 3 arguments");
2474         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "putter base is object");
2475         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "putter offset is correct");
2476       } else {
2477         assert(sig-&gt;count() == 2, "native putter has 2 arguments");
2478         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "putter base is long");
2479       }
2480       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
2481       if (vtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::putAddress_name())
2482         vtype = T_ADDRESS;  // it is really a C void*
2483       assert(vtype == type, "putter must accept the expected value");
2484     }
2485 #endif // ASSERT
2486  }
2487 #endif //PRODUCT
2488 
2489   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2490 
2491   Node* receiver = argument(0);  // type: oop
2492 
2493   // Build address expression.  See the code in inline_unsafe_prefetch.
2494   Node* adr;
2495   Node* heap_base_oop = top();
2496   Node* offset = top();
2497   Node* val;
2498 
2499   if (!is_native_ptr) {
2500     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2501     Node* base = argument(1);  // type: oop
2502     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2503     offset = argument(2);  // type: long
2504     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2505     // to be plain byte offsets, which are also the same as those accepted
2506     // by oopDesc::field_base.
2507     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2508            "fieldOffset must be byte-scaled");
2509     // 32-bit machines ignore the high half!
2510     offset = ConvL2X(offset);
2511     adr = make_unsafe_address(base, offset);
2512     heap_base_oop = base;
2513     val = is_store ? argument(4) : NULL;
2514   } else {
2515     Node* ptr = argument(1);  // type: long
2516     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2517     adr = make_unsafe_address(NULL, ptr);
2518     val = is_store ? argument(3) : NULL;
2519   }
2520 
2521   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2522 
2523   // First guess at the value type.
2524   const Type *value_type = Type::get_const_basic_type(type);
2525 
2526   // Try to categorize the address.  If it comes up as TypeJavaPtr::BOTTOM,
2527   // there was not enough information to nail it down.
2528   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2529   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2530 
2531   // We will need memory barriers unless we can determine a unique
2532   // alias category for this reference.  (Note:  If for some reason
2533   // the barriers get omitted and the unsafe reference begins to "pollute"
2534   // the alias analysis of the rest of the graph, either Compile::can_alias
2535   // or Compile::must_alias will throw a diagnostic assert.)
2536   bool need_mem_bar = (alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM);
2537 
2538   // If we are reading the value of the referent field of a Reference
2539   // object (either by using Unsafe directly or through reflection)
2540   // then, if G1 is enabled, we need to record the referent in an
2541   // SATB log buffer using the pre-barrier mechanism.
2542   // Also we need to add memory barrier to prevent commoning reads
2543   // from this field across safepoint since GC can change its value.
2544   bool need_read_barrier = !is_native_ptr &amp;&amp; !is_store &amp;&amp;
2545                            offset != top() &amp;&amp; heap_base_oop != top();
2546 
2547   if (!is_store &amp;&amp; type == T_OBJECT) {
2548     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type, is_native_ptr);
2549     if (tjp != NULL) {
2550       value_type = tjp;
2551     }
2552   }
2553 
2554   receiver = null_check(receiver);
2555   if (stopped()) {
2556     return true;
2557   }
2558   // Heap pointers get a null-check from the interpreter,
2559   // as a courtesy.  However, this is not guaranteed by Unsafe,
2560   // and it is not possible to fully distinguish unintended nulls
2561   // from intended ones in this API.
2562 
2563   if (is_volatile) {
2564     // We need to emit leading and trailing CPU membars (see below) in
2565     // addition to memory membars when is_volatile. This is a little
2566     // too strong, but avoids the need to insert per-alias-type
2567     // volatile membars (for stores; compare Parse::do_put_xxx), which
2568     // we cannot do effectively here because we probably only have a
2569     // rough approximation of type.
2570     need_mem_bar = true;
2571     // For Stores, place a memory ordering barrier now.
2572     if (is_store) {
2573       insert_mem_bar(Op_MemBarRelease);
2574     } else {
2575       if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2576         insert_mem_bar(Op_MemBarVolatile);
2577       }
2578     }
2579   }
2580 
2581   // Memory barrier to prevent normal and 'unsafe' accesses from
2582   // bypassing each other.  Happens after null checks, so the
2583   // exception paths do not take memory state from the memory barrier,
2584   // so there's no problems making a strong assert about mixing users
2585   // of safe &amp; unsafe memory.  Otherwise fails in a CTW of rt.jar
2586   // around 5701, class sun/reflect/UnsafeBooleanFieldAccessorImpl.
2587   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2588 
2589   if (!is_store) {
2590     Node* p = make_load(control(), adr, value_type, type, adr_type, MemNode::unordered, is_volatile);
2591     // load value
2592     switch (type) {
2593     case T_BOOLEAN:
2594     case T_CHAR:
2595     case T_BYTE:
2596     case T_SHORT:
2597     case T_INT:
2598     case T_LONG:
2599     case T_FLOAT:
2600     case T_DOUBLE:
2601       break;
2602     case T_OBJECT:
2603       if (need_read_barrier) {
2604         insert_pre_barrier(heap_base_oop, offset, p, !(is_volatile || need_mem_bar));
2605       }
2606       break;
2607     case T_ADDRESS:
2608       // Cast to an int type.
2609       p = _gvn.transform(new (C) CastP2XNode(NULL, p));
2610       p = ConvX2UL(p);
2611       break;
2612     default:
2613       fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2614       break;
2615     }
2616     // The load node has the control of the preceding MemBarCPUOrder.  All
2617     // following nodes will have the control of the MemBarCPUOrder inserted at
2618     // the end of this method.  So, pushing the load onto the stack at a later
2619     // point is fine.
2620     set_result(p);
2621   } else {
2622     // place effect of store into memory
2623     switch (type) {
2624     case T_DOUBLE:
2625       val = dstore_rounding(val);
2626       break;
2627     case T_ADDRESS:
2628       // Repackage the long as a pointer.
2629       val = ConvL2X(val);
2630       val = _gvn.transform(new (C) CastX2PNode(val));
2631       break;
2632     }
2633 
2634     MemNode::MemOrd mo = is_volatile ? MemNode::release : MemNode::unordered;
2635     if (type != T_OBJECT ) {
2636       (void) store_to_memory(control(), adr, val, type, adr_type, mo, is_volatile);
2637     } else {
2638       // Possibly an oop being stored to Java heap or native memory
2639       if (!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(heap_base_oop))) {
2640         // oop to Java heap.
2641         (void) store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2642       } else {
2643         // We can't tell at compile time if we are storing in the Java heap or outside
2644         // of it. So we need to emit code to conditionally do the proper type of
2645         // store.
2646 
2647         IdealKit ideal(this);
2648 #define __ ideal.
2649         // QQQ who knows what probability is here??
2650         __ if_then(heap_base_oop, BoolTest::ne, null(), PROB_UNLIKELY(0.999)); {
2651           // Sync IdealKit and graphKit.
2652           sync_kit(ideal);
2653           Node* st = store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2654           // Update IdealKit memory.
2655           __ sync_kit(this);
2656         } __ else_(); {
2657           __ store(__ ctrl(), adr, val, type, alias_type-&gt;index(), mo, is_volatile);
2658         } __ end_if();
2659         // Final sync IdealKit and GraphKit.
2660         final_sync(ideal);
2661 #undef __
2662       }
2663     }
2664   }
2665 
2666   if (is_volatile) {
2667     if (!is_store) {
2668       insert_mem_bar(Op_MemBarAcquire);
2669     } else {
2670       if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2671         insert_mem_bar(Op_MemBarVolatile);
2672       }
2673     }
2674   }
2675 
2676   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2677 
2678   return true;
2679 }
2680 
2681 //----------------------------inline_unsafe_prefetch----------------------------
2682 
2683 bool LibraryCallKit::inline_unsafe_prefetch(bool is_native_ptr, bool is_store, bool is_static) {
2684 #ifndef PRODUCT
2685   {
2686     ResourceMark rm;
2687     // Check the signatures.
2688     ciSignature* sig = callee()-&gt;signature();
2689 #ifdef ASSERT
2690     // Object getObject(Object base, int/long offset), etc.
2691     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2692     if (!is_native_ptr) {
2693       assert(sig-&gt;count() == 2, "oop prefetch has 2 arguments");
2694       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "prefetch base is object");
2695       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "prefetcha offset is correct");
2696     } else {
2697       assert(sig-&gt;count() == 1, "native prefetch has 1 argument");
2698       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "prefetch base is long");
2699     }
2700 #endif // ASSERT
2701   }
2702 #endif // !PRODUCT
2703 
2704   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2705 
2706   const int idx = is_static ? 0 : 1;
2707   if (!is_static) {
2708     null_check_receiver();
2709     if (stopped()) {
2710       return true;
2711     }
2712   }
2713 
2714   // Build address expression.  See the code in inline_unsafe_access.
2715   Node *adr;
2716   if (!is_native_ptr) {
2717     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2718     Node* base   = argument(idx + 0);  // type: oop
2719     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2720     Node* offset = argument(idx + 1);  // type: long
2721     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2722     // to be plain byte offsets, which are also the same as those accepted
2723     // by oopDesc::field_base.
2724     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2725            "fieldOffset must be byte-scaled");
2726     // 32-bit machines ignore the high half!
2727     offset = ConvL2X(offset);
2728     adr = make_unsafe_address(base, offset);
2729   } else {
2730     Node* ptr = argument(idx + 0);  // type: long
2731     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2732     adr = make_unsafe_address(NULL, ptr);
2733   }
2734 
2735   // Generate the read or write prefetch
2736   Node *prefetch;
2737   if (is_store) {
2738     prefetch = new (C) PrefetchWriteNode(i_o(), adr);
2739   } else {
2740     prefetch = new (C) PrefetchReadNode(i_o(), adr);
2741   }
2742   prefetch-&gt;init_req(0, control());
2743   set_i_o(_gvn.transform(prefetch));
2744 
2745   return true;
2746 }
2747 
2748 //----------------------------inline_unsafe_load_store----------------------------
2749 // This method serves a couple of different customers (depending on LoadStoreKind):
2750 //
2751 // LS_cmpxchg:
2752 //   public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object x);
2753 //   public final native boolean compareAndSwapInt(   Object o, long offset, int    expected, int    x);
2754 //   public final native boolean compareAndSwapLong(  Object o, long offset, long   expected, long   x);
2755 //
2756 // LS_xadd:
2757 //   public int  getAndAddInt( Object o, long offset, int  delta)
2758 //   public long getAndAddLong(Object o, long offset, long delta)
2759 //
2760 // LS_xchg:
2761 //   int    getAndSet(Object o, long offset, int    newValue)
2762 //   long   getAndSet(Object o, long offset, long   newValue)
2763 //   Object getAndSet(Object o, long offset, Object newValue)
2764 //
2765 bool LibraryCallKit::inline_unsafe_load_store(BasicType type, LoadStoreKind kind) {
2766   // This basic scheme here is the same as inline_unsafe_access, but
2767   // differs in enough details that combining them would make the code
2768   // overly confusing.  (This is a true fact! I originally combined
2769   // them, but even I was confused by it!) As much code/comments as
2770   // possible are retained from inline_unsafe_access though to make
2771   // the correspondences clearer. - dl
2772 
2773   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2774 
2775 #ifndef PRODUCT
2776   BasicType rtype;
2777   {
2778     ResourceMark rm;
2779     // Check the signatures.
2780     ciSignature* sig = callee()-&gt;signature();
2781     rtype = sig-&gt;return_type()-&gt;basic_type();
2782     if (kind == LS_xadd || kind == LS_xchg) {
2783       // Check the signatures.
2784 #ifdef ASSERT
2785       assert(rtype == type, "get and set must return the expected type");
2786       assert(sig-&gt;count() == 3, "get and set has 3 arguments");
2787       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "get and set base is object");
2788       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "get and set offset is long");
2789       assert(sig-&gt;type_at(2)-&gt;basic_type() == type, "get and set must take expected type as new value/delta");
2790 #endif // ASSERT
2791     } else if (kind == LS_cmpxchg) {
2792       // Check the signatures.
2793 #ifdef ASSERT
2794       assert(rtype == T_BOOLEAN, "CAS must return boolean");
2795       assert(sig-&gt;count() == 4, "CAS has 4 arguments");
2796       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "CAS base is object");
2797       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "CAS offset is long");
2798 #endif // ASSERT
2799     } else {
2800       ShouldNotReachHere();
2801     }
2802   }
2803 #endif //PRODUCT
2804 
2805   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2806 
2807   // Get arguments:
2808   Node* receiver = NULL;
2809   Node* base     = NULL;
2810   Node* offset   = NULL;
2811   Node* oldval   = NULL;
2812   Node* newval   = NULL;
2813   if (kind == LS_cmpxchg) {
2814     const bool two_slot_type = type2size[type] == 2;
2815     receiver = argument(0);  // type: oop
2816     base     = argument(1);  // type: oop
2817     offset   = argument(2);  // type: long
2818     oldval   = argument(4);  // type: oop, int, or long
2819     newval   = argument(two_slot_type ? 6 : 5);  // type: oop, int, or long
2820   } else if (kind == LS_xadd || kind == LS_xchg){
2821     receiver = argument(0);  // type: oop
2822     base     = argument(1);  // type: oop
2823     offset   = argument(2);  // type: long
2824     oldval   = NULL;
2825     newval   = argument(4);  // type: oop, int, or long
2826   }
2827 
2828   // Null check receiver.
2829   receiver = null_check(receiver);
2830   if (stopped()) {
2831     return true;
2832   }
2833 
2834   // Build field offset expression.
2835   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2836   // to be plain byte offsets, which are also the same as those accepted
2837   // by oopDesc::field_base.
2838   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2839   // 32-bit machines ignore the high half of long offsets
2840   offset = ConvL2X(offset);
2841   Node* adr = make_unsafe_address(base, offset);
2842   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2843 
2844   // For CAS, unlike inline_unsafe_access, there seems no point in
2845   // trying to refine types. Just use the coarse types here.
2846   const Type *value_type = Type::get_const_basic_type(type);
2847   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2848   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2849 
2850   if (kind == LS_xchg &amp;&amp; type == T_OBJECT) {
2851     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
2852     if (tjp != NULL) {
2853       value_type = tjp;
2854     }
2855   }
2856 
2857   int alias_idx = C-&gt;get_alias_index(adr_type);
2858 
2859   // Memory-model-wise, a LoadStore acts like a little synchronized
2860   // block, so needs barriers on each side.  These don't translate
2861   // into actual barriers on most machines, but we still need rest of
2862   // compiler to respect ordering.
2863 
2864   insert_mem_bar(Op_MemBarRelease);
2865   insert_mem_bar(Op_MemBarCPUOrder);
2866 
2867   // 4984716: MemBars must be inserted before this
2868   //          memory node in order to avoid a false
2869   //          dependency which will confuse the scheduler.
2870   Node *mem = memory(alias_idx);
2871 
2872   // For now, we handle only those cases that actually exist: ints,
2873   // longs, and Object. Adding others should be straightforward.
2874   Node* load_store;
2875   switch(type) {
2876   case T_INT:
2877     if (kind == LS_xadd) {
2878       load_store = _gvn.transform(new (C) GetAndAddINode(control(), mem, adr, newval, adr_type));
2879     } else if (kind == LS_xchg) {
2880       load_store = _gvn.transform(new (C) GetAndSetINode(control(), mem, adr, newval, adr_type));
2881     } else if (kind == LS_cmpxchg) {
2882       load_store = _gvn.transform(new (C) CompareAndSwapINode(control(), mem, adr, newval, oldval));
2883     } else {
2884       ShouldNotReachHere();
2885     }
2886     break;
2887   case T_LONG:
2888     if (kind == LS_xadd) {
2889       load_store = _gvn.transform(new (C) GetAndAddLNode(control(), mem, adr, newval, adr_type));
2890     } else if (kind == LS_xchg) {
2891       load_store = _gvn.transform(new (C) GetAndSetLNode(control(), mem, adr, newval, adr_type));
2892     } else if (kind == LS_cmpxchg) {
2893       load_store = _gvn.transform(new (C) CompareAndSwapLNode(control(), mem, adr, newval, oldval));
2894     } else {
2895       ShouldNotReachHere();
2896     }
2897     break;
2898   case T_OBJECT:
2899     // Transformation of a value which could be NULL pointer (CastPP #NULL)
2900     // could be delayed during Parse (for example, in adjust_map_after_if()).
2901     // Execute transformation here to avoid barrier generation in such case.
2902     if (_gvn.type(newval) == TypePtr::NULL_PTR)
2903       newval = _gvn.makecon(TypePtr::NULL_PTR);
2904 
2905     // Reference stores need a store barrier.
2906     if (kind == LS_xchg) {
2907       // If pre-barrier must execute before the oop store, old value will require do_load here.
2908       if (!can_move_pre_barrier()) {
2909         pre_barrier(true /* do_load*/,
2910                     control(), base, adr, alias_idx, newval, value_type-&gt;make_oopptr(),
2911                     NULL /* pre_val*/,
2912                     T_OBJECT);
2913       } // Else move pre_barrier to use load_store value, see below.
2914     } else if (kind == LS_cmpxchg) {
2915       // Same as for newval above:
2916       if (_gvn.type(oldval) == TypePtr::NULL_PTR) {
2917         oldval = _gvn.makecon(TypePtr::NULL_PTR);
2918       }
2919       // The only known value which might get overwritten is oldval.
2920       pre_barrier(false /* do_load */,
2921                   control(), NULL, NULL, max_juint, NULL, NULL,
2922                   oldval /* pre_val */,
2923                   T_OBJECT);
2924     } else {
2925       ShouldNotReachHere();
2926     }
2927 
2928 #ifdef _LP64
2929     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2930       Node *newval_enc = _gvn.transform(new (C) EncodePNode(newval, newval-&gt;bottom_type()-&gt;make_narrowoop()));
2931       if (kind == LS_xchg) {
2932         load_store = _gvn.transform(new (C) GetAndSetNNode(control(), mem, adr,
2933                                                            newval_enc, adr_type, value_type-&gt;make_narrowoop()));
2934       } else {
2935         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2936         Node *oldval_enc = _gvn.transform(new (C) EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
2937         load_store = _gvn.transform(new (C) CompareAndSwapNNode(control(), mem, adr,
2938                                                                 newval_enc, oldval_enc));
2939       }
2940     } else
2941 #endif
2942     {
2943       if (kind == LS_xchg) {
2944         load_store = _gvn.transform(new (C) GetAndSetPNode(control(), mem, adr, newval, adr_type, value_type-&gt;is_oopptr()));
2945       } else {
2946         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
2947         load_store = _gvn.transform(new (C) CompareAndSwapPNode(control(), mem, adr, newval, oldval));
2948       }
2949     }
2950     post_barrier(control(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
2951     break;
2952   default:
2953     fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2954     break;
2955   }
2956 
2957   // SCMemProjNodes represent the memory state of a LoadStore. Their
2958   // main role is to prevent LoadStore nodes from being optimized away
2959   // when their results aren't used.
2960   Node* proj = _gvn.transform(new (C) SCMemProjNode(load_store));
2961   set_memory(proj, alias_idx);
2962 
2963   if (type == T_OBJECT &amp;&amp; kind == LS_xchg) {
2964 #ifdef _LP64
2965     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2966       load_store = _gvn.transform(new (C) DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
2967     }
2968 #endif
2969     if (can_move_pre_barrier()) {
2970       // Don't need to load pre_val. The old value is returned by load_store.
2971       // The pre_barrier can execute after the xchg as long as no safepoint
2972       // gets inserted between them.
2973       pre_barrier(false /* do_load */,
2974                   control(), NULL, NULL, max_juint, NULL, NULL,
2975                   load_store /* pre_val */,
2976                   T_OBJECT);
2977     }
2978   }
2979 
2980   // Add the trailing membar surrounding the access
2981   insert_mem_bar(Op_MemBarCPUOrder);
2982   insert_mem_bar(Op_MemBarAcquire);
2983 
2984   assert(type2size[load_store-&gt;bottom_type()-&gt;basic_type()] == type2size[rtype], "result type should match");
2985   set_result(load_store);
2986   return true;
2987 }
2988 
2989 //----------------------------inline_unsafe_ordered_store----------------------
2990 // public native void sun.misc.Unsafe.putOrderedObject(Object o, long offset, Object x);
2991 // public native void sun.misc.Unsafe.putOrderedInt(Object o, long offset, int x);
2992 // public native void sun.misc.Unsafe.putOrderedLong(Object o, long offset, long x);
2993 bool LibraryCallKit::inline_unsafe_ordered_store(BasicType type) {
2994   // This is another variant of inline_unsafe_access, differing in
2995   // that it always issues store-store ("release") barrier and ensures
2996   // store-atomicity (which only matters for "long").
2997 
2998   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2999 
3000 #ifndef PRODUCT
3001   {
3002     ResourceMark rm;
3003     // Check the signatures.
3004     ciSignature* sig = callee()-&gt;signature();
3005 #ifdef ASSERT
3006     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
3007     assert(rtype == T_VOID, "must return void");
3008     assert(sig-&gt;count() == 3, "has 3 arguments");
3009     assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "base is object");
3010     assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "offset is long");
3011 #endif // ASSERT
3012   }
3013 #endif //PRODUCT
3014 
3015   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
3016 
3017   // Get arguments:
3018   Node* receiver = argument(0);  // type: oop
3019   Node* base     = argument(1);  // type: oop
3020   Node* offset   = argument(2);  // type: long
3021   Node* val      = argument(4);  // type: oop, int, or long
3022 
3023   // Null check receiver.
3024   receiver = null_check(receiver);
3025   if (stopped()) {
3026     return true;
3027   }
3028 
3029   // Build field offset expression.
3030   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
3031   // 32-bit machines ignore the high half of long offsets
3032   offset = ConvL2X(offset);
3033   Node* adr = make_unsafe_address(base, offset);
3034   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
3035   const Type *value_type = Type::get_const_basic_type(type);
3036   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
3037 
3038   insert_mem_bar(Op_MemBarRelease);
3039   insert_mem_bar(Op_MemBarCPUOrder);
3040   // Ensure that the store is atomic for longs:
3041   const bool require_atomic_access = true;
3042   Node* store;
3043   if (type == T_OBJECT) // reference stores need a store barrier.
3044     store = store_oop_to_unknown(control(), base, adr, adr_type, val, type, MemNode::release);
3045   else {
3046     store = store_to_memory(control(), adr, val, type, adr_type, MemNode::release, require_atomic_access);
3047   }
3048   insert_mem_bar(Op_MemBarCPUOrder);
3049   return true;
3050 }
3051 
3052 bool LibraryCallKit::inline_unsafe_fence(vmIntrinsics::ID id) {
3053   // Regardless of form, don't allow previous ld/st to move down,
3054   // then issue acquire, release, or volatile mem_bar.
3055   insert_mem_bar(Op_MemBarCPUOrder);
3056   switch(id) {
3057     case vmIntrinsics::_loadFence:
3058       insert_mem_bar(Op_LoadFence);
3059       return true;
3060     case vmIntrinsics::_storeFence:
3061       insert_mem_bar(Op_StoreFence);
3062       return true;
3063     case vmIntrinsics::_fullFence:
3064       insert_mem_bar(Op_MemBarVolatile);
3065       return true;
3066     default:
3067       fatal_unexpected_iid(id);
3068       return false;
3069   }
3070 }
3071 
3072 bool LibraryCallKit::klass_needs_init_guard(Node* kls) {
3073   if (!kls-&gt;is_Con()) {
3074     return true;
3075   }
3076   const TypeKlassPtr* klsptr = kls-&gt;bottom_type()-&gt;isa_klassptr();
3077   if (klsptr == NULL) {
3078     return true;
3079   }
3080   ciInstanceKlass* ik = klsptr-&gt;klass()-&gt;as_instance_klass();
3081   // don't need a guard for a klass that is already initialized
3082   return !ik-&gt;is_initialized();
3083 }
3084 
3085 //----------------------------inline_unsafe_allocate---------------------------
3086 // public native Object sun.misc.Unsafe.allocateInstance(Class&lt;?&gt; cls);
3087 bool LibraryCallKit::inline_unsafe_allocate() {
3088   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3089 
3090   null_check_receiver();  // null-check, then ignore
3091   Node* cls = null_check(argument(1));
3092   if (stopped())  return true;
3093 
3094   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3095   kls = null_check(kls);
3096   if (stopped())  return true;  // argument was like int.class
3097 
3098   Node* test = NULL;
3099   if (LibraryCallKit::klass_needs_init_guard(kls)) {
3100     // Note:  The argument might still be an illegal value like
3101     // Serializable.class or Object[].class.   The runtime will handle it.
3102     // But we must make an explicit check for initialization.
3103     Node* insp = basic_plus_adr(kls, in_bytes(InstanceKlass::init_state_offset()));
3104     // Use T_BOOLEAN for InstanceKlass::_init_state so the compiler
3105     // can generate code to load it as unsigned byte.
3106     Node* inst = make_load(NULL, insp, TypeInt::UBYTE, T_BOOLEAN, MemNode::unordered);
3107     Node* bits = intcon(InstanceKlass::fully_initialized);
3108     test = _gvn.transform(new (C) SubINode(inst, bits));
3109     // The 'test' is non-zero if we need to take a slow path.
3110   }
3111 
3112   Node* obj = new_instance(kls, test);
3113   set_result(obj);
3114   return true;
3115 }
3116 
3117 #ifdef TRACE_HAVE_INTRINSICS
3118 /*
3119  * oop -&gt; myklass
3120  * myklass-&gt;trace_id |= USED
3121  * return myklass-&gt;trace_id &amp; ~0x3
3122  */
3123 bool LibraryCallKit::inline_native_classID() {
3124   null_check_receiver();  // null-check, then ignore
3125   Node* cls = null_check(argument(1), T_OBJECT);
3126   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3127   kls = null_check(kls, T_OBJECT);
3128   ByteSize offset = TRACE_ID_OFFSET;
3129   Node* insp = basic_plus_adr(kls, in_bytes(offset));
3130   Node* tvalue = make_load(NULL, insp, TypeLong::LONG, T_LONG, MemNode::unordered);
3131   Node* bits = longcon(~0x03l); // ignore bit 0 &amp; 1
3132   Node* andl = _gvn.transform(new (C) AndLNode(tvalue, bits));
3133   Node* clsused = longcon(0x01l); // set the class bit
3134   Node* orl = _gvn.transform(new (C) OrLNode(tvalue, clsused));
3135 
3136   const TypePtr *adr_type = _gvn.type(insp)-&gt;isa_ptr();
3137   store_to_memory(control(), insp, orl, T_LONG, adr_type, MemNode::unordered);
3138   set_result(andl);
3139   return true;
3140 }
3141 
3142 bool LibraryCallKit::inline_native_threadID() {
3143   Node* tls_ptr = NULL;
3144   Node* cur_thr = generate_current_thread(tls_ptr);
3145   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3146   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3147   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::thread_id_offset()));
3148 
3149   Node* threadid = NULL;
3150   size_t thread_id_size = OSThread::thread_id_size();
3151   if (thread_id_size == (size_t) BytesPerLong) {
3152     threadid = ConvL2I(make_load(control(), p, TypeLong::LONG, T_LONG, MemNode::unordered));
3153   } else if (thread_id_size == (size_t) BytesPerInt) {
3154     threadid = make_load(control(), p, TypeInt::INT, T_INT, MemNode::unordered);
3155   } else {
3156     ShouldNotReachHere();
3157   }
3158   set_result(threadid);
3159   return true;
3160 }
3161 #endif
3162 
3163 //------------------------inline_native_time_funcs--------------
3164 // inline code for System.currentTimeMillis() and System.nanoTime()
3165 // these have the same type and signature
3166 bool LibraryCallKit::inline_native_time_funcs(address funcAddr, const char* funcName) {
3167   const TypeFunc* tf = OptoRuntime::void_long_Type();
3168   const TypePtr* no_memory_effects = NULL;
3169   Node* time = make_runtime_call(RC_LEAF, tf, funcAddr, funcName, no_memory_effects);
3170   Node* value = _gvn.transform(new (C) ProjNode(time, TypeFunc::Parms+0));
3171 #ifdef ASSERT
3172   Node* value_top = _gvn.transform(new (C) ProjNode(time, TypeFunc::Parms+1));
3173   assert(value_top == top(), "second value must be top");
3174 #endif
3175   set_result(value);
3176   return true;
3177 }
3178 
3179 //------------------------inline_native_currentThread------------------
3180 bool LibraryCallKit::inline_native_currentThread() {
3181   Node* junk = NULL;
3182   set_result(generate_current_thread(junk));
3183   return true;
3184 }
3185 
3186 //------------------------inline_native_isInterrupted------------------
3187 // private native boolean java.lang.Thread.isInterrupted(boolean ClearInterrupted);
3188 bool LibraryCallKit::inline_native_isInterrupted() {
3189   // Add a fast path to t.isInterrupted(clear_int):
3190   //   (t == Thread.current() &amp;&amp;
3191   //    (!TLS._osthread._interrupted || WINDOWS_ONLY(false) NOT_WINDOWS(!clear_int)))
3192   //   ? TLS._osthread._interrupted : /*slow path:*/ t.isInterrupted(clear_int)
3193   // So, in the common case that the interrupt bit is false,
3194   // we avoid making a call into the VM.  Even if the interrupt bit
3195   // is true, if the clear_int argument is false, we avoid the VM call.
3196   // However, if the receiver is not currentThread, we must call the VM,
3197   // because there must be some locking done around the operation.
3198 
3199   // We only go to the fast case code if we pass two guards.
3200   // Paths which do not pass are accumulated in the slow_region.
3201 
3202   enum {
3203     no_int_result_path   = 1, // t == Thread.current() &amp;&amp; !TLS._osthread._interrupted
3204     no_clear_result_path = 2, // t == Thread.current() &amp;&amp;  TLS._osthread._interrupted &amp;&amp; !clear_int
3205     slow_result_path     = 3, // slow path: t.isInterrupted(clear_int)
3206     PATH_LIMIT
3207   };
3208 
3209   // Ensure that it's not possible to move the load of TLS._osthread._interrupted flag
3210   // out of the function.
3211   insert_mem_bar(Op_MemBarCPUOrder);
3212 
3213   RegionNode* result_rgn = new (C) RegionNode(PATH_LIMIT);
3214   PhiNode*    result_val = new (C) PhiNode(result_rgn, TypeInt::BOOL);
3215 
3216   RegionNode* slow_region = new (C) RegionNode(1);
3217   record_for_igvn(slow_region);
3218 
3219   // (a) Receiving thread must be the current thread.
3220   Node* rec_thr = argument(0);
3221   Node* tls_ptr = NULL;
3222   Node* cur_thr = generate_current_thread(tls_ptr);
3223   Node* cmp_thr = _gvn.transform(new (C) CmpPNode(cur_thr, rec_thr));
3224   Node* bol_thr = _gvn.transform(new (C) BoolNode(cmp_thr, BoolTest::ne));
3225 
3226   generate_slow_guard(bol_thr, slow_region);
3227 
3228   // (b) Interrupt bit on TLS must be false.
3229   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3230   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3231   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::interrupted_offset()));
3232 
3233   // Set the control input on the field _interrupted read to prevent it floating up.
3234   Node* int_bit = make_load(control(), p, TypeInt::BOOL, T_INT, MemNode::unordered);
3235   Node* cmp_bit = _gvn.transform(new (C) CmpINode(int_bit, intcon(0)));
3236   Node* bol_bit = _gvn.transform(new (C) BoolNode(cmp_bit, BoolTest::ne));
3237 
3238   IfNode* iff_bit = create_and_map_if(control(), bol_bit, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
3239 
3240   // First fast path:  if (!TLS._interrupted) return false;
3241   Node* false_bit = _gvn.transform(new (C) IfFalseNode(iff_bit));
3242   result_rgn-&gt;init_req(no_int_result_path, false_bit);
3243   result_val-&gt;init_req(no_int_result_path, intcon(0));
3244 
3245   // drop through to next case
3246   set_control( _gvn.transform(new (C) IfTrueNode(iff_bit)));
3247 
3248 #ifndef TARGET_OS_FAMILY_windows
3249   // (c) Or, if interrupt bit is set and clear_int is false, use 2nd fast path.
3250   Node* clr_arg = argument(1);
3251   Node* cmp_arg = _gvn.transform(new (C) CmpINode(clr_arg, intcon(0)));
3252   Node* bol_arg = _gvn.transform(new (C) BoolNode(cmp_arg, BoolTest::ne));
3253   IfNode* iff_arg = create_and_map_if(control(), bol_arg, PROB_FAIR, COUNT_UNKNOWN);
3254 
3255   // Second fast path:  ... else if (!clear_int) return true;
3256   Node* false_arg = _gvn.transform(new (C) IfFalseNode(iff_arg));
3257   result_rgn-&gt;init_req(no_clear_result_path, false_arg);
3258   result_val-&gt;init_req(no_clear_result_path, intcon(1));
3259 
3260   // drop through to next case
3261   set_control( _gvn.transform(new (C) IfTrueNode(iff_arg)));
3262 #else
3263   // To return true on Windows you must read the _interrupted field
3264   // and check the the event state i.e. take the slow path.
3265 #endif // TARGET_OS_FAMILY_windows
3266 
3267   // (d) Otherwise, go to the slow path.
3268   slow_region-&gt;add_req(control());
3269   set_control( _gvn.transform(slow_region));
3270 
3271   if (stopped()) {
3272     // There is no slow path.
3273     result_rgn-&gt;init_req(slow_result_path, top());
3274     result_val-&gt;init_req(slow_result_path, top());
3275   } else {
3276     // non-virtual because it is a private non-static
3277     CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_isInterrupted);
3278 
3279     Node* slow_val = set_results_for_java_call(slow_call);
3280     // this-&gt;control() comes from set_results_for_java_call
3281 
3282     Node* fast_io  = slow_call-&gt;in(TypeFunc::I_O);
3283     Node* fast_mem = slow_call-&gt;in(TypeFunc::Memory);
3284 
3285     // These two phis are pre-filled with copies of of the fast IO and Memory
3286     PhiNode* result_mem  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
3287     PhiNode* result_io   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
3288 
3289     result_rgn-&gt;init_req(slow_result_path, control());
3290     result_io -&gt;init_req(slow_result_path, i_o());
3291     result_mem-&gt;init_req(slow_result_path, reset_memory());
3292     result_val-&gt;init_req(slow_result_path, slow_val);
3293 
3294     set_all_memory(_gvn.transform(result_mem));
3295     set_i_o(       _gvn.transform(result_io));
3296   }
3297 
3298   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3299   set_result(result_rgn, result_val);
3300   return true;
3301 }
3302 
3303 //---------------------------load_mirror_from_klass----------------------------
3304 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3305 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3306   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3307   return make_load(NULL, p, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3308 }
3309 
3310 //-----------------------load_klass_from_mirror_common-------------------------
3311 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3312 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3313 // and branch to the given path on the region.
3314 // If never_see_null, take an uncommon trap on null, so we can optimistically
3315 // compile for the non-null case.
3316 // If the region is NULL, force never_see_null = true.
3317 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3318                                                     bool never_see_null,
3319                                                     RegionNode* region,
3320                                                     int null_path,
3321                                                     int offset) {
3322   if (region == NULL)  never_see_null = true;
3323   Node* p = basic_plus_adr(mirror, offset);
3324   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3325   Node* kls = _gvn.transform( LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3326   Node* null_ctl = top();
3327   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3328   if (region != NULL) {
3329     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
3330     region-&gt;init_req(null_path, null_ctl);
3331   } else {
3332     assert(null_ctl == top(), "no loose ends");
3333   }
3334   return kls;
3335 }
3336 
3337 //--------------------(inline_native_Class_query helpers)---------------------
3338 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE, JVM_ACC_HAS_FINALIZER.
3339 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3340 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3341   // Branch around if the given klass has the given modifier bit set.
3342   // Like generate_guard, adds a new path onto the region.
3343   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3344   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3345   Node* mask = intcon(modifier_mask);
3346   Node* bits = intcon(modifier_bits);
3347   Node* mbit = _gvn.transform(new (C) AndINode(mods, mask));
3348   Node* cmp  = _gvn.transform(new (C) CmpINode(mbit, bits));
3349   Node* bol  = _gvn.transform(new (C) BoolNode(cmp, BoolTest::ne));
3350   return generate_fair_guard(bol, region);
3351 }
3352 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3353   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3354 }
3355 
3356 //-------------------------inline_native_Class_query-------------------
3357 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3358   const Type* return_type = TypeInt::BOOL;
3359   Node* prim_return_value = top();  // what happens if it's a primitive class?
3360   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3361   bool expect_prim = false;     // most of these guys expect to work on refs
3362 
3363   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3364 
3365   Node* mirror = argument(0);
3366   Node* obj    = top();
3367 
3368   switch (id) {
3369   case vmIntrinsics::_isInstance:
3370     // nothing is an instance of a primitive type
3371     prim_return_value = intcon(0);
3372     obj = argument(1);
3373     break;
3374   case vmIntrinsics::_getModifiers:
3375     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3376     assert(is_power_of_2((int)JVM_ACC_WRITTEN_FLAGS+1), "change next line");
3377     return_type = TypeInt::make(0, JVM_ACC_WRITTEN_FLAGS, Type::WidenMin);
3378     break;
3379   case vmIntrinsics::_isInterface:
3380     prim_return_value = intcon(0);
3381     break;
3382   case vmIntrinsics::_isArray:
3383     prim_return_value = intcon(0);
3384     expect_prim = true;  // cf. ObjectStreamClass.getClassSignature
3385     break;
3386   case vmIntrinsics::_isPrimitive:
3387     prim_return_value = intcon(1);
3388     expect_prim = true;  // obviously
3389     break;
3390   case vmIntrinsics::_getSuperclass:
3391     prim_return_value = null();
3392     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3393     break;
3394   case vmIntrinsics::_getComponentType:
3395     prim_return_value = null();
3396     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3397     break;
3398   case vmIntrinsics::_getClassAccessFlags:
3399     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3400     return_type = TypeInt::INT;  // not bool!  6297094
3401     break;
3402   default:
3403     fatal_unexpected_iid(id);
3404     break;
3405   }
3406 
3407   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3408   if (mirror_con == NULL)  return false;  // cannot happen?
3409 
3410 #ifndef PRODUCT
3411   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
3412     ciType* k = mirror_con-&gt;java_mirror_type();
3413     if (k) {
3414       tty-&gt;print("Inlining %s on constant Class ", vmIntrinsics::name_at(intrinsic_id()));
3415       k-&gt;print_name();
3416       tty-&gt;cr();
3417     }
3418   }
3419 #endif
3420 
3421   // Null-check the mirror, and the mirror's klass ptr (in case it is a primitive).
3422   RegionNode* region = new (C) RegionNode(PATH_LIMIT);
3423   record_for_igvn(region);
3424   PhiNode* phi = new (C) PhiNode(region, return_type);
3425 
3426   // The mirror will never be null of Reflection.getClassAccessFlags, however
3427   // it may be null for Class.isInstance or Class.getModifiers. Throw a NPE
3428   // if it is. See bug 4774291.
3429 
3430   // For Reflection.getClassAccessFlags(), the null check occurs in
3431   // the wrong place; see inline_unsafe_access(), above, for a similar
3432   // situation.
3433   mirror = null_check(mirror);
3434   // If mirror or obj is dead, only null-path is taken.
3435   if (stopped())  return true;
3436 
3437   if (expect_prim)  never_see_null = false;  // expect nulls (meaning prims)
3438 
3439   // Now load the mirror's klass metaobject, and null-check it.
3440   // Side-effects region with the control path if the klass is null.
3441   Node* kls = load_klass_from_mirror(mirror, never_see_null, region, _prim_path);
3442   // If kls is null, we have a primitive mirror.
3443   phi-&gt;init_req(_prim_path, prim_return_value);
3444   if (stopped()) { set_result(region, phi); return true; }
3445   bool safe_for_replace = (region-&gt;in(_prim_path) == top());
3446 
3447   Node* p;  // handy temp
3448   Node* null_ctl;
3449 
3450   // Now that we have the non-null klass, we can perform the real query.
3451   // For constant classes, the query will constant-fold in LoadNode::Value.
3452   Node* query_value = top();
3453   switch (id) {
3454   case vmIntrinsics::_isInstance:
3455     // nothing is an instance of a primitive type
3456     query_value = gen_instanceof(obj, kls, safe_for_replace);
3457     break;
3458 
3459   case vmIntrinsics::_getModifiers:
3460     p = basic_plus_adr(kls, in_bytes(Klass::modifier_flags_offset()));
3461     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3462     break;
3463 
3464   case vmIntrinsics::_isInterface:
3465     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3466     if (generate_interface_guard(kls, region) != NULL)
3467       // A guard was added.  If the guard is taken, it was an interface.
3468       phi-&gt;add_req(intcon(1));
3469     // If we fall through, it's a plain class.
3470     query_value = intcon(0);
3471     break;
3472 
3473   case vmIntrinsics::_isArray:
3474     // (To verify this code sequence, check the asserts in JVM_IsArrayClass.)
3475     if (generate_array_guard(kls, region) != NULL)
3476       // A guard was added.  If the guard is taken, it was an array.
3477       phi-&gt;add_req(intcon(1));
3478     // If we fall through, it's a plain class.
3479     query_value = intcon(0);
3480     break;
3481 
3482   case vmIntrinsics::_isPrimitive:
3483     query_value = intcon(0); // "normal" path produces false
3484     break;
3485 
3486   case vmIntrinsics::_getSuperclass:
3487     // The rules here are somewhat unfortunate, but we can still do better
3488     // with random logic than with a JNI call.
3489     // Interfaces store null or Object as _super, but must report null.
3490     // Arrays store an intermediate super as _super, but must report Object.
3491     // Other types can report the actual _super.
3492     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3493     if (generate_interface_guard(kls, region) != NULL)
3494       // A guard was added.  If the guard is taken, it was an interface.
3495       phi-&gt;add_req(null());
3496     if (generate_array_guard(kls, region) != NULL)
3497       // A guard was added.  If the guard is taken, it was an array.
3498       phi-&gt;add_req(makecon(TypeInstPtr::make(env()-&gt;Object_klass()-&gt;java_mirror())));
3499     // If we fall through, it's a plain class.  Get its _super.
3500     p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));
3501     kls = _gvn.transform( LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeKlassPtr::OBJECT_OR_NULL));
3502     null_ctl = top();
3503     kls = null_check_oop(kls, &amp;null_ctl);
3504     if (null_ctl != top()) {
3505       // If the guard is taken, Object.superClass is null (both klass and mirror).
3506       region-&gt;add_req(null_ctl);
3507       phi   -&gt;add_req(null());
3508     }
3509     if (!stopped()) {
3510       query_value = load_mirror_from_klass(kls);
3511     }
3512     break;
3513 
3514   case vmIntrinsics::_getComponentType:
3515     if (generate_array_guard(kls, region) != NULL) {
3516       // Be sure to pin the oop load to the guard edge just created:
3517       Node* is_array_ctrl = region-&gt;in(region-&gt;req()-1);
3518       Node* cma = basic_plus_adr(kls, in_bytes(ArrayKlass::component_mirror_offset()));
3519       Node* cmo = make_load(is_array_ctrl, cma, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3520       phi-&gt;add_req(cmo);
3521     }
3522     query_value = null();  // non-array case is null
3523     break;
3524 
3525   case vmIntrinsics::_getClassAccessFlags:
3526     p = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3527     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3528     break;
3529 
3530   default:
3531     fatal_unexpected_iid(id);
3532     break;
3533   }
3534 
3535   // Fall-through is the normal case of a query to a real class.
3536   phi-&gt;init_req(1, query_value);
3537   region-&gt;init_req(1, control());
3538 
3539   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3540   set_result(region, phi);
3541   return true;
3542 }
3543 
3544 //--------------------------inline_native_subtype_check------------------------
3545 // This intrinsic takes the JNI calls out of the heart of
3546 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3547 bool LibraryCallKit::inline_native_subtype_check() {
3548   // Pull both arguments off the stack.
3549   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3550   args[0] = argument(0);
3551   args[1] = argument(1);
3552   Node* klasses[2];             // corresponding Klasses: superk, subk
3553   klasses[0] = klasses[1] = top();
3554 
3555   enum {
3556     // A full decision tree on {superc is prim, subc is prim}:
3557     _prim_0_path = 1,           // {P,N} =&gt; false
3558                                 // {P,P} &amp; superc!=subc =&gt; false
3559     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3560     _prim_1_path,               // {N,P} =&gt; false
3561     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3562     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3563     PATH_LIMIT
3564   };
3565 
3566   RegionNode* region = new (C) RegionNode(PATH_LIMIT);
3567   Node*       phi    = new (C) PhiNode(region, TypeInt::BOOL);
3568   record_for_igvn(region);
3569 
3570   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3571   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3572   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3573 
3574   // First null-check both mirrors and load each mirror's klass metaobject.
3575   int which_arg;
3576   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3577     Node* arg = args[which_arg];
3578     arg = null_check(arg);
3579     if (stopped())  break;
3580     args[which_arg] = arg;
3581 
3582     Node* p = basic_plus_adr(arg, class_klass_offset);
3583     Node* kls = LoadKlassNode::make(_gvn, immutable_memory(), p, adr_type, kls_type);
3584     klasses[which_arg] = _gvn.transform(kls);
3585   }
3586 
3587   // Having loaded both klasses, test each for null.
3588   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3589   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3590     Node* kls = klasses[which_arg];
3591     Node* null_ctl = top();
3592     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3593     int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);
3594     region-&gt;init_req(prim_path, null_ctl);
3595     if (stopped())  break;
3596     klasses[which_arg] = kls;
3597   }
3598 
3599   if (!stopped()) {
3600     // now we have two reference types, in klasses[0..1]
3601     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3602     Node* superk = klasses[0];  // the receiver
3603     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3604     // now we have a successful reference subtype check
3605     region-&gt;set_req(_ref_subtype_path, control());
3606   }
3607 
3608   // If both operands are primitive (both klasses null), then
3609   // we must return true when they are identical primitives.
3610   // It is convenient to test this after the first null klass check.
3611   set_control(region-&gt;in(_prim_0_path)); // go back to first null check
3612   if (!stopped()) {
3613     // Since superc is primitive, make a guard for the superc==subc case.
3614     Node* cmp_eq = _gvn.transform(new (C) CmpPNode(args[0], args[1]));
3615     Node* bol_eq = _gvn.transform(new (C) BoolNode(cmp_eq, BoolTest::eq));
3616     generate_guard(bol_eq, region, PROB_FAIR);
3617     if (region-&gt;req() == PATH_LIMIT+1) {
3618       // A guard was added.  If the added guard is taken, superc==subc.
3619       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3620       region-&gt;del_req(PATH_LIMIT);
3621     }
3622     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3623   }
3624 
3625   // these are the only paths that produce 'true':
3626   phi-&gt;set_req(_prim_same_path,   intcon(1));
3627   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3628 
3629   // pull together the cases:
3630   assert(region-&gt;req() == PATH_LIMIT, "sane region");
3631   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3632     Node* ctl = region-&gt;in(i);
3633     if (ctl == NULL || ctl == top()) {
3634       region-&gt;set_req(i, top());
3635       phi   -&gt;set_req(i, top());
3636     } else if (phi-&gt;in(i) == NULL) {
3637       phi-&gt;set_req(i, intcon(0)); // all other paths produce 'false'
3638     }
3639   }
3640 
3641   set_control(_gvn.transform(region));
3642   set_result(_gvn.transform(phi));
3643   return true;
3644 }
3645 
3646 //---------------------generate_array_guard_common------------------------
3647 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,
3648                                                   bool obj_array, bool not_array) {
3649   // If obj_array/non_array==false/false:
3650   // Branch around if the given klass is in fact an array (either obj or prim).
3651   // If obj_array/non_array==false/true:
3652   // Branch around if the given klass is not an array klass of any kind.
3653   // If obj_array/non_array==true/true:
3654   // Branch around if the kls is not an oop array (kls is int[], String, etc.)
3655   // If obj_array/non_array==true/false:
3656   // Branch around if the kls is an oop array (Object[] or subtype)
3657   //
3658   // Like generate_guard, adds a new path onto the region.
3659   jint  layout_con = 0;
3660   Node* layout_val = get_layout_helper(kls, layout_con);
3661   if (layout_val == NULL) {
3662     bool query = (obj_array
3663                   ? Klass::layout_helper_is_objArray(layout_con)
3664                   : Klass::layout_helper_is_array(layout_con));
3665     if (query == not_array) {
3666       return NULL;                       // never a branch
3667     } else {                             // always a branch
3668       Node* always_branch = control();
3669       if (region != NULL)
3670         region-&gt;add_req(always_branch);
3671       set_control(top());
3672       return always_branch;
3673     }
3674   }
3675   // Now test the correct condition.
3676   jint  nval = (obj_array
3677                 ? ((jint)Klass::_lh_array_tag_type_value
3678                    &lt;&lt;    Klass::_lh_array_tag_shift)
3679                 : Klass::_lh_neutral_value);
3680   Node* cmp = _gvn.transform(new(C) CmpINode(layout_val, intcon(nval)));
3681   BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array
3682   // invert the test if we are looking for a non-array
3683   if (not_array)  btest = BoolTest(btest).negate();
3684   Node* bol = _gvn.transform(new(C) BoolNode(cmp, btest));
3685   return generate_fair_guard(bol, region);
3686 }
3687 
3688 
3689 //-----------------------inline_native_newArray--------------------------
3690 // private static native Object java.lang.reflect.newArray(Class&lt;?&gt; componentType, int length);
3691 bool LibraryCallKit::inline_native_newArray() {
3692   Node* mirror    = argument(0);
3693   Node* count_val = argument(1);
3694 
3695   mirror = null_check(mirror);
3696   // If mirror or obj is dead, only null-path is taken.
3697   if (stopped())  return true;
3698 
3699   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3700   RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
3701   PhiNode*    result_val = new(C) PhiNode(result_reg,
3702                                           TypeInstPtr::NOTNULL);
3703   PhiNode*    result_io  = new(C) PhiNode(result_reg, Type::ABIO);
3704   PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY,
3705                                           TypePtr::BOTTOM);
3706 
3707   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3708   Node* klass_node = load_array_klass_from_mirror(mirror, never_see_null,
3709                                                   result_reg, _slow_path);
3710   Node* normal_ctl   = control();
3711   Node* no_array_ctl = result_reg-&gt;in(_slow_path);
3712 
3713   // Generate code for the slow case.  We make a call to newArray().
3714   set_control(no_array_ctl);
3715   if (!stopped()) {
3716     // Either the input type is void.class, or else the
3717     // array klass has not yet been cached.  Either the
3718     // ensuing call will throw an exception, or else it
3719     // will cache the array klass for next time.
3720     PreserveJVMState pjvms(this);
3721     CallJavaNode* slow_call = generate_method_call_static(vmIntrinsics::_newArray);
3722     Node* slow_result = set_results_for_java_call(slow_call);
3723     // this-&gt;control() comes from set_results_for_java_call
3724     result_reg-&gt;set_req(_slow_path, control());
3725     result_val-&gt;set_req(_slow_path, slow_result);
3726     result_io -&gt;set_req(_slow_path, i_o());
3727     result_mem-&gt;set_req(_slow_path, reset_memory());
3728   }
3729 
3730   set_control(normal_ctl);
3731   if (!stopped()) {
3732     // Normal case:  The array type has been cached in the java.lang.Class.
3733     // The following call works fine even if the array type is polymorphic.
3734     // It could be a dynamic mix of int[], boolean[], Object[], etc.
3735     Node* obj = new_array(klass_node, count_val, 0);  // no arguments to push
3736     result_reg-&gt;init_req(_normal_path, control());
3737     result_val-&gt;init_req(_normal_path, obj);
3738     result_io -&gt;init_req(_normal_path, i_o());
3739     result_mem-&gt;init_req(_normal_path, reset_memory());
3740   }
3741 
3742   // Return the combined state.
3743   set_i_o(        _gvn.transform(result_io)  );
3744   set_all_memory( _gvn.transform(result_mem));
3745 
3746   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3747   set_result(result_reg, result_val);
3748   return true;
3749 }
3750 
3751 //----------------------inline_native_getLength--------------------------
3752 // public static native int java.lang.reflect.Array.getLength(Object array);
3753 bool LibraryCallKit::inline_native_getLength() {
3754   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3755 
3756   Node* array = null_check(argument(0));
3757   // If array is dead, only null-path is taken.
3758   if (stopped())  return true;
3759 
3760   // Deoptimize if it is a non-array.
3761   Node* non_array = generate_non_array_guard(load_object_klass(array), NULL);
3762 
3763   if (non_array != NULL) {
3764     PreserveJVMState pjvms(this);
3765     set_control(non_array);
3766     uncommon_trap(Deoptimization::Reason_intrinsic,
3767                   Deoptimization::Action_maybe_recompile);
3768   }
3769 
3770   // If control is dead, only non-array-path is taken.
3771   if (stopped())  return true;
3772 
3773   // The works fine even if the array type is polymorphic.
3774   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3775   Node* result = load_array_length(array);
3776 
3777   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3778   set_result(result);
3779   return true;
3780 }
3781 
3782 //------------------------inline_array_copyOf----------------------------
3783 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3784 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3785 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3786   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3787 
3788   // Get the arguments.
3789   Node* original          = argument(0);
3790   Node* start             = is_copyOfRange? argument(1): intcon(0);
3791   Node* end               = is_copyOfRange? argument(2): argument(1);
3792   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3793 
3794   Node* newcopy;
3795 
3796   // Set the original stack and the reexecute bit for the interpreter to reexecute
3797   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3798   { PreserveReexecuteState preexecs(this);
3799     jvms()-&gt;set_should_reexecute(true);
3800 
3801     array_type_mirror = null_check(array_type_mirror);
3802     original          = null_check(original);
3803 
3804     // Check if a null path was taken unconditionally.
3805     if (stopped())  return true;
3806 
3807     Node* orig_length = load_array_length(original);
3808 
3809     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3810     klass_node = null_check(klass_node);
3811 
3812     RegionNode* bailout = new (C) RegionNode(1);
3813     record_for_igvn(bailout);
3814 
3815     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3816     // Bail out if that is so.
3817     Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);
3818     if (not_objArray != NULL) {
3819       // Improve the klass node's type from the new optimistic assumption:
3820       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3821       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
3822       Node* cast = new (C) CastPPNode(klass_node, akls);
3823       cast-&gt;init_req(0, control());
3824       klass_node = _gvn.transform(cast);
3825     }
3826 
3827     // Bail out if either start or end is negative.
3828     generate_negative_guard(start, bailout, &amp;start);
3829     generate_negative_guard(end,   bailout, &amp;end);
3830 
3831     Node* length = end;
3832     if (_gvn.type(start) != TypeInt::ZERO) {
3833       length = _gvn.transform(new (C) SubINode(end, start));
3834     }
3835 
3836     // Bail out if length is negative.
3837     // Without this the new_array would throw
3838     // NegativeArraySizeException but IllegalArgumentException is what
3839     // should be thrown
3840     generate_negative_guard(length, bailout, &amp;length);
3841 
3842     if (bailout-&gt;req() &gt; 1) {
3843       PreserveJVMState pjvms(this);
3844       set_control(_gvn.transform(bailout));
3845       uncommon_trap(Deoptimization::Reason_intrinsic,
3846                     Deoptimization::Action_maybe_recompile);
3847     }
3848 
3849     if (!stopped()) {
3850       // How many elements will we copy from the original?
3851       // The answer is MinI(orig_length - start, length).
3852       Node* orig_tail = _gvn.transform(new (C) SubINode(orig_length, start));
3853       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
3854 
3855       newcopy = new_array(klass_node, length, 0);  // no argments to push
3856 
3857       // Generate a direct call to the right arraycopy function(s).
3858       // We know the copy is disjoint but we might not know if the
3859       // oop stores need checking.
3860       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
3861       // This will fail a store-check if x contains any non-nulls.
3862       bool disjoint_bases = true;
3863       // if start &gt; orig_length then the length of the copy may be
3864       // negative.
3865       bool length_never_negative = !is_copyOfRange;
3866       generate_arraycopy(TypeAryPtr::OOPS, T_OBJECT,
3867                          original, start, newcopy, intcon(0), moved,
3868                          disjoint_bases, length_never_negative);
3869     }
3870   } // original reexecute is set back here
3871 
3872   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3873   if (!stopped()) {
3874     set_result(newcopy);
3875   }
3876   return true;
3877 }
3878 
3879 
3880 //----------------------generate_virtual_guard---------------------------
3881 // Helper for hashCode and clone.  Peeks inside the vtable to avoid a call.
3882 Node* LibraryCallKit::generate_virtual_guard(Node* obj_klass,
3883                                              RegionNode* slow_region) {
3884   ciMethod* method = callee();
3885   int vtable_index = method-&gt;vtable_index();
3886   assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
3887          err_msg_res("bad index %d", vtable_index));
3888   // Get the Method* out of the appropriate vtable entry.
3889   int entry_offset  = (InstanceKlass::vtable_start_offset() +
3890                      vtable_index*vtableEntry::size()) * wordSize +
3891                      vtableEntry::method_offset_in_bytes();
3892   Node* entry_addr  = basic_plus_adr(obj_klass, entry_offset);
3893   Node* target_call = make_load(NULL, entry_addr, TypePtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3894 
3895   // Compare the target method with the expected method (e.g., Object.hashCode).
3896   const TypePtr* native_call_addr = TypeMetadataPtr::make(method);
3897 
3898   Node* native_call = makecon(native_call_addr);
3899   Node* chk_native  = _gvn.transform(new(C) CmpPNode(target_call, native_call));
3900   Node* test_native = _gvn.transform(new(C) BoolNode(chk_native, BoolTest::ne));
3901 
3902   return generate_slow_guard(test_native, slow_region);
3903 }
3904 
3905 //-----------------------generate_method_call----------------------------
3906 // Use generate_method_call to make a slow-call to the real
3907 // method if the fast path fails.  An alternative would be to
3908 // use a stub like OptoRuntime::slow_arraycopy_Java.
3909 // This only works for expanding the current library call,
3910 // not another intrinsic.  (E.g., don't use this for making an
3911 // arraycopy call inside of the copyOf intrinsic.)
3912 CallJavaNode*
3913 LibraryCallKit::generate_method_call(vmIntrinsics::ID method_id, bool is_virtual, bool is_static) {
3914   // When compiling the intrinsic method itself, do not use this technique.
3915   guarantee(callee() != C-&gt;method(), "cannot make slow-call to self");
3916 
3917   ciMethod* method = callee();
3918   // ensure the JVMS we have will be correct for this call
3919   guarantee(method_id == method-&gt;intrinsic_id(), "must match");
3920 
3921   const TypeFunc* tf = TypeFunc::make(method);
3922   CallJavaNode* slow_call;
3923   if (is_static) {
3924     assert(!is_virtual, "");
3925     slow_call = new(C) CallStaticJavaNode(C, tf,
3926                            SharedRuntime::get_resolve_static_call_stub(),
3927                            method, bci());
3928   } else if (is_virtual) {
3929     null_check_receiver();
3930     int vtable_index = Method::invalid_vtable_index;
3931     if (UseInlineCaches) {
3932       // Suppress the vtable call
3933     } else {
3934       // hashCode and clone are not a miranda methods,
3935       // so the vtable index is fixed.
3936       // No need to use the linkResolver to get it.
3937        vtable_index = method-&gt;vtable_index();
3938        assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
3939               err_msg_res("bad index %d", vtable_index));
3940     }
3941     slow_call = new(C) CallDynamicJavaNode(tf,
3942                           SharedRuntime::get_resolve_virtual_call_stub(),
3943                           method, vtable_index, bci());
3944   } else {  // neither virtual nor static:  opt_virtual
3945     null_check_receiver();
3946     slow_call = new(C) CallStaticJavaNode(C, tf,
3947                                 SharedRuntime::get_resolve_opt_virtual_call_stub(),
3948                                 method, bci());
3949     slow_call-&gt;set_optimized_virtual(true);
3950   }
3951   set_arguments_for_java_call(slow_call);
3952   set_edges_for_java_call(slow_call);
3953   return slow_call;
3954 }
3955 
3956 
3957 //------------------------------inline_native_hashcode--------------------
3958 // Build special case code for calls to hashCode on an object.
3959 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
3960   assert(is_static == callee()-&gt;is_static(), "correct intrinsic selection");
3961   assert(!(is_virtual &amp;&amp; is_static), "either virtual, special, or static");
3962 
3963   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
3964 
3965   RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
3966   PhiNode*    result_val = new(C) PhiNode(result_reg,
3967                                           TypeInt::INT);
3968   PhiNode*    result_io  = new(C) PhiNode(result_reg, Type::ABIO);
3969   PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY,
3970                                           TypePtr::BOTTOM);
3971   Node* obj = NULL;
3972   if (!is_static) {
3973     // Check for hashing null object
3974     obj = null_check_receiver();
3975     if (stopped())  return true;        // unconditionally null
3976     result_reg-&gt;init_req(_null_path, top());
3977     result_val-&gt;init_req(_null_path, top());
3978   } else {
3979     // Do a null check, and return zero if null.
3980     // System.identityHashCode(null) == 0
3981     obj = argument(0);
3982     Node* null_ctl = top();
3983     obj = null_check_oop(obj, &amp;null_ctl);
3984     result_reg-&gt;init_req(_null_path, null_ctl);
3985     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
3986   }
3987 
3988   // Unconditionally null?  Then return right away.
3989   if (stopped()) {
3990     set_control( result_reg-&gt;in(_null_path));
3991     if (!stopped())
3992       set_result(result_val-&gt;in(_null_path));
3993     return true;
3994   }
3995 
3996   // After null check, get the object's klass.
3997   Node* obj_klass = load_object_klass(obj);
3998 
3999   // This call may be virtual (invokevirtual) or bound (invokespecial).
4000   // For each case we generate slightly different code.
4001 
4002   // We only go to the fast case code if we pass a number of guards.  The
4003   // paths which do not pass are accumulated in the slow_region.
4004   RegionNode* slow_region = new (C) RegionNode(1);
4005   record_for_igvn(slow_region);
4006 
4007   // If this is a virtual call, we generate a funny guard.  We pull out
4008   // the vtable entry corresponding to hashCode() from the target object.
4009   // If the target method which we are calling happens to be the native
4010   // Object hashCode() method, we pass the guard.  We do not need this
4011   // guard for non-virtual calls -- the caller is known to be the native
4012   // Object hashCode().
4013   if (is_virtual) {
4014     generate_virtual_guard(obj_klass, slow_region);
4015   }
4016 
4017   // Get the header out of the object, use LoadMarkNode when available
4018   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
4019   Node* header = make_load(control(), header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
4020 
4021   // Test the header to see if it is unlocked.
4022   Node *lock_mask      = _gvn.MakeConX(markOopDesc::biased_lock_mask_in_place);
4023   Node *lmasked_header = _gvn.transform(new (C) AndXNode(header, lock_mask));
4024   Node *unlocked_val   = _gvn.MakeConX(markOopDesc::unlocked_value);
4025   Node *chk_unlocked   = _gvn.transform(new (C) CmpXNode( lmasked_header, unlocked_val));
4026   Node *test_unlocked  = _gvn.transform(new (C) BoolNode( chk_unlocked, BoolTest::ne));
4027 
4028   generate_slow_guard(test_unlocked, slow_region);
4029 
4030   // Get the hash value and check to see that it has been properly assigned.
4031   // We depend on hash_mask being at most 32 bits and avoid the use of
4032   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
4033   // vm: see markOop.hpp.
4034   Node *hash_mask      = _gvn.intcon(markOopDesc::hash_mask);
4035   Node *hash_shift     = _gvn.intcon(markOopDesc::hash_shift);
4036   Node *hshifted_header= _gvn.transform(new (C) URShiftXNode(header, hash_shift));
4037   // This hack lets the hash bits live anywhere in the mark object now, as long
4038   // as the shift drops the relevant bits into the low 32 bits.  Note that
4039   // Java spec says that HashCode is an int so there's no point in capturing
4040   // an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
4041   hshifted_header      = ConvX2I(hshifted_header);
4042   Node *hash_val       = _gvn.transform(new (C) AndINode(hshifted_header, hash_mask));
4043 
4044   Node *no_hash_val    = _gvn.intcon(markOopDesc::no_hash);
4045   Node *chk_assigned   = _gvn.transform(new (C) CmpINode( hash_val, no_hash_val));
4046   Node *test_assigned  = _gvn.transform(new (C) BoolNode( chk_assigned, BoolTest::eq));
4047 
4048   generate_slow_guard(test_assigned, slow_region);
4049 
4050   Node* init_mem = reset_memory();
4051   // fill in the rest of the null path:
4052   result_io -&gt;init_req(_null_path, i_o());
4053   result_mem-&gt;init_req(_null_path, init_mem);
4054 
4055   result_val-&gt;init_req(_fast_path, hash_val);
4056   result_reg-&gt;init_req(_fast_path, control());
4057   result_io -&gt;init_req(_fast_path, i_o());
4058   result_mem-&gt;init_req(_fast_path, init_mem);
4059 
4060   // Generate code for the slow case.  We make a call to hashCode().
4061   set_control(_gvn.transform(slow_region));
4062   if (!stopped()) {
4063     // No need for PreserveJVMState, because we're using up the present state.
4064     set_all_memory(init_mem);
4065     vmIntrinsics::ID hashCode_id = is_static ? vmIntrinsics::_identityHashCode : vmIntrinsics::_hashCode;
4066     CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static);
4067     Node* slow_result = set_results_for_java_call(slow_call);
4068     // this-&gt;control() comes from set_results_for_java_call
4069     result_reg-&gt;init_req(_slow_path, control());
4070     result_val-&gt;init_req(_slow_path, slow_result);
4071     result_io  -&gt;set_req(_slow_path, i_o());
4072     result_mem -&gt;set_req(_slow_path, reset_memory());
4073   }
4074 
4075   // Return the combined state.
4076   set_i_o(        _gvn.transform(result_io)  );
4077   set_all_memory( _gvn.transform(result_mem));
4078 
4079   set_result(result_reg, result_val);
4080   return true;
4081 }
4082 
4083 //---------------------------inline_native_getClass----------------------------
4084 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4085 //
4086 // Build special case code for calls to getClass on an object.
4087 bool LibraryCallKit::inline_native_getClass() {
4088   Node* obj = null_check_receiver();
4089   if (stopped())  return true;
4090   set_result(load_mirror_from_klass(load_object_klass(obj)));
4091   return true;
4092 }
4093 
4094 //-----------------inline_native_Reflection_getCallerClass---------------------
4095 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4096 //
4097 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4098 //
4099 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4100 // in that it must skip particular security frames and checks for
4101 // caller sensitive methods.
4102 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4103 #ifndef PRODUCT
4104   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4105     tty-&gt;print_cr("Attempting to inline sun.reflect.Reflection.getCallerClass");
4106   }
4107 #endif
4108 
4109   if (!jvms()-&gt;has_method()) {
4110 #ifndef PRODUCT
4111     if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4112       tty-&gt;print_cr("  Bailing out because intrinsic was inlined at top level");
4113     }
4114 #endif
4115     return false;
4116   }
4117 
4118   // Walk back up the JVM state to find the caller at the required
4119   // depth.
4120   JVMState* caller_jvms = jvms();
4121 
4122   // Cf. JVM_GetCallerClass
4123   // NOTE: Start the loop at depth 1 because the current JVM state does
4124   // not include the Reflection.getCallerClass() frame.
4125   for (int n = 1; caller_jvms != NULL; caller_jvms = caller_jvms-&gt;caller(), n++) {
4126     ciMethod* m = caller_jvms-&gt;method();
4127     switch (n) {
4128     case 0:
4129       fatal("current JVM state does not include the Reflection.getCallerClass frame");
4130       break;
4131     case 1:
4132       // Frame 0 and 1 must be caller sensitive (see JVM_GetCallerClass).
4133       if (!m-&gt;caller_sensitive()) {
4134 #ifndef PRODUCT
4135         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4136           tty-&gt;print_cr("  Bailing out: CallerSensitive annotation expected at frame %d", n);
4137         }
4138 #endif
4139         return false;  // bail-out; let JVM_GetCallerClass do the work
4140       }
4141       break;
4142     default:
4143       if (!m-&gt;is_ignored_by_security_stack_walk()) {
4144         // We have reached the desired frame; return the holder class.
4145         // Acquire method holder as java.lang.Class and push as constant.
4146         ciInstanceKlass* caller_klass = caller_jvms-&gt;method()-&gt;holder();
4147         ciInstance* caller_mirror = caller_klass-&gt;java_mirror();
4148         set_result(makecon(TypeInstPtr::make(caller_mirror)));
4149 
4150 #ifndef PRODUCT
4151         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4152           tty-&gt;print_cr("  Succeeded: caller = %d) %s.%s, JVMS depth = %d", n, caller_klass-&gt;name()-&gt;as_utf8(), caller_jvms-&gt;method()-&gt;name()-&gt;as_utf8(), jvms()-&gt;depth());
4153           tty-&gt;print_cr("  JVM state at this point:");
4154           for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4155             ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4156             tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4157           }
4158         }
4159 #endif
4160         return true;
4161       }
4162       break;
4163     }
4164   }
4165 
4166 #ifndef PRODUCT
4167   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4168     tty-&gt;print_cr("  Bailing out because caller depth exceeded inlining depth = %d", jvms()-&gt;depth());
4169     tty-&gt;print_cr("  JVM state at this point:");
4170     for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4171       ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4172       tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4173     }
4174   }
4175 #endif
4176 
4177   return false;  // bail-out; let JVM_GetCallerClass do the work
4178 }
4179 
4180 bool LibraryCallKit::inline_fp_conversions(vmIntrinsics::ID id) {
4181   Node* arg = argument(0);
4182   Node* result;
4183 
4184   switch (id) {
4185   case vmIntrinsics::_floatToRawIntBits:    result = new (C) MoveF2INode(arg);  break;
4186   case vmIntrinsics::_intBitsToFloat:       result = new (C) MoveI2FNode(arg);  break;
4187   case vmIntrinsics::_doubleToRawLongBits:  result = new (C) MoveD2LNode(arg);  break;
4188   case vmIntrinsics::_longBitsToDouble:     result = new (C) MoveL2DNode(arg);  break;
4189 
4190   case vmIntrinsics::_doubleToLongBits: {
4191     // two paths (plus control) merge in a wood
4192     RegionNode *r = new (C) RegionNode(3);
4193     Node *phi = new (C) PhiNode(r, TypeLong::LONG);
4194 
4195     Node *cmpisnan = _gvn.transform(new (C) CmpDNode(arg, arg));
4196     // Build the boolean node
4197     Node *bolisnan = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::ne));
4198 
4199     // Branch either way.
4200     // NaN case is less traveled, which makes all the difference.
4201     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4202     Node *opt_isnan = _gvn.transform(ifisnan);
4203     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4204     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4205     Node *iftrue = _gvn.transform(new (C) IfTrueNode(opt_ifisnan));
4206 
4207     set_control(iftrue);
4208 
4209     static const jlong nan_bits = CONST64(0x7ff8000000000000);
4210     Node *slow_result = longcon(nan_bits); // return NaN
4211     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4212     r-&gt;init_req(1, iftrue);
4213 
4214     // Else fall through
4215     Node *iffalse = _gvn.transform(new (C) IfFalseNode(opt_ifisnan));
4216     set_control(iffalse);
4217 
4218     phi-&gt;init_req(2, _gvn.transform(new (C) MoveD2LNode(arg)));
4219     r-&gt;init_req(2, iffalse);
4220 
4221     // Post merge
4222     set_control(_gvn.transform(r));
4223     record_for_igvn(r);
4224 
4225     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4226     result = phi;
4227     assert(result-&gt;bottom_type()-&gt;isa_long(), "must be");
4228     break;
4229   }
4230 
4231   case vmIntrinsics::_floatToIntBits: {
4232     // two paths (plus control) merge in a wood
4233     RegionNode *r = new (C) RegionNode(3);
4234     Node *phi = new (C) PhiNode(r, TypeInt::INT);
4235 
4236     Node *cmpisnan = _gvn.transform(new (C) CmpFNode(arg, arg));
4237     // Build the boolean node
4238     Node *bolisnan = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::ne));
4239 
4240     // Branch either way.
4241     // NaN case is less traveled, which makes all the difference.
4242     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4243     Node *opt_isnan = _gvn.transform(ifisnan);
4244     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4245     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4246     Node *iftrue = _gvn.transform(new (C) IfTrueNode(opt_ifisnan));
4247 
4248     set_control(iftrue);
4249 
4250     static const jint nan_bits = 0x7fc00000;
4251     Node *slow_result = makecon(TypeInt::make(nan_bits)); // return NaN
4252     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4253     r-&gt;init_req(1, iftrue);
4254 
4255     // Else fall through
4256     Node *iffalse = _gvn.transform(new (C) IfFalseNode(opt_ifisnan));
4257     set_control(iffalse);
4258 
4259     phi-&gt;init_req(2, _gvn.transform(new (C) MoveF2INode(arg)));
4260     r-&gt;init_req(2, iffalse);
4261 
4262     // Post merge
4263     set_control(_gvn.transform(r));
4264     record_for_igvn(r);
4265 
4266     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4267     result = phi;
4268     assert(result-&gt;bottom_type()-&gt;isa_int(), "must be");
4269     break;
4270   }
4271 
4272   default:
4273     fatal_unexpected_iid(id);
4274     break;
4275   }
4276   set_result(_gvn.transform(result));
4277   return true;
4278 }
4279 
4280 #ifdef _LP64
4281 #define XTOP ,top() /*additional argument*/
4282 #else  //_LP64
4283 #define XTOP        /*no additional argument*/
4284 #endif //_LP64
4285 
4286 //----------------------inline_unsafe_copyMemory-------------------------
4287 // public native void sun.misc.Unsafe.copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);
4288 bool LibraryCallKit::inline_unsafe_copyMemory() {
4289   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
4290   null_check_receiver();  // null-check receiver
4291   if (stopped())  return true;
4292 
4293   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
4294 
4295   Node* src_ptr =         argument(1);   // type: oop
4296   Node* src_off = ConvL2X(argument(2));  // type: long
4297   Node* dst_ptr =         argument(4);   // type: oop
4298   Node* dst_off = ConvL2X(argument(5));  // type: long
4299   Node* size    = ConvL2X(argument(7));  // type: long
4300 
4301   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
4302          "fieldOffset must be byte-scaled");
4303 
4304   Node* src = make_unsafe_address(src_ptr, src_off);
4305   Node* dst = make_unsafe_address(dst_ptr, dst_off);
4306 
4307   // Conservatively insert a memory barrier on all memory slices.
4308   // Do not let writes of the copy source or destination float below the copy.
4309   insert_mem_bar(Op_MemBarCPUOrder);
4310 
4311   // Call it.  Note that the length argument is not scaled.
4312   make_runtime_call(RC_LEAF|RC_NO_FP,
4313                     OptoRuntime::fast_arraycopy_Type(),
4314                     StubRoutines::unsafe_arraycopy(),
4315                     "unsafe_arraycopy",
4316                     TypeRawPtr::BOTTOM,
4317                     src, dst, size XTOP);
4318 
4319   // Do not let reads of the copy destination float above the copy.
4320   insert_mem_bar(Op_MemBarCPUOrder);
4321 
4322   return true;
4323 }
4324 
4325 //------------------------clone_coping-----------------------------------
4326 // Helper function for inline_native_clone.
4327 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark) {
4328   assert(obj_size != NULL, "");
4329   Node* raw_obj = alloc_obj-&gt;in(1);
4330   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), "");
4331 
4332   AllocateNode* alloc = NULL;
4333   if (ReduceBulkZeroing) {
4334     // We will be completely responsible for initializing this object -
4335     // mark Initialize node as complete.
4336     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4337     // The object was just allocated - there should be no any stores!
4338     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), "");
4339     // Mark as complete_with_arraycopy so that on AllocateNode
4340     // expansion, we know this AllocateNode is initialized by an array
4341     // copy and a StoreStore barrier exists after the array copy.
4342     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4343   }
4344 
4345   // Copy the fastest available way.
4346   // TODO: generate fields copies for small objects instead.
4347   Node* src  = obj;
4348   Node* dest = alloc_obj;
4349   Node* size = _gvn.transform(obj_size);
4350 
4351   // Exclude the header but include array length to copy by 8 bytes words.
4352   // Can't use base_offset_in_bytes(bt) since basic type is unknown.
4353   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
4354                             instanceOopDesc::base_offset_in_bytes();
4355   // base_off:
4356   // 8  - 32-bit VM
4357   // 12 - 64-bit VM, compressed klass
4358   // 16 - 64-bit VM, normal klass
4359   if (base_off % BytesPerLong != 0) {
4360     assert(UseCompressedClassPointers, "");
4361     if (is_array) {
4362       // Exclude length to copy by 8 bytes words.
4363       base_off += sizeof(int);
4364     } else {
4365       // Include klass to copy by 8 bytes words.
4366       base_off = instanceOopDesc::klass_offset_in_bytes();
4367     }
4368     assert(base_off % BytesPerLong == 0, "expect 8 bytes alignment");
4369   }
4370   src  = basic_plus_adr(src,  base_off);
4371   dest = basic_plus_adr(dest, base_off);
4372 
4373   // Compute the length also, if needed:
4374   Node* countx = size;
4375   countx = _gvn.transform(new (C) SubXNode(countx, MakeConX(base_off)));
4376   countx = _gvn.transform(new (C) URShiftXNode(countx, intcon(LogBytesPerLong) ));
4377 
4378   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4379   bool disjoint_bases = true;
4380   generate_unchecked_arraycopy(raw_adr_type, T_LONG, disjoint_bases,
4381                                src, NULL, dest, NULL, countx,
4382                                /*dest_uninitialized*/true);
4383 
4384   // If necessary, emit some card marks afterwards.  (Non-arrays only.)
4385   if (card_mark) {
4386     assert(!is_array, "");
4387     // Put in store barrier for any and all oops we are sticking
4388     // into this object.  (We could avoid this if we could prove
4389     // that the object type contains no oop fields at all.)
4390     Node* no_particular_value = NULL;
4391     Node* no_particular_field = NULL;
4392     int raw_adr_idx = Compile::AliasIdxRaw;
4393     post_barrier(control(),
4394                  memory(raw_adr_type),
4395                  alloc_obj,
4396                  no_particular_field,
4397                  raw_adr_idx,
4398                  no_particular_value,
4399                  T_OBJECT,
4400                  false);
4401   }
4402 
4403   // Do not let reads from the cloned object float above the arraycopy.
4404   if (alloc != NULL) {
4405     // Do not let stores that initialize this object be reordered with
4406     // a subsequent store that would make this object accessible by
4407     // other threads.
4408     // Record what AllocateNode this StoreStore protects so that
4409     // escape analysis can go from the MemBarStoreStoreNode to the
4410     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4411     // based on the escape status of the AllocateNode.
4412     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
4413   } else {
4414     insert_mem_bar(Op_MemBarCPUOrder);
4415   }
4416 }
4417 
4418 //------------------------inline_native_clone----------------------------
4419 // protected native Object java.lang.Object.clone();
4420 //
4421 // Here are the simple edge cases:
4422 //  null receiver =&gt; normal trap
4423 //  virtual and clone was overridden =&gt; slow path to out-of-line clone
4424 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4425 //
4426 // The general case has two steps, allocation and copying.
4427 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4428 //
4429 // Copying also has two cases, oop arrays and everything else.
4430 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4431 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4432 //
4433 // These steps fold up nicely if and when the cloned object's klass
4434 // can be sharply typed as an object array, a type array, or an instance.
4435 //
4436 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4437   PhiNode* result_val;
4438 
4439   // Set the reexecute bit for the interpreter to reexecute
4440   // the bytecode that invokes Object.clone if deoptimization happens.
4441   { PreserveReexecuteState preexecs(this);
4442     jvms()-&gt;set_should_reexecute(true);
4443 
4444     Node* obj = null_check_receiver();
4445     if (stopped())  return true;
4446 
4447     Node* obj_klass = load_object_klass(obj);
4448     const TypeKlassPtr* tklass = _gvn.type(obj_klass)-&gt;isa_klassptr();
4449     const TypeOopPtr*   toop   = ((tklass != NULL)
4450                                 ? tklass-&gt;as_instance_type()
4451                                 : TypeInstPtr::NOTNULL);
4452 
4453     // Conservatively insert a memory barrier on all memory slices.
4454     // Do not let writes into the original float below the clone.
4455     insert_mem_bar(Op_MemBarCPUOrder);
4456 
4457     // paths into result_reg:
4458     enum {
4459       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4460       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4461       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4462       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4463       PATH_LIMIT
4464     };
4465     RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
4466     result_val             = new(C) PhiNode(result_reg,
4467                                             TypeInstPtr::NOTNULL);
4468     PhiNode*    result_i_o = new(C) PhiNode(result_reg, Type::ABIO);
4469     PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY,
4470                                             TypePtr::BOTTOM);
4471     record_for_igvn(result_reg);
4472 
4473     const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4474     int raw_adr_idx = Compile::AliasIdxRaw;
4475 
4476     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4477     if (array_ctl != NULL) {
4478       // It's an array.
4479       PreserveJVMState pjvms(this);
4480       set_control(array_ctl);
4481       Node* obj_length = load_array_length(obj);
4482       Node* obj_size  = NULL;
4483       Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push
4484 
4485       if (!use_ReduceInitialCardMarks()) {
4486         // If it is an oop array, it requires very special treatment,
4487         // because card marking is required on each card of the array.
4488         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4489         if (is_obja != NULL) {
4490           PreserveJVMState pjvms2(this);
4491           set_control(is_obja);
4492           // Generate a direct call to the right arraycopy function(s).
4493           bool disjoint_bases = true;
4494           bool length_never_negative = true;
4495           generate_arraycopy(TypeAryPtr::OOPS, T_OBJECT,
4496                              obj, intcon(0), alloc_obj, intcon(0),
4497                              obj_length,
4498                              disjoint_bases, length_never_negative);
4499           result_reg-&gt;init_req(_objArray_path, control());
4500           result_val-&gt;init_req(_objArray_path, alloc_obj);
4501           result_i_o -&gt;set_req(_objArray_path, i_o());
4502           result_mem -&gt;set_req(_objArray_path, reset_memory());
4503         }
4504       }
4505       // Otherwise, there are no card marks to worry about.
4506       // (We can dispense with card marks if we know the allocation
4507       //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
4508       //  causes the non-eden paths to take compensating steps to
4509       //  simulate a fresh allocation, so that no further
4510       //  card marks are required in compiled code to initialize
4511       //  the object.)
4512 
4513       if (!stopped()) {
4514         copy_to_clone(obj, alloc_obj, obj_size, true, false);
4515 
4516         // Present the results of the copy.
4517         result_reg-&gt;init_req(_array_path, control());
4518         result_val-&gt;init_req(_array_path, alloc_obj);
4519         result_i_o -&gt;set_req(_array_path, i_o());
4520         result_mem -&gt;set_req(_array_path, reset_memory());
4521       }
4522     }
4523 
4524     // We only go to the instance fast case code if we pass a number of guards.
4525     // The paths which do not pass are accumulated in the slow_region.
4526     RegionNode* slow_region = new (C) RegionNode(1);
4527     record_for_igvn(slow_region);
4528     if (!stopped()) {
4529       // It's an instance (we did array above).  Make the slow-path tests.
4530       // If this is a virtual call, we generate a funny guard.  We grab
4531       // the vtable entry corresponding to clone() from the target object.
4532       // If the target method which we are calling happens to be the
4533       // Object clone() method, we pass the guard.  We do not need this
4534       // guard for non-virtual calls; the caller is known to be the native
4535       // Object clone().
4536       if (is_virtual) {
4537         generate_virtual_guard(obj_klass, slow_region);
4538       }
4539 
4540       // The object must be cloneable and must not have a finalizer.
4541       // Both of these conditions may be checked in a single test.
4542       // We could optimize the cloneable test further, but we don't care.
4543       generate_access_flags_guard(obj_klass,
4544                                   // Test both conditions:
4545                                   JVM_ACC_IS_CLONEABLE | JVM_ACC_HAS_FINALIZER,
4546                                   // Must be cloneable but not finalizer:
4547                                   JVM_ACC_IS_CLONEABLE,
4548                                   slow_region);
4549     }
4550 
4551     if (!stopped()) {
4552       // It's an instance, and it passed the slow-path tests.
4553       PreserveJVMState pjvms(this);
4554       Node* obj_size  = NULL;
4555       Node* alloc_obj = new_instance(obj_klass, NULL, &amp;obj_size);
4556 
4557       copy_to_clone(obj, alloc_obj, obj_size, false, !use_ReduceInitialCardMarks());
4558 
4559       // Present the results of the slow call.
4560       result_reg-&gt;init_req(_instance_path, control());
4561       result_val-&gt;init_req(_instance_path, alloc_obj);
4562       result_i_o -&gt;set_req(_instance_path, i_o());
4563       result_mem -&gt;set_req(_instance_path, reset_memory());
4564     }
4565 
4566     // Generate code for the slow case.  We make a call to clone().
4567     set_control(_gvn.transform(slow_region));
4568     if (!stopped()) {
4569       PreserveJVMState pjvms(this);
4570       CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual);
4571       Node* slow_result = set_results_for_java_call(slow_call);
4572       // this-&gt;control() comes from set_results_for_java_call
4573       result_reg-&gt;init_req(_slow_path, control());
4574       result_val-&gt;init_req(_slow_path, slow_result);
4575       result_i_o -&gt;set_req(_slow_path, i_o());
4576       result_mem -&gt;set_req(_slow_path, reset_memory());
4577     }
4578 
4579     // Return the combined state.
4580     set_control(    _gvn.transform(result_reg));
4581     set_i_o(        _gvn.transform(result_i_o));
4582     set_all_memory( _gvn.transform(result_mem));
4583   } // original reexecute is set back here
4584 
4585   set_result(_gvn.transform(result_val));
4586   return true;
4587 }
4588 
4589 //------------------------------basictype2arraycopy----------------------------
4590 address LibraryCallKit::basictype2arraycopy(BasicType t,
4591                                             Node* src_offset,
4592                                             Node* dest_offset,
4593                                             bool disjoint_bases,
4594                                             const char* &amp;name,
4595                                             bool dest_uninitialized) {
4596   const TypeInt* src_offset_inttype  = gvn().find_int_type(src_offset);;
4597   const TypeInt* dest_offset_inttype = gvn().find_int_type(dest_offset);;
4598 
4599   bool aligned = false;
4600   bool disjoint = disjoint_bases;
4601 
4602   // if the offsets are the same, we can treat the memory regions as
4603   // disjoint, because either the memory regions are in different arrays,
4604   // or they are identical (which we can treat as disjoint.)  We can also
4605   // treat a copy with a destination index  less that the source index
4606   // as disjoint since a low-&gt;high copy will work correctly in this case.
4607   if (src_offset_inttype != NULL &amp;&amp; src_offset_inttype-&gt;is_con() &amp;&amp;
4608       dest_offset_inttype != NULL &amp;&amp; dest_offset_inttype-&gt;is_con()) {
4609     // both indices are constants
4610     int s_offs = src_offset_inttype-&gt;get_con();
4611     int d_offs = dest_offset_inttype-&gt;get_con();
4612     int element_size = type2aelembytes(t);
4613     aligned = ((arrayOopDesc::base_offset_in_bytes(t) + s_offs * element_size) % HeapWordSize == 0) &amp;&amp;
4614               ((arrayOopDesc::base_offset_in_bytes(t) + d_offs * element_size) % HeapWordSize == 0);
4615     if (s_offs &gt;= d_offs)  disjoint = true;
4616   } else if (src_offset == dest_offset &amp;&amp; src_offset != NULL) {
4617     // This can occur if the offsets are identical non-constants.
4618     disjoint = true;
4619   }
4620 
4621   return StubRoutines::select_arraycopy_function(t, aligned, disjoint, name, dest_uninitialized);
4622 }
4623 
4624 
4625 //------------------------------inline_arraycopy-----------------------
4626 // public static native void java.lang.System.arraycopy(Object src,  int  srcPos,
4627 //                                                      Object dest, int destPos,
4628 //                                                      int length);
4629 bool LibraryCallKit::inline_arraycopy() {
4630   // Get the arguments.
4631   Node* src         = argument(0);  // type: oop
4632   Node* src_offset  = argument(1);  // type: int
4633   Node* dest        = argument(2);  // type: oop
4634   Node* dest_offset = argument(3);  // type: int
4635   Node* length      = argument(4);  // type: int
4636 
4637   // Compile time checks.  If any of these checks cannot be verified at compile time,
4638   // we do not make a fast path for this call.  Instead, we let the call remain as it
4639   // is.  The checks we choose to mandate at compile time are:
4640   //
4641   // (1) src and dest are arrays.
4642   const Type* src_type  = src-&gt;Value(&amp;_gvn);
4643   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
4644   const TypeAryPtr* top_src  = src_type-&gt;isa_aryptr();
4645   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
4646 
4647   // Do we have the type of src?
4648   bool has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4649   // Do we have the type of dest?
4650   bool has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4651   // Is the type for src from speculation?
4652   bool src_spec = false;
4653   // Is the type for dest from speculation?
4654   bool dest_spec = false;
4655 
4656   if (!has_src || !has_dest) {
4657     // We don't have sufficient type information, let's see if
4658     // speculative types can help. We need to have types for both src
4659     // and dest so that it pays off.
4660 
4661     // Do we already have or could we have type information for src
4662     bool could_have_src = has_src;
4663     // Do we already have or could we have type information for dest
4664     bool could_have_dest = has_dest;
4665 
4666     ciKlass* src_k = NULL;
4667     if (!has_src) {
4668       src_k = src_type-&gt;speculative_type_not_null();
4669       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4670         could_have_src = true;
4671       }
4672     }
4673 
4674     ciKlass* dest_k = NULL;
4675     if (!has_dest) {
4676       dest_k = dest_type-&gt;speculative_type_not_null();
4677       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4678         could_have_dest = true;
4679       }
4680     }
4681 
4682     if (could_have_src &amp;&amp; could_have_dest) {
4683       // This is going to pay off so emit the required guards
4684       if (!has_src) {
4685         src = maybe_cast_profiled_obj(src, src_k);
4686         src_type  = _gvn.type(src);
4687         top_src  = src_type-&gt;isa_aryptr();
4688         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4689         src_spec = true;
4690       }
4691       if (!has_dest) {
4692         dest = maybe_cast_profiled_obj(dest, dest_k);
4693         dest_type  = _gvn.type(dest);
4694         top_dest  = dest_type-&gt;isa_aryptr();
4695         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4696         dest_spec = true;
4697       }
4698     }
4699   }
4700 
4701   if (!has_src || !has_dest) {
4702     // Conservatively insert a memory barrier on all memory slices.
4703     // Do not let writes into the source float below the arraycopy.
4704     insert_mem_bar(Op_MemBarCPUOrder);
4705 
4706     // Call StubRoutines::generic_arraycopy stub.
4707     generate_arraycopy(TypeRawPtr::BOTTOM, T_CONFLICT,
4708                        src, src_offset, dest, dest_offset, length);
4709 
4710     // Do not let reads from the destination float above the arraycopy.
4711     // Since we cannot type the arrays, we don't know which slices
4712     // might be affected.  We could restrict this barrier only to those
4713     // memory slices which pertain to array elements--but don't bother.
4714     if (!InsertMemBarAfterArraycopy)
4715       // (If InsertMemBarAfterArraycopy, there is already one in place.)
4716       insert_mem_bar(Op_MemBarCPUOrder);
4717     return true;
4718   }
4719 
4720   // (2) src and dest arrays must have elements of the same BasicType
4721   // Figure out the size and type of the elements we will be copying.
4722   BasicType src_elem  =  top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4723   BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4724   if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
4725   if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
4726 
4727   if (src_elem != dest_elem || dest_elem == T_VOID) {
4728     // The component types are not the same or are not recognized.  Punt.
4729     // (But, avoid the native method wrapper to JVM_ArrayCopy.)
4730     generate_slow_arraycopy(TypePtr::BOTTOM,
4731                             src, src_offset, dest, dest_offset, length,
4732                             /*dest_uninitialized*/false);
4733     return true;
4734   }
4735 
4736   if (src_elem == T_OBJECT) {
4737     // If both arrays are object arrays then having the exact types
4738     // for both will remove the need for a subtype check at runtime
4739     // before the call and may make it possible to pick a faster copy
4740     // routine (without a subtype check on every element)
4741     // Do we have the exact type of src?
4742     bool could_have_src = src_spec;
4743     // Do we have the exact type of dest?
4744     bool could_have_dest = dest_spec;
4745     ciKlass* src_k = top_src-&gt;klass();
4746     ciKlass* dest_k = top_dest-&gt;klass();
4747     if (!src_spec) {
4748       src_k = src_type-&gt;speculative_type_not_null();
4749       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4750           could_have_src = true;
4751       }
4752     }
4753     if (!dest_spec) {
4754       dest_k = dest_type-&gt;speculative_type_not_null();
4755       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4756         could_have_dest = true;
4757       }
4758     }
4759     if (could_have_src &amp;&amp; could_have_dest) {
4760       // If we can have both exact types, emit the missing guards
4761       if (could_have_src &amp;&amp; !src_spec) {
4762         src = maybe_cast_profiled_obj(src, src_k);
4763       }
4764       if (could_have_dest &amp;&amp; !dest_spec) {
4765         dest = maybe_cast_profiled_obj(dest, dest_k);
4766       }
4767     }
4768   }
4769 
4770   //---------------------------------------------------------------------------
4771   // We will make a fast path for this call to arraycopy.
4772 
4773   // We have the following tests left to perform:
4774   //
4775   // (3) src and dest must not be null.
4776   // (4) src_offset must not be negative.
4777   // (5) dest_offset must not be negative.
4778   // (6) length must not be negative.
4779   // (7) src_offset + length must not exceed length of src.
4780   // (8) dest_offset + length must not exceed length of dest.
4781   // (9) each element of an oop array must be assignable
4782 
4783   RegionNode* slow_region = new (C) RegionNode(1);
4784   record_for_igvn(slow_region);
4785 
4786   // (3) operands must not be null
4787   // We currently perform our null checks with the null_check routine.
4788   // This means that the null exceptions will be reported in the caller
4789   // rather than (correctly) reported inside of the native arraycopy call.
4790   // This should be corrected, given time.  We do our null check with the
4791   // stack pointer restored.
4792   src  = null_check(src,  T_ARRAY);
4793   dest = null_check(dest, T_ARRAY);
4794 
4795   // (4) src_offset must not be negative.
4796   generate_negative_guard(src_offset, slow_region);
4797 
4798   // (5) dest_offset must not be negative.
4799   generate_negative_guard(dest_offset, slow_region);
4800 
4801   // (6) length must not be negative (moved to generate_arraycopy()).
4802   // generate_negative_guard(length, slow_region);
4803 
4804   // (7) src_offset + length must not exceed length of src.
4805   generate_limit_guard(src_offset, length,
4806                        load_array_length(src),
4807                        slow_region);
4808 
4809   // (8) dest_offset + length must not exceed length of dest.
4810   generate_limit_guard(dest_offset, length,
4811                        load_array_length(dest),
4812                        slow_region);
4813 
4814   // (9) each element of an oop array must be assignable
4815   // The generate_arraycopy subroutine checks this.
4816 
4817   // This is where the memory effects are placed:
4818   const TypePtr* adr_type = TypeAryPtr::get_array_body_type(dest_elem);
4819   generate_arraycopy(adr_type, dest_elem,
4820                      src, src_offset, dest, dest_offset, length,
4821                      false, false, slow_region);
4822 
4823   return true;
4824 }
4825 
4826 //-----------------------------generate_arraycopy----------------------
4827 // Generate an optimized call to arraycopy.
4828 // Caller must guard against non-arrays.
4829 // Caller must determine a common array basic-type for both arrays.
4830 // Caller must validate offsets against array bounds.
4831 // The slow_region has already collected guard failure paths
4832 // (such as out of bounds length or non-conformable array types).
4833 // The generated code has this shape, in general:
4834 //
4835 //     if (length == 0)  return   // via zero_path
4836 //     slowval = -1
4837 //     if (types unknown) {
4838 //       slowval = call generic copy loop
4839 //       if (slowval == 0)  return  // via checked_path
4840 //     } else if (indexes in bounds) {
4841 //       if ((is object array) &amp;&amp; !(array type check)) {
4842 //         slowval = call checked copy loop
4843 //         if (slowval == 0)  return  // via checked_path
4844 //       } else {
4845 //         call bulk copy loop
4846 //         return  // via fast_path
4847 //       }
4848 //     }
4849 //     // adjust params for remaining work:
4850 //     if (slowval != -1) {
4851 //       n = -1^slowval; src_offset += n; dest_offset += n; length -= n
4852 //     }
4853 //   slow_region:
4854 //     call slow arraycopy(src, src_offset, dest, dest_offset, length)
4855 //     return  // via slow_call_path
4856 //
4857 // This routine is used from several intrinsics:  System.arraycopy,
4858 // Object.clone (the array subcase), and Arrays.copyOf[Range].
4859 //
4860 void
4861 LibraryCallKit::generate_arraycopy(const TypePtr* adr_type,
4862                                    BasicType basic_elem_type,
4863                                    Node* src,  Node* src_offset,
4864                                    Node* dest, Node* dest_offset,
4865                                    Node* copy_length,
4866                                    bool disjoint_bases,
4867                                    bool length_never_negative,
4868                                    RegionNode* slow_region) {
4869 
4870   if (slow_region == NULL) {
4871     slow_region = new(C) RegionNode(1);
4872     record_for_igvn(slow_region);
4873   }
4874 
4875   Node* original_dest      = dest;
4876   AllocateArrayNode* alloc = NULL;  // used for zeroing, if needed
4877   bool  dest_uninitialized = false;
4878 
4879   // See if this is the initialization of a newly-allocated array.
4880   // If so, we will take responsibility here for initializing it to zero.
4881   // (Note:  Because tightly_coupled_allocation performs checks on the
4882   // out-edges of the dest, we need to avoid making derived pointers
4883   // from it until we have checked its uses.)
4884   if (ReduceBulkZeroing
4885       &amp;&amp; !ZeroTLAB              // pointless if already zeroed
4886       &amp;&amp; basic_elem_type != T_CONFLICT // avoid corner case
4887       &amp;&amp; !src-&gt;eqv_uncast(dest)
4888       &amp;&amp; ((alloc = tightly_coupled_allocation(dest, slow_region))
4889           != NULL)
4890       &amp;&amp; _gvn.find_int_con(alloc-&gt;in(AllocateNode::ALength), 1) &gt; 0
4891       &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn)) {
4892     // "You break it, you buy it."
4893     InitializeNode* init = alloc-&gt;initialization();
4894     assert(init-&gt;is_complete(), "we just did this");
4895     init-&gt;set_complete_with_arraycopy();
4896     assert(dest-&gt;is_CheckCastPP(), "sanity");
4897     assert(dest-&gt;in(0)-&gt;in(0) == init, "dest pinned");
4898     adr_type = TypeRawPtr::BOTTOM;  // all initializations are into raw memory
4899     // From this point on, every exit path is responsible for
4900     // initializing any non-copied parts of the object to zero.
4901     // Also, if this flag is set we make sure that arraycopy interacts properly
4902     // with G1, eliding pre-barriers. See CR 6627983.
4903     dest_uninitialized = true;
4904   } else {
4905     // No zeroing elimination here.
4906     alloc             = NULL;
4907     //original_dest   = dest;
4908     //dest_uninitialized = false;
4909   }
4910 
4911   // Results are placed here:
4912   enum { fast_path        = 1,  // normal void-returning assembly stub
4913          checked_path     = 2,  // special assembly stub with cleanup
4914          slow_call_path   = 3,  // something went wrong; call the VM
4915          zero_path        = 4,  // bypass when length of copy is zero
4916          bcopy_path       = 5,  // copy primitive array by 64-bit blocks
4917          PATH_LIMIT       = 6
4918   };
4919   RegionNode* result_region = new(C) RegionNode(PATH_LIMIT);
4920   PhiNode*    result_i_o    = new(C) PhiNode(result_region, Type::ABIO);
4921   PhiNode*    result_memory = new(C) PhiNode(result_region, Type::MEMORY, adr_type);
4922   record_for_igvn(result_region);
4923   _gvn.set_type_bottom(result_i_o);
4924   _gvn.set_type_bottom(result_memory);
4925   assert(adr_type != TypePtr::BOTTOM, "must be RawMem or a T[] slice");
4926 
4927   // The slow_control path:
4928   Node* slow_control;
4929   Node* slow_i_o = i_o();
4930   Node* slow_mem = memory(adr_type);
4931   debug_only(slow_control = (Node*) badAddress);
4932 
4933   // Checked control path:
4934   Node* checked_control = top();
4935   Node* checked_mem     = NULL;
4936   Node* checked_i_o     = NULL;
4937   Node* checked_value   = NULL;
4938 
4939   if (basic_elem_type == T_CONFLICT) {
4940     assert(!dest_uninitialized, "");
4941     Node* cv = generate_generic_arraycopy(adr_type,
4942                                           src, src_offset, dest, dest_offset,
4943                                           copy_length, dest_uninitialized);
4944     if (cv == NULL)  cv = intcon(-1);  // failure (no stub available)
4945     checked_control = control();
4946     checked_i_o     = i_o();
4947     checked_mem     = memory(adr_type);
4948     checked_value   = cv;
4949     set_control(top());         // no fast path
4950   }
4951 
4952   Node* not_pos = generate_nonpositive_guard(copy_length, length_never_negative);
4953   if (not_pos != NULL) {
4954     PreserveJVMState pjvms(this);
4955     set_control(not_pos);
4956 
4957     // (6) length must not be negative.
4958     if (!length_never_negative) {
4959       generate_negative_guard(copy_length, slow_region);
4960     }
4961 
4962     // copy_length is 0.
4963     if (!stopped() &amp;&amp; dest_uninitialized) {
4964       Node* dest_length = alloc-&gt;in(AllocateNode::ALength);
4965       if (copy_length-&gt;eqv_uncast(dest_length)
4966           || _gvn.find_int_con(dest_length, 1) &lt;= 0) {
4967         // There is no zeroing to do. No need for a secondary raw memory barrier.
4968       } else {
4969         // Clear the whole thing since there are no source elements to copy.
4970         generate_clear_array(adr_type, dest, basic_elem_type,
4971                              intcon(0), NULL,
4972                              alloc-&gt;in(AllocateNode::AllocSize));
4973         // Use a secondary InitializeNode as raw memory barrier.
4974         // Currently it is needed only on this path since other
4975         // paths have stub or runtime calls as raw memory barriers.
4976         InitializeNode* init = insert_mem_bar_volatile(Op_Initialize,
4977                                                        Compile::AliasIdxRaw,
4978                                                        top())-&gt;as_Initialize();
4979         init-&gt;set_complete(&amp;_gvn);  // (there is no corresponding AllocateNode)
4980       }
4981     }
4982 
4983     // Present the results of the fast call.
4984     result_region-&gt;init_req(zero_path, control());
4985     result_i_o   -&gt;init_req(zero_path, i_o());
4986     result_memory-&gt;init_req(zero_path, memory(adr_type));
4987   }
4988 
4989   if (!stopped() &amp;&amp; dest_uninitialized) {
4990     // We have to initialize the *uncopied* part of the array to zero.
4991     // The copy destination is the slice dest[off..off+len].  The other slices
4992     // are dest_head = dest[0..off] and dest_tail = dest[off+len..dest.length].
4993     Node* dest_size   = alloc-&gt;in(AllocateNode::AllocSize);
4994     Node* dest_length = alloc-&gt;in(AllocateNode::ALength);
4995     Node* dest_tail   = _gvn.transform(new(C) AddINode(dest_offset,
4996                                                           copy_length));
4997 
4998     // If there is a head section that needs zeroing, do it now.
4999     if (find_int_con(dest_offset, -1) != 0) {
5000       generate_clear_array(adr_type, dest, basic_elem_type,
5001                            intcon(0), dest_offset,
5002                            NULL);
5003     }
5004 
5005     // Next, perform a dynamic check on the tail length.
5006     // It is often zero, and we can win big if we prove this.
5007     // There are two wins:  Avoid generating the ClearArray
5008     // with its attendant messy index arithmetic, and upgrade
5009     // the copy to a more hardware-friendly word size of 64 bits.
5010     Node* tail_ctl = NULL;
5011     if (!stopped() &amp;&amp; !dest_tail-&gt;eqv_uncast(dest_length)) {
5012       Node* cmp_lt   = _gvn.transform(new(C) CmpINode(dest_tail, dest_length));
5013       Node* bol_lt   = _gvn.transform(new(C) BoolNode(cmp_lt, BoolTest::lt));
5014       tail_ctl = generate_slow_guard(bol_lt, NULL);
5015       assert(tail_ctl != NULL || !stopped(), "must be an outcome");
5016     }
5017 
5018     // At this point, let's assume there is no tail.
5019     if (!stopped() &amp;&amp; alloc != NULL &amp;&amp; basic_elem_type != T_OBJECT) {
5020       // There is no tail.  Try an upgrade to a 64-bit copy.
5021       bool didit = false;
5022       { PreserveJVMState pjvms(this);
5023         didit = generate_block_arraycopy(adr_type, basic_elem_type, alloc,
5024                                          src, src_offset, dest, dest_offset,
5025                                          dest_size, dest_uninitialized);
5026         if (didit) {
5027           // Present the results of the block-copying fast call.
5028           result_region-&gt;init_req(bcopy_path, control());
5029           result_i_o   -&gt;init_req(bcopy_path, i_o());
5030           result_memory-&gt;init_req(bcopy_path, memory(adr_type));
5031         }
5032       }
5033       if (didit)
5034         set_control(top());     // no regular fast path
5035     }
5036 
5037     // Clear the tail, if any.
5038     if (tail_ctl != NULL) {
5039       Node* notail_ctl = stopped() ? NULL : control();
5040       set_control(tail_ctl);
5041       if (notail_ctl == NULL) {
5042         generate_clear_array(adr_type, dest, basic_elem_type,
5043                              dest_tail, NULL,
5044                              dest_size);
5045       } else {
5046         // Make a local merge.
5047         Node* done_ctl = new(C) RegionNode(3);
5048         Node* done_mem = new(C) PhiNode(done_ctl, Type::MEMORY, adr_type);
5049         done_ctl-&gt;init_req(1, notail_ctl);
5050         done_mem-&gt;init_req(1, memory(adr_type));
5051         generate_clear_array(adr_type, dest, basic_elem_type,
5052                              dest_tail, NULL,
5053                              dest_size);
5054         done_ctl-&gt;init_req(2, control());
5055         done_mem-&gt;init_req(2, memory(adr_type));
5056         set_control( _gvn.transform(done_ctl));
5057         set_memory(  _gvn.transform(done_mem), adr_type );
5058       }
5059     }
5060   }
5061 
5062   BasicType copy_type = basic_elem_type;
5063   assert(basic_elem_type != T_ARRAY, "caller must fix this");
5064   if (!stopped() &amp;&amp; copy_type == T_OBJECT) {
5065     // If src and dest have compatible element types, we can copy bits.
5066     // Types S[] and D[] are compatible if D is a supertype of S.
5067     //
5068     // If they are not, we will use checked_oop_disjoint_arraycopy,
5069     // which performs a fast optimistic per-oop check, and backs off
5070     // further to JVM_ArrayCopy on the first per-oop check that fails.
5071     // (Actually, we don't move raw bits only; the GC requires card marks.)
5072 
5073     // Get the Klass* for both src and dest
5074     Node* src_klass  = load_object_klass(src);
5075     Node* dest_klass = load_object_klass(dest);
5076 
5077     // Generate the subtype check.
5078     // This might fold up statically, or then again it might not.
5079     //
5080     // Non-static example:  Copying List&lt;String&gt;.elements to a new String[].
5081     // The backing store for a List&lt;String&gt; is always an Object[],
5082     // but its elements are always type String, if the generic types
5083     // are correct at the source level.
5084     //
5085     // Test S[] against D[], not S against D, because (probably)
5086     // the secondary supertype cache is less busy for S[] than S.
5087     // This usually only matters when D is an interface.
5088     Node* not_subtype_ctrl = gen_subtype_check(src_klass, dest_klass);
5089     // Plug failing path into checked_oop_disjoint_arraycopy
5090     if (not_subtype_ctrl != top()) {
5091       PreserveJVMState pjvms(this);
5092       set_control(not_subtype_ctrl);
5093       // (At this point we can assume disjoint_bases, since types differ.)
5094       int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
5095       Node* p1 = basic_plus_adr(dest_klass, ek_offset);
5096       Node* n1 = LoadKlassNode::make(_gvn, immutable_memory(), p1, TypeRawPtr::BOTTOM);
5097       Node* dest_elem_klass = _gvn.transform(n1);
5098       Node* cv = generate_checkcast_arraycopy(adr_type,
5099                                               dest_elem_klass,
5100                                               src, src_offset, dest, dest_offset,
5101                                               ConvI2X(copy_length), dest_uninitialized);
5102       if (cv == NULL)  cv = intcon(-1);  // failure (no stub available)
5103       checked_control = control();
5104       checked_i_o     = i_o();
5105       checked_mem     = memory(adr_type);
5106       checked_value   = cv;
5107     }
5108     // At this point we know we do not need type checks on oop stores.
5109 
5110     // Let's see if we need card marks:
5111     if (alloc != NULL &amp;&amp; use_ReduceInitialCardMarks()) {
5112       // If we do not need card marks, copy using the jint or jlong stub.
5113       copy_type = LP64_ONLY(UseCompressedOops ? T_INT : T_LONG) NOT_LP64(T_INT);
5114       assert(type2aelembytes(basic_elem_type) == type2aelembytes(copy_type),
5115              "sizes agree");
5116     }
5117   }
5118 
5119   if (!stopped()) {
5120     // Generate the fast path, if possible.
5121     PreserveJVMState pjvms(this);
5122     generate_unchecked_arraycopy(adr_type, copy_type, disjoint_bases,
5123                                  src, src_offset, dest, dest_offset,
5124                                  ConvI2X(copy_length), dest_uninitialized);
5125 
5126     // Present the results of the fast call.
5127     result_region-&gt;init_req(fast_path, control());
5128     result_i_o   -&gt;init_req(fast_path, i_o());
5129     result_memory-&gt;init_req(fast_path, memory(adr_type));
5130   }
5131 
5132   // Here are all the slow paths up to this point, in one bundle:
5133   slow_control = top();
5134   if (slow_region != NULL)
5135     slow_control = _gvn.transform(slow_region);
5136   DEBUG_ONLY(slow_region = (RegionNode*)badAddress);
5137 
5138   set_control(checked_control);
5139   if (!stopped()) {
5140     // Clean up after the checked call.
5141     // The returned value is either 0 or -1^K,
5142     // where K = number of partially transferred array elements.
5143     Node* cmp = _gvn.transform(new(C) CmpINode(checked_value, intcon(0)));
5144     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::eq));
5145     IfNode* iff = create_and_map_if(control(), bol, PROB_MAX, COUNT_UNKNOWN);
5146 
5147     // If it is 0, we are done, so transfer to the end.
5148     Node* checks_done = _gvn.transform(new(C) IfTrueNode(iff));
5149     result_region-&gt;init_req(checked_path, checks_done);
5150     result_i_o   -&gt;init_req(checked_path, checked_i_o);
5151     result_memory-&gt;init_req(checked_path, checked_mem);
5152 
5153     // If it is not zero, merge into the slow call.
5154     set_control( _gvn.transform(new(C) IfFalseNode(iff) ));
5155     RegionNode* slow_reg2 = new(C) RegionNode(3);
5156     PhiNode*    slow_i_o2 = new(C) PhiNode(slow_reg2, Type::ABIO);
5157     PhiNode*    slow_mem2 = new(C) PhiNode(slow_reg2, Type::MEMORY, adr_type);
5158     record_for_igvn(slow_reg2);
5159     slow_reg2  -&gt;init_req(1, slow_control);
5160     slow_i_o2  -&gt;init_req(1, slow_i_o);
5161     slow_mem2  -&gt;init_req(1, slow_mem);
5162     slow_reg2  -&gt;init_req(2, control());
5163     slow_i_o2  -&gt;init_req(2, checked_i_o);
5164     slow_mem2  -&gt;init_req(2, checked_mem);
5165 
5166     slow_control = _gvn.transform(slow_reg2);
5167     slow_i_o     = _gvn.transform(slow_i_o2);
5168     slow_mem     = _gvn.transform(slow_mem2);
5169 
5170     if (alloc != NULL) {
5171       // We'll restart from the very beginning, after zeroing the whole thing.
5172       // This can cause double writes, but that's OK since dest is brand new.
5173       // So we ignore the low 31 bits of the value returned from the stub.
5174     } else {
5175       // We must continue the copy exactly where it failed, or else
5176       // another thread might see the wrong number of writes to dest.
5177       Node* checked_offset = _gvn.transform(new(C) XorINode(checked_value, intcon(-1)));
5178       Node* slow_offset    = new(C) PhiNode(slow_reg2, TypeInt::INT);
5179       slow_offset-&gt;init_req(1, intcon(0));
5180       slow_offset-&gt;init_req(2, checked_offset);
5181       slow_offset  = _gvn.transform(slow_offset);
5182 
5183       // Adjust the arguments by the conditionally incoming offset.
5184       Node* src_off_plus  = _gvn.transform(new(C) AddINode(src_offset,  slow_offset));
5185       Node* dest_off_plus = _gvn.transform(new(C) AddINode(dest_offset, slow_offset));
5186       Node* length_minus  = _gvn.transform(new(C) SubINode(copy_length, slow_offset));
5187 
5188       // Tweak the node variables to adjust the code produced below:
5189       src_offset  = src_off_plus;
5190       dest_offset = dest_off_plus;
5191       copy_length = length_minus;
5192     }
5193   }
5194 
5195   set_control(slow_control);
5196   if (!stopped()) {
5197     // Generate the slow path, if needed.
5198     PreserveJVMState pjvms(this);   // replace_in_map may trash the map
5199 
5200     set_memory(slow_mem, adr_type);
5201     set_i_o(slow_i_o);
5202 
5203     if (dest_uninitialized) {
5204       generate_clear_array(adr_type, dest, basic_elem_type,
5205                            intcon(0), NULL,
5206                            alloc-&gt;in(AllocateNode::AllocSize));
5207     }
5208 
5209     generate_slow_arraycopy(adr_type,
5210                             src, src_offset, dest, dest_offset,
5211                             copy_length, /*dest_uninitialized*/false);
5212 
5213     result_region-&gt;init_req(slow_call_path, control());
5214     result_i_o   -&gt;init_req(slow_call_path, i_o());
5215     result_memory-&gt;init_req(slow_call_path, memory(adr_type));
5216   }
5217 
5218   // Remove unused edges.
5219   for (uint i = 1; i &lt; result_region-&gt;req(); i++) {
5220     if (result_region-&gt;in(i) == NULL)
5221       result_region-&gt;init_req(i, top());
5222   }
5223 
5224   // Finished; return the combined state.
5225   set_control( _gvn.transform(result_region));
5226   set_i_o(     _gvn.transform(result_i_o)    );
5227   set_memory(  _gvn.transform(result_memory), adr_type );
5228 
5229   // The memory edges above are precise in order to model effects around
5230   // array copies accurately to allow value numbering of field loads around
5231   // arraycopy.  Such field loads, both before and after, are common in Java
5232   // collections and similar classes involving header/array data structures.
5233   //
5234   // But with low number of register or when some registers are used or killed
5235   // by arraycopy calls it causes registers spilling on stack. See 6544710.
5236   // The next memory barrier is added to avoid it. If the arraycopy can be
5237   // optimized away (which it can, sometimes) then we can manually remove
5238   // the membar also.
5239   //
5240   // Do not let reads from the cloned object float above the arraycopy.
5241   if (alloc != NULL) {
5242     // Do not let stores that initialize this object be reordered with
5243     // a subsequent store that would make this object accessible by
5244     // other threads.
5245     // Record what AllocateNode this StoreStore protects so that
5246     // escape analysis can go from the MemBarStoreStoreNode to the
5247     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
5248     // based on the escape status of the AllocateNode.
5249     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
5250   } else if (InsertMemBarAfterArraycopy)
5251     insert_mem_bar(Op_MemBarCPUOrder);
5252 }
5253 
5254 
5255 // Helper function which determines if an arraycopy immediately follows
5256 // an allocation, with no intervening tests or other escapes for the object.
5257 AllocateArrayNode*
5258 LibraryCallKit::tightly_coupled_allocation(Node* ptr,
5259                                            RegionNode* slow_region) {
5260   if (stopped())             return NULL;  // no fast path
5261   if (C-&gt;AliasLevel() == 0)  return NULL;  // no MergeMems around
5262 
5263   AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr, &amp;_gvn);
5264   if (alloc == NULL)  return NULL;
5265 
5266   Node* rawmem = memory(Compile::AliasIdxRaw);
5267   // Is the allocation's memory state untouched?
5268   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0)-&gt;is_Initialize())) {
5269     // Bail out if there have been raw-memory effects since the allocation.
5270     // (Example:  There might have been a call or safepoint.)
5271     return NULL;
5272   }
5273   rawmem = rawmem-&gt;in(0)-&gt;as_Initialize()-&gt;memory(Compile::AliasIdxRaw);
5274   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0) == alloc)) {
5275     return NULL;
5276   }
5277 
5278   // There must be no unexpected observers of this allocation.
5279   for (DUIterator_Fast imax, i = ptr-&gt;fast_outs(imax); i &lt; imax; i++) {
5280     Node* obs = ptr-&gt;fast_out(i);
5281     if (obs != this-&gt;map()) {
5282       return NULL;
5283     }
5284   }
5285 
5286   // This arraycopy must unconditionally follow the allocation of the ptr.
5287   Node* alloc_ctl = ptr-&gt;in(0);
5288   assert(just_allocated_object(alloc_ctl) == ptr, "most recent allo");
5289 
5290   Node* ctl = control();
5291   while (ctl != alloc_ctl) {
5292     // There may be guards which feed into the slow_region.
5293     // Any other control flow means that we might not get a chance
5294     // to finish initializing the allocated object.
5295     if ((ctl-&gt;is_IfFalse() || ctl-&gt;is_IfTrue()) &amp;&amp; ctl-&gt;in(0)-&gt;is_If()) {
5296       IfNode* iff = ctl-&gt;in(0)-&gt;as_If();
5297       Node* not_ctl = iff-&gt;proj_out(1 - ctl-&gt;as_Proj()-&gt;_con);
5298       assert(not_ctl != NULL &amp;&amp; not_ctl != ctl, "found alternate");
5299       if (slow_region != NULL &amp;&amp; slow_region-&gt;find_edge(not_ctl) &gt;= 1) {
5300         ctl = iff-&gt;in(0);       // This test feeds the known slow_region.
5301         continue;
5302       }
5303       // One more try:  Various low-level checks bottom out in
5304       // uncommon traps.  If the debug-info of the trap omits
5305       // any reference to the allocation, as we've already
5306       // observed, then there can be no objection to the trap.
5307       bool found_trap = false;
5308       for (DUIterator_Fast jmax, j = not_ctl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
5309         Node* obs = not_ctl-&gt;fast_out(j);
5310         if (obs-&gt;in(0) == not_ctl &amp;&amp; obs-&gt;is_Call() &amp;&amp;
5311             (obs-&gt;as_Call()-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point())) {
5312           found_trap = true; break;
5313         }
5314       }
5315       if (found_trap) {
5316         ctl = iff-&gt;in(0);       // This test feeds a harmless uncommon trap.
5317         continue;
5318       }
5319     }
5320     return NULL;
5321   }
5322 
5323   // If we get this far, we have an allocation which immediately
5324   // precedes the arraycopy, and we can take over zeroing the new object.
5325   // The arraycopy will finish the initialization, and provide
5326   // a new control state to which we will anchor the destination pointer.
5327 
5328   return alloc;
5329 }
5330 
5331 // Helper for initialization of arrays, creating a ClearArray.
5332 // It writes zero bits in [start..end), within the body of an array object.
5333 // The memory effects are all chained onto the 'adr_type' alias category.
5334 //
5335 // Since the object is otherwise uninitialized, we are free
5336 // to put a little "slop" around the edges of the cleared area,
5337 // as long as it does not go back into the array's header,
5338 // or beyond the array end within the heap.
5339 //
5340 // The lower edge can be rounded down to the nearest jint and the
5341 // upper edge can be rounded up to the nearest MinObjAlignmentInBytes.
5342 //
5343 // Arguments:
5344 //   adr_type           memory slice where writes are generated
5345 //   dest               oop of the destination array
5346 //   basic_elem_type    element type of the destination
5347 //   slice_idx          array index of first element to store
5348 //   slice_len          number of elements to store (or NULL)
5349 //   dest_size          total size in bytes of the array object
5350 //
5351 // Exactly one of slice_len or dest_size must be non-NULL.
5352 // If dest_size is non-NULL, zeroing extends to the end of the object.
5353 // If slice_len is non-NULL, the slice_idx value must be a constant.
5354 void
5355 LibraryCallKit::generate_clear_array(const TypePtr* adr_type,
5356                                      Node* dest,
5357                                      BasicType basic_elem_type,
5358                                      Node* slice_idx,
5359                                      Node* slice_len,
5360                                      Node* dest_size) {
5361   // one or the other but not both of slice_len and dest_size:
5362   assert((slice_len != NULL? 1: 0) + (dest_size != NULL? 1: 0) == 1, "");
5363   if (slice_len == NULL)  slice_len = top();
5364   if (dest_size == NULL)  dest_size = top();
5365 
5366   // operate on this memory slice:
5367   Node* mem = memory(adr_type); // memory slice to operate on
5368 
5369   // scaling and rounding of indexes:
5370   int scale = exact_log2(type2aelembytes(basic_elem_type));
5371   int abase = arrayOopDesc::base_offset_in_bytes(basic_elem_type);
5372   int clear_low = (-1 &lt;&lt; scale) &amp; (BytesPerInt  - 1);
5373   int bump_bit  = (-1 &lt;&lt; scale) &amp; BytesPerInt;
5374 
5375   // determine constant starts and ends
5376   const intptr_t BIG_NEG = -128;
5377   assert(BIG_NEG + 2*abase &lt; 0, "neg enough");
5378   intptr_t slice_idx_con = (intptr_t) find_int_con(slice_idx, BIG_NEG);
5379   intptr_t slice_len_con = (intptr_t) find_int_con(slice_len, BIG_NEG);
5380   if (slice_len_con == 0) {
5381     return;                     // nothing to do here
5382   }
5383   intptr_t start_con = (abase + (slice_idx_con &lt;&lt; scale)) &amp; ~clear_low;
5384   intptr_t end_con   = find_intptr_t_con(dest_size, -1);
5385   if (slice_idx_con &gt;= 0 &amp;&amp; slice_len_con &gt;= 0) {
5386     assert(end_con &lt; 0, "not two cons");
5387     end_con = round_to(abase + ((slice_idx_con + slice_len_con) &lt;&lt; scale),
5388                        BytesPerLong);
5389   }
5390 
5391   if (start_con &gt;= 0 &amp;&amp; end_con &gt;= 0) {
5392     // Constant start and end.  Simple.
5393     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5394                                        start_con, end_con, &amp;_gvn);
5395   } else if (start_con &gt;= 0 &amp;&amp; dest_size != top()) {
5396     // Constant start, pre-rounded end after the tail of the array.
5397     Node* end = dest_size;
5398     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5399                                        start_con, end, &amp;_gvn);
5400   } else if (start_con &gt;= 0 &amp;&amp; slice_len != top()) {
5401     // Constant start, non-constant end.  End needs rounding up.
5402     // End offset = round_up(abase + ((slice_idx_con + slice_len) &lt;&lt; scale), 8)
5403     intptr_t end_base  = abase + (slice_idx_con &lt;&lt; scale);
5404     int      end_round = (-1 &lt;&lt; scale) &amp; (BytesPerLong  - 1);
5405     Node*    end       = ConvI2X(slice_len);
5406     if (scale != 0)
5407       end = _gvn.transform(new(C) LShiftXNode(end, intcon(scale) ));
5408     end_base += end_round;
5409     end = _gvn.transform(new(C) AddXNode(end, MakeConX(end_base)));
5410     end = _gvn.transform(new(C) AndXNode(end, MakeConX(~end_round)));
5411     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5412                                        start_con, end, &amp;_gvn);
5413   } else if (start_con &lt; 0 &amp;&amp; dest_size != top()) {
5414     // Non-constant start, pre-rounded end after the tail of the array.
5415     // This is almost certainly a "round-to-end" operation.
5416     Node* start = slice_idx;
5417     start = ConvI2X(start);
5418     if (scale != 0)
5419       start = _gvn.transform(new(C) LShiftXNode( start, intcon(scale) ));
5420     start = _gvn.transform(new(C) AddXNode(start, MakeConX(abase)));
5421     if ((bump_bit | clear_low) != 0) {
5422       int to_clear = (bump_bit | clear_low);
5423       // Align up mod 8, then store a jint zero unconditionally
5424       // just before the mod-8 boundary.
5425       if (((abase + bump_bit) &amp; ~to_clear) - bump_bit
5426           &lt; arrayOopDesc::length_offset_in_bytes() + BytesPerInt) {
5427         bump_bit = 0;
5428         assert((abase &amp; to_clear) == 0, "array base must be long-aligned");
5429       } else {
5430         // Bump 'start' up to (or past) the next jint boundary:
5431         start = _gvn.transform(new(C) AddXNode(start, MakeConX(bump_bit)));
5432         assert((abase &amp; clear_low) == 0, "array base must be int-aligned");
5433       }
5434       // Round bumped 'start' down to jlong boundary in body of array.
5435       start = _gvn.transform(new(C) AndXNode(start, MakeConX(~to_clear)));
5436       if (bump_bit != 0) {
5437         // Store a zero to the immediately preceding jint:
5438         Node* x1 = _gvn.transform(new(C) AddXNode(start, MakeConX(-bump_bit)));
5439         Node* p1 = basic_plus_adr(dest, x1);
5440         mem = StoreNode::make(_gvn, control(), mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);
5441         mem = _gvn.transform(mem);
5442       }
5443     }
5444     Node* end = dest_size; // pre-rounded
5445     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5446                                        start, end, &amp;_gvn);
5447   } else {
5448     // Non-constant start, unrounded non-constant end.
5449     // (Nobody zeroes a random midsection of an array using this routine.)
5450     ShouldNotReachHere();       // fix caller
5451   }
5452 
5453   // Done.
5454   set_memory(mem, adr_type);
5455 }
5456 
5457 
5458 bool
5459 LibraryCallKit::generate_block_arraycopy(const TypePtr* adr_type,
5460                                          BasicType basic_elem_type,
5461                                          AllocateNode* alloc,
5462                                          Node* src,  Node* src_offset,
5463                                          Node* dest, Node* dest_offset,
5464                                          Node* dest_size, bool dest_uninitialized) {
5465   // See if there is an advantage from block transfer.
5466   int scale = exact_log2(type2aelembytes(basic_elem_type));
5467   if (scale &gt;= LogBytesPerLong)
5468     return false;               // it is already a block transfer
5469 
5470   // Look at the alignment of the starting offsets.
5471   int abase = arrayOopDesc::base_offset_in_bytes(basic_elem_type);
5472 
5473   intptr_t src_off_con  = (intptr_t) find_int_con(src_offset, -1);
5474   intptr_t dest_off_con = (intptr_t) find_int_con(dest_offset, -1);
5475   if (src_off_con &lt; 0 || dest_off_con &lt; 0)
5476     // At present, we can only understand constants.
5477     return false;
5478 
5479   intptr_t src_off  = abase + (src_off_con  &lt;&lt; scale);
5480   intptr_t dest_off = abase + (dest_off_con &lt;&lt; scale);
5481 
5482   if (((src_off | dest_off) &amp; (BytesPerLong-1)) != 0) {
5483     // Non-aligned; too bad.
5484     // One more chance:  Pick off an initial 32-bit word.
5485     // This is a common case, since abase can be odd mod 8.
5486     if (((src_off | dest_off) &amp; (BytesPerLong-1)) == BytesPerInt &amp;&amp;
5487         ((src_off ^ dest_off) &amp; (BytesPerLong-1)) == 0) {
5488       Node* sptr = basic_plus_adr(src,  src_off);
5489       Node* dptr = basic_plus_adr(dest, dest_off);
5490       Node* sval = make_load(control(), sptr, TypeInt::INT, T_INT, adr_type, MemNode::unordered);
5491       store_to_memory(control(), dptr, sval, T_INT, adr_type, MemNode::unordered);
5492       src_off += BytesPerInt;
5493       dest_off += BytesPerInt;
5494     } else {
5495       return false;
5496     }
5497   }
5498   assert(src_off % BytesPerLong == 0, "");
5499   assert(dest_off % BytesPerLong == 0, "");
5500 
5501   // Do this copy by giant steps.
5502   Node* sptr  = basic_plus_adr(src,  src_off);
5503   Node* dptr  = basic_plus_adr(dest, dest_off);
5504   Node* countx = dest_size;
5505   countx = _gvn.transform(new (C) SubXNode(countx, MakeConX(dest_off)));
5506   countx = _gvn.transform(new (C) URShiftXNode(countx, intcon(LogBytesPerLong)));
5507 
5508   bool disjoint_bases = true;   // since alloc != NULL
5509   generate_unchecked_arraycopy(adr_type, T_LONG, disjoint_bases,
5510                                sptr, NULL, dptr, NULL, countx, dest_uninitialized);
5511 
5512   return true;
5513 }
5514 
5515 
5516 // Helper function; generates code for the slow case.
5517 // We make a call to a runtime method which emulates the native method,
5518 // but without the native wrapper overhead.
5519 void
5520 LibraryCallKit::generate_slow_arraycopy(const TypePtr* adr_type,
5521                                         Node* src,  Node* src_offset,
5522                                         Node* dest, Node* dest_offset,
5523                                         Node* copy_length, bool dest_uninitialized) {
5524   assert(!dest_uninitialized, "Invariant");
5525   Node* call = make_runtime_call(RC_NO_LEAF | RC_UNCOMMON,
5526                                  OptoRuntime::slow_arraycopy_Type(),
5527                                  OptoRuntime::slow_arraycopy_Java(),
5528                                  "slow_arraycopy", adr_type,
5529                                  src, src_offset, dest, dest_offset,
5530                                  copy_length);
5531 
5532   // Handle exceptions thrown by this fellow:
5533   make_slow_call_ex(call, env()-&gt;Throwable_klass(), false);
5534 }
5535 
5536 // Helper function; generates code for cases requiring runtime checks.
5537 Node*
5538 LibraryCallKit::generate_checkcast_arraycopy(const TypePtr* adr_type,
5539                                              Node* dest_elem_klass,
5540                                              Node* src,  Node* src_offset,
5541                                              Node* dest, Node* dest_offset,
5542                                              Node* copy_length, bool dest_uninitialized) {
5543   if (stopped())  return NULL;
5544 
5545   address copyfunc_addr = StubRoutines::checkcast_arraycopy(dest_uninitialized);
5546   if (copyfunc_addr == NULL) { // Stub was not generated, go slow path.
5547     return NULL;
5548   }
5549 
5550   // Pick out the parameters required to perform a store-check
5551   // for the target array.  This is an optimistic check.  It will
5552   // look in each non-null element's class, at the desired klass's
5553   // super_check_offset, for the desired klass.
5554   int sco_offset = in_bytes(Klass::super_check_offset_offset());
5555   Node* p3 = basic_plus_adr(dest_elem_klass, sco_offset);
5556   Node* n3 = new(C) LoadINode(NULL, memory(p3), p3, _gvn.type(p3)-&gt;is_ptr(), TypeInt::INT, MemNode::unordered);
5557   Node* check_offset = ConvI2X(_gvn.transform(n3));
5558   Node* check_value  = dest_elem_klass;
5559 
5560   Node* src_start  = array_element_address(src,  src_offset,  T_OBJECT);
5561   Node* dest_start = array_element_address(dest, dest_offset, T_OBJECT);
5562 
5563   // (We know the arrays are never conjoint, because their types differ.)
5564   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5565                                  OptoRuntime::checkcast_arraycopy_Type(),
5566                                  copyfunc_addr, "checkcast_arraycopy", adr_type,
5567                                  // five arguments, of which two are
5568                                  // intptr_t (jlong in LP64)
5569                                  src_start, dest_start,
5570                                  copy_length XTOP,
5571                                  check_offset XTOP,
5572                                  check_value);
5573 
5574   return _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5575 }
5576 
5577 
5578 // Helper function; generates code for cases requiring runtime checks.
5579 Node*
5580 LibraryCallKit::generate_generic_arraycopy(const TypePtr* adr_type,
5581                                            Node* src,  Node* src_offset,
5582                                            Node* dest, Node* dest_offset,
5583                                            Node* copy_length, bool dest_uninitialized) {
5584   assert(!dest_uninitialized, "Invariant");
5585   if (stopped())  return NULL;
5586   address copyfunc_addr = StubRoutines::generic_arraycopy();
5587   if (copyfunc_addr == NULL) { // Stub was not generated, go slow path.
5588     return NULL;
5589   }
5590 
5591   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5592                     OptoRuntime::generic_arraycopy_Type(),
5593                     copyfunc_addr, "generic_arraycopy", adr_type,
5594                     src, src_offset, dest, dest_offset, copy_length);
5595 
5596   return _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5597 }
5598 
5599 // Helper function; generates the fast out-of-line call to an arraycopy stub.
5600 void
5601 LibraryCallKit::generate_unchecked_arraycopy(const TypePtr* adr_type,
5602                                              BasicType basic_elem_type,
5603                                              bool disjoint_bases,
5604                                              Node* src,  Node* src_offset,
5605                                              Node* dest, Node* dest_offset,
5606                                              Node* copy_length, bool dest_uninitialized) {
5607   if (stopped())  return;               // nothing to do
5608 
5609   Node* src_start  = src;
5610   Node* dest_start = dest;
5611   if (src_offset != NULL || dest_offset != NULL) {
5612     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5613     src_start  = array_element_address(src,  src_offset,  basic_elem_type);
5614     dest_start = array_element_address(dest, dest_offset, basic_elem_type);
5615   }
5616 
5617   // Figure out which arraycopy runtime method to call.
5618   const char* copyfunc_name = "arraycopy";
5619   address     copyfunc_addr =
5620       basictype2arraycopy(basic_elem_type, src_offset, dest_offset,
5621                           disjoint_bases, copyfunc_name, dest_uninitialized);
5622 
5623   // Call it.  Note that the count_ix value is not scaled to a byte-size.
5624   make_runtime_call(RC_LEAF|RC_NO_FP,
5625                     OptoRuntime::fast_arraycopy_Type(),
5626                     copyfunc_addr, copyfunc_name, adr_type,
5627                     src_start, dest_start, copy_length XTOP);
5628 }
5629 
5630 //-------------inline_encodeISOArray-----------------------------------
5631 // encode char[] to byte[] in ISO_8859_1
5632 bool LibraryCallKit::inline_encodeISOArray() {
5633   assert(callee()-&gt;signature()-&gt;size() == 5, "encodeISOArray has 5 parameters");
5634   // no receiver since it is static method
5635   Node *src         = argument(0);
5636   Node *src_offset  = argument(1);
5637   Node *dst         = argument(2);
5638   Node *dst_offset  = argument(3);
5639   Node *length      = argument(4);
5640 
5641   const Type* src_type = src-&gt;Value(&amp;_gvn);
5642   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
5643   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5644   const TypeAryPtr* top_dest = dst_type-&gt;isa_aryptr();
5645   if (top_src  == NULL || top_src-&gt;klass()  == NULL ||
5646       top_dest == NULL || top_dest-&gt;klass() == NULL) {
5647     // failed array check
5648     return false;
5649   }
5650 
5651   // Figure out the size and type of the elements we will be copying.
5652   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5653   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5654   if (src_elem != T_CHAR || dst_elem != T_BYTE) {
5655     return false;
5656   }
5657   Node* src_start = array_element_address(src, src_offset, src_elem);
5658   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
5659   // 'src_start' points to src array + scaled offset
5660   // 'dst_start' points to dst array + scaled offset
5661 
5662   const TypeAryPtr* mtype = TypeAryPtr::BYTES;
5663   Node* enc = new (C) EncodeISOArrayNode(control(), memory(mtype), src_start, dst_start, length);
5664   enc = _gvn.transform(enc);
5665   Node* res_mem = _gvn.transform(new (C) SCMemProjNode(enc));
5666   set_memory(res_mem, mtype);
5667   set_result(enc);
5668   return true;
5669 }
5670 
5671 /**
5672  * Calculate CRC32 for byte.
5673  * int java.util.zip.CRC32.update(int crc, int b)
5674  */
5675 bool LibraryCallKit::inline_updateCRC32() {
5676   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5677   assert(callee()-&gt;signature()-&gt;size() == 2, "update has 2 parameters");
5678   // no receiver since it is static method
5679   Node* crc  = argument(0); // type: int
5680   Node* b    = argument(1); // type: int
5681 
5682   /*
5683    *    int c = ~ crc;
5684    *    b = timesXtoThe32[(b ^ c) &amp; 0xFF];
5685    *    b = b ^ (c &gt;&gt;&gt; 8);
5686    *    crc = ~b;
5687    */
5688 
5689   Node* M1 = intcon(-1);
5690   crc = _gvn.transform(new (C) XorINode(crc, M1));
5691   Node* result = _gvn.transform(new (C) XorINode(crc, b));
5692   result = _gvn.transform(new (C) AndINode(result, intcon(0xFF)));
5693 
5694   Node* base = makecon(TypeRawPtr::make(StubRoutines::crc_table_addr()));
5695   Node* offset = _gvn.transform(new (C) LShiftINode(result, intcon(0x2)));
5696   Node* adr = basic_plus_adr(top(), base, ConvI2X(offset));
5697   result = make_load(control(), adr, TypeInt::INT, T_INT, MemNode::unordered);
5698 
5699   crc = _gvn.transform(new (C) URShiftINode(crc, intcon(8)));
5700   result = _gvn.transform(new (C) XorINode(crc, result));
5701   result = _gvn.transform(new (C) XorINode(result, M1));
5702   set_result(result);
5703   return true;
5704 }
5705 
5706 /**
5707  * Calculate CRC32 for byte[] array.
5708  * int java.util.zip.CRC32.updateBytes(int crc, byte[] buf, int off, int len)
5709  */
5710 bool LibraryCallKit::inline_updateBytesCRC32() {
5711   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5712   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5713   // no receiver since it is static method
5714   Node* crc     = argument(0); // type: int
5715   Node* src     = argument(1); // type: oop
5716   Node* offset  = argument(2); // type: int
5717   Node* length  = argument(3); // type: int
5718 
5719   const Type* src_type = src-&gt;Value(&amp;_gvn);
5720   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5721   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5722     // failed array check
5723     return false;
5724   }
5725 
5726   // Figure out the size and type of the elements we will be copying.
5727   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5728   if (src_elem != T_BYTE) {
5729     return false;
5730   }
5731 
5732   // 'src_start' points to src array + scaled offset
5733   Node* src_start = array_element_address(src, offset, src_elem);
5734 
5735   // We assume that range check is done by caller.
5736   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5737 
5738   // Call the stub.
5739   address stubAddr = StubRoutines::updateBytesCRC32();
5740   const char *stubName = "updateBytesCRC32";
5741 
5742   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5743                                  stubAddr, stubName, TypePtr::BOTTOM,
5744                                  crc, src_start, length);
5745   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5746   set_result(result);
5747   return true;
5748 }
5749 
5750 /**
5751  * Calculate CRC32 for ByteBuffer.
5752  * int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)
5753  */
5754 bool LibraryCallKit::inline_updateByteBufferCRC32() {
5755   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5756   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5757   // no receiver since it is static method
5758   Node* crc     = argument(0); // type: int
5759   Node* src     = argument(1); // type: long
5760   Node* offset  = argument(3); // type: int
5761   Node* length  = argument(4); // type: int
5762 
5763   src = ConvL2X(src);  // adjust Java long to machine word
5764   Node* base = _gvn.transform(new (C) CastX2PNode(src));
5765   offset = ConvI2X(offset);
5766 
5767   // 'src_start' points to src array + scaled offset
5768   Node* src_start = basic_plus_adr(top(), base, offset);
5769 
5770   // Call the stub.
5771   address stubAddr = StubRoutines::updateBytesCRC32();
5772   const char *stubName = "updateBytesCRC32";
5773 
5774   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5775                                  stubAddr, stubName, TypePtr::BOTTOM,
5776                                  crc, src_start, length);
5777   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5778   set_result(result);
5779   return true;
5780 }
5781 
5782 //----------------------------inline_reference_get----------------------------
5783 // public T java.lang.ref.Reference.get();
5784 bool LibraryCallKit::inline_reference_get() {
5785   const int referent_offset = java_lang_ref_Reference::referent_offset;
5786   guarantee(referent_offset &gt; 0, "should have already been set");
5787 
5788   // Get the argument:
5789   Node* reference_obj = null_check_receiver();
5790   if (stopped()) return true;
5791 
5792   Node* adr = basic_plus_adr(reference_obj, reference_obj, referent_offset);
5793 
5794   ciInstanceKlass* klass = env()-&gt;Object_klass();
5795   const TypeOopPtr* object_type = TypeOopPtr::make_from_klass(klass);
5796 
5797   Node* no_ctrl = NULL;
5798   Node* result = make_load(no_ctrl, adr, object_type, T_OBJECT, MemNode::unordered);
5799 
5800   // Use the pre-barrier to record the value in the referent field
5801   pre_barrier(false /* do_load */,
5802               control(),
5803               NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
5804               result /* pre_val */,
5805               T_OBJECT);
5806 
5807   // Add memory barrier to prevent commoning reads from this field
5808   // across safepoint since GC can change its value.
5809   insert_mem_bar(Op_MemBarCPUOrder);
5810 
5811   set_result(result);
5812   return true;
5813 }
5814 
5815 
5816 Node * LibraryCallKit::load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString,
5817                                               bool is_exact=true, bool is_static=false) {
5818 
5819   const TypeInstPtr* tinst = _gvn.type(fromObj)-&gt;isa_instptr();
5820   assert(tinst != NULL, "obj is null");
5821   assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
5822   assert(!is_exact || tinst-&gt;klass_is_exact(), "klass not exact");
5823 
5824   ciField* field = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;get_field_by_name(ciSymbol::make(fieldName),
5825                                                                           ciSymbol::make(fieldTypeString),
5826                                                                           is_static);
5827   if (field == NULL) return (Node *) NULL;
5828   assert (field != NULL, "undefined field");
5829 
5830   // Next code  copied from Parse::do_get_xxx():
5831 
5832   // Compute address and memory type.
5833   int offset  = field-&gt;offset_in_bytes();
5834   bool is_vol = field-&gt;is_volatile();
5835   ciType* field_klass = field-&gt;type();
5836   assert(field_klass-&gt;is_loaded(), "should be loaded");
5837   const TypePtr* adr_type = C-&gt;alias_type(field)-&gt;adr_type();
5838   Node *adr = basic_plus_adr(fromObj, fromObj, offset);
5839   BasicType bt = field-&gt;layout_type();
5840 
5841   // Build the resultant type of the load
5842   const Type *type = TypeOopPtr::make_from_klass(field_klass-&gt;as_klass());
5843 
5844   // Build the load.
5845   Node* loadedField = make_load(NULL, adr, type, bt, adr_type, MemNode::unordered, is_vol);
5846   return loadedField;
5847 }
5848 
5849 
5850 //------------------------------inline_aescrypt_Block-----------------------
5851 bool LibraryCallKit::inline_aescrypt_Block(vmIntrinsics::ID id) {
5852   address stubAddr;
5853   const char *stubName;
5854   assert(UseAES, "need AES instruction support");
5855 
5856   switch(id) {
5857   case vmIntrinsics::_aescrypt_encryptBlock:
5858     stubAddr = StubRoutines::aescrypt_encryptBlock();
5859     stubName = "aescrypt_encryptBlock";
5860     break;
5861   case vmIntrinsics::_aescrypt_decryptBlock:
5862     stubAddr = StubRoutines::aescrypt_decryptBlock();
5863     stubName = "aescrypt_decryptBlock";
5864     break;
5865   }
5866   if (stubAddr == NULL) return false;
5867 
5868   Node* aescrypt_object = argument(0);
5869   Node* src             = argument(1);
5870   Node* src_offset      = argument(2);
5871   Node* dest            = argument(3);
5872   Node* dest_offset     = argument(4);
5873 
5874   // (1) src and dest are arrays.
5875   const Type* src_type = src-&gt;Value(&amp;_gvn);
5876   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5877   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5878   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5879   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5880 
5881   // for the quick and dirty code we will skip all the checks.
5882   // we are just trying to get the call to be generated.
5883   Node* src_start  = src;
5884   Node* dest_start = dest;
5885   if (src_offset != NULL || dest_offset != NULL) {
5886     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5887     src_start  = array_element_address(src,  src_offset,  T_BYTE);
5888     dest_start = array_element_address(dest, dest_offset, T_BYTE);
5889   }
5890 
5891   // now need to get the start of its expanded key array
5892   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
5893   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
5894   if (k_start == NULL) return false;
5895 
5896   if (Matcher::pass_original_key_for_aes()) {
5897     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
5898     // compatibility issues between Java key expansion and SPARC crypto instructions
5899     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
5900     if (original_k_start == NULL) return false;
5901 
5902     // Call the stub.
5903     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
5904                       stubAddr, stubName, TypePtr::BOTTOM,
5905                       src_start, dest_start, k_start, original_k_start);
5906   } else {
5907     // Call the stub.
5908     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
5909                       stubAddr, stubName, TypePtr::BOTTOM,
5910                       src_start, dest_start, k_start);
5911   }
5912 
5913   return true;
5914 }
5915 
5916 //------------------------------inline_cipherBlockChaining_AESCrypt-----------------------
5917 bool LibraryCallKit::inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id) {
5918   address stubAddr;
5919   const char *stubName;
5920 
5921   assert(UseAES, "need AES instruction support");
5922 
5923   switch(id) {
5924   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
5925     stubAddr = StubRoutines::cipherBlockChaining_encryptAESCrypt();
5926     stubName = "cipherBlockChaining_encryptAESCrypt";
5927     break;
5928   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
5929     stubAddr = StubRoutines::cipherBlockChaining_decryptAESCrypt();
5930     stubName = "cipherBlockChaining_decryptAESCrypt";
5931     break;
5932   }
5933   if (stubAddr == NULL) return false;
5934 
5935   Node* cipherBlockChaining_object = argument(0);
5936   Node* src                        = argument(1);
5937   Node* src_offset                 = argument(2);
5938   Node* len                        = argument(3);
5939   Node* dest                       = argument(4);
5940   Node* dest_offset                = argument(5);
5941 
5942   // (1) src and dest are arrays.
5943   const Type* src_type = src-&gt;Value(&amp;_gvn);
5944   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
5945   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5946   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
5947   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL
5948           &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
5949 
5950   // checks are the responsibility of the caller
5951   Node* src_start  = src;
5952   Node* dest_start = dest;
5953   if (src_offset != NULL || dest_offset != NULL) {
5954     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5955     src_start  = array_element_address(src,  src_offset,  T_BYTE);
5956     dest_start = array_element_address(dest, dest_offset, T_BYTE);
5957   }
5958 
5959   // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
5960   // (because of the predicated logic executed earlier).
5961   // so we cast it here safely.
5962   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
5963 
5964   Node* embeddedCipherObj = load_field_from_object(cipherBlockChaining_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
5965   if (embeddedCipherObj == NULL) return false;
5966 
5967   // cast it to what we know it will be at runtime
5968   const TypeInstPtr* tinst = _gvn.type(cipherBlockChaining_object)-&gt;isa_instptr();
5969   assert(tinst != NULL, "CBC obj is null");
5970   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBC obj is not loaded");
5971   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
5972   if (!klass_AESCrypt-&gt;is_loaded()) return false;
5973 
5974   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
5975   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
5976   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
5977   Node* aescrypt_object = new(C) CheckCastPPNode(control(), embeddedCipherObj, xtype);
5978   aescrypt_object = _gvn.transform(aescrypt_object);
5979 
5980   // we need to get the start of the aescrypt_object's expanded key array
5981   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
5982   if (k_start == NULL) return false;
5983 
5984   // similarly, get the start address of the r vector
5985   Node* objRvec = load_field_from_object(cipherBlockChaining_object, "r", "[B", /*is_exact*/ false);
5986   if (objRvec == NULL) return false;
5987   Node* r_start = array_element_address(objRvec, intcon(0), T_BYTE);
5988 
5989   Node* cbcCrypt;
5990   if (Matcher::pass_original_key_for_aes()) {
5991     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
5992     // compatibility issues between Java key expansion and SPARC crypto instructions
5993     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
5994     if (original_k_start == NULL) return false;
5995 
5996     // Call the stub, passing src_start, dest_start, k_start, r_start, src_len and original_k_start
5997     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
5998                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
5999                                  stubAddr, stubName, TypePtr::BOTTOM,
6000                                  src_start, dest_start, k_start, r_start, len, original_k_start);
6001   } else {
6002     // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
6003     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6004                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6005                                  stubAddr, stubName, TypePtr::BOTTOM,
6006                                  src_start, dest_start, k_start, r_start, len);
6007   }
6008 
6009   // return cipher length (int)
6010   Node* retvalue = _gvn.transform(new (C) ProjNode(cbcCrypt, TypeFunc::Parms));
6011   set_result(retvalue);
6012   return true;
6013 }
6014 
6015 //------------------------------get_key_start_from_aescrypt_object-----------------------
6016 Node * LibraryCallKit::get_key_start_from_aescrypt_object(Node *aescrypt_object) {
6017   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "K", "[I", /*is_exact*/ false);
6018   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6019   if (objAESCryptKey == NULL) return (Node *) NULL;
6020 
6021   // now have the array, need to get the start address of the K array
6022   Node* k_start = array_element_address(objAESCryptKey, intcon(0), T_INT);
6023   return k_start;
6024 }
6025 
6026 //------------------------------get_original_key_start_from_aescrypt_object-----------------------
6027 Node * LibraryCallKit::get_original_key_start_from_aescrypt_object(Node *aescrypt_object) {
6028   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "lastKey", "[B", /*is_exact*/ false);
6029   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6030   if (objAESCryptKey == NULL) return (Node *) NULL;
6031 
6032   // now have the array, need to get the start address of the lastKey array
6033   Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);
6034   return original_k_start;
6035 }
6036 
6037 //----------------------------inline_cipherBlockChaining_AESCrypt_predicate----------------------------
6038 // Return node representing slow path of predicate check.
6039 // the pseudo code we want to emulate with this predicate is:
6040 // for encryption:
6041 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
6042 // for decryption:
6043 //    if ((embeddedCipherObj instanceof AESCrypt) &amp;&amp; (cipher!=plain)) do_intrinsic, else do_javapath
6044 //    note cipher==plain is more conservative than the original java code but that's OK
6045 //
6046 Node* LibraryCallKit::inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting) {
6047   // First, check receiver for NULL since it is virtual method.
6048   Node* objCBC = argument(0);
6049   objCBC = null_check(objCBC);
6050 
6051   if (stopped()) return NULL; // Always NULL
6052 
6053   // Load embeddedCipher field of CipherBlockChaining object.
6054   Node* embeddedCipherObj = load_field_from_object(objCBC, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6055 
6056   // get AESCrypt klass for instanceOf check
6057   // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
6058   // will have same classloader as CipherBlockChaining object
6059   const TypeInstPtr* tinst = _gvn.type(objCBC)-&gt;isa_instptr();
6060   assert(tinst != NULL, "CBCobj is null");
6061   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBCobj is not loaded");
6062 
6063   // we want to do an instanceof comparison against the AESCrypt class
6064   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6065   if (!klass_AESCrypt-&gt;is_loaded()) {
6066     // if AESCrypt is not even loaded, we never take the intrinsic fast path
6067     Node* ctrl = control();
6068     set_control(top()); // no regular fast path
6069     return ctrl;
6070   }
6071   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6072 
6073   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
6074   Node* cmp_instof  = _gvn.transform(new (C) CmpINode(instof, intcon(1)));
6075   Node* bool_instof  = _gvn.transform(new (C) BoolNode(cmp_instof, BoolTest::ne));
6076 
6077   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6078 
6079   // for encryption, we are done
6080   if (!decrypting)
6081     return instof_false;  // even if it is NULL
6082 
6083   // for decryption, we need to add a further check to avoid
6084   // taking the intrinsic path when cipher and plain are the same
6085   // see the original java code for why.
6086   RegionNode* region = new(C) RegionNode(3);
6087   region-&gt;init_req(1, instof_false);
6088   Node* src = argument(1);
6089   Node* dest = argument(4);
6090   Node* cmp_src_dest = _gvn.transform(new (C) CmpPNode(src, dest));
6091   Node* bool_src_dest = _gvn.transform(new (C) BoolNode(cmp_src_dest, BoolTest::eq));
6092   Node* src_dest_conjoint = generate_guard(bool_src_dest, NULL, PROB_MIN);
6093   region-&gt;init_req(2, src_dest_conjoint);
6094 
6095   record_for_igvn(region);
6096   return _gvn.transform(region);
6097 }
</pre></body></html>
